<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
<meta property="og:type" content="website">
<meta property="og:title" content="Yam">
<meta property="og:url" content="http://www.yam.gift/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yam">
<meta name="twitter:description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="http://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main">
  
    <article id="post-NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP" class="post-NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/">自然语言计算机形式分析的理论与方法笔记(Ch18)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/" data-id="cju9u9zay000x5occ9pitz900" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第十八章：自然语言处理中的理性主义与经验主义"><a href="#第十八章：自然语言处理中的理性主义与经验主义" class="headerlink" title="第十八章：自然语言处理中的理性主义与经验主义"></a>第十八章：自然语言处理中的理性主义与经验主义</h1><ul>
<li>理性主义：以生成语言学为基础的方法</li>
<li>经验主义：以大规模语料库的分析为基础的方法</li>
</ul>
<h2 id="哲学中的理性主义和经验主义"><a href="#哲学中的理性主义和经验主义" class="headerlink" title="哲学中的理性主义和经验主义"></a>哲学中的理性主义和经验主义</h2><p>感性与理性的矛盾也是经验主义和理性主义的矛盾，人人类哲学思想发展的内在动力之一。</p>
<p>理性主义：</p>
<ul>
<li>笛卡尔改造了传统的演绎法，制定了理性的演绎法。他认为任何真理的认识都必须首先在人的认识中找到一个最确定、最可靠的支点，才能保证推出的知识也是确定可靠的。</li>
<li>斯宾诺莎把几何学方法应用于伦理学研究，使用几何学的公理、定义、命题、证明等步骤进行演绎推理。</li>
<li>莱布尼茨把逻辑学高度抽象化、形式化和精确化，使逻辑学成为一种用符号进行演算的工具。</li>
</ul>
<p>经验主义：</p>
<ul>
<li>培根提出 “三表法”，制定了经验归纳法，建立了归纳逻辑体系。他批评理性派哲学家：“只是从经验中抓到一些既没有适当审定也没有仔细考察和衡量的普遍例证，而把其余的事情都交给了玄想和个人的机智活动”。</li>
<li>霍布斯认为归纳法不仅包含分析，也包含综合，分析得出的普遍原因只有通过综合才能成为研究对象的特殊原因。</li>
<li>洛克把理性演绎隶属于经验归纳之下，对演绎法做了经验主义的理解。他认为一切知识和推论的直接对象是一些个别、特殊的事物，我们获取知识只能从个别、特殊到一般。只是后来人心采取了相反的途径，要尽力把它的知识形成概括的命题。</li>
<li>休谟运用实验推理的方法剖析人性，试图建立一个精神哲学体系。他指出：“一切关于事实的推理，似乎都建立在因果关系上面，只要依照这种关系推理便能超出我们的记忆和感觉的见证以外”，他认为，“原因和结果的发现，是不能通过理性，只能通过经验的”，经验是我们关于因果关系的一切推论和结论的基础。</li>
<li>牛顿认为自然哲学只能从经验事实出发去解释世界事物，把经验归纳法作为科学研究的一般方法论原理。</li>
</ul>
<h2 id="自然语言处理中理性主义和经验主义的消长"><a href="#自然语言处理中理性主义和经验主义的消长" class="headerlink" title="自然语言处理中理性主义和经验主义的消长"></a>自然语言处理中理性主义和经验主义的消长</h2><ul>
<li>早期 NLP 带有鲜明的经验主义色彩<ul>
<li>1913 年 Markov 提出 Markov 随机过程理论，建立了 Markov 模型</li>
<li>1948 年 Shannon 把 Markov 过程的概率模型应用于描述语言的自动机，还把 “熵” 作为测量语言信息量的一种方法</li>
</ul>
</li>
<li>Noam Chomsky 带来的重大转向<ul>
<li>1956 年 Chomsky 吸取了有限状态 Markov 过程的思想，首次把有限状态自动机作为一种工具来刻画语言的语法，并把有限状态语言定义为有限状态语法生成的语言，建立了自然语言的有限状态模型。</li>
<li>1969 年他主张采用公理化、形式化的方法，严格地按照一定的规则描述自然语言的特征，提出了 “生成语法”，试图使用有限的规则描述无限的语言现象，发现人类普遍的语言机制，建立 “普遍语法”。</li>
<li>他认为经验主义的统计方法只能解释语言的表面现象，不能解释语言的内在规则或生成机制。</li>
<li>他认为生成语法的研究应遵循自然科学研究中的 Galileo-Newton 风格：人们应当努力构建最好的理论，不要为干扰理论解释力的现象而分散精力，同时应当认识到，世界与常识直觉是不相一致的。<ul>
<li>Galileo 风格的核心内容是：人们正在构建的理论体系是确实的真理，由于存在过多的因素和各种各样的事物，现象序列往往是对于真理的某种歪曲。所以科学研究中要去寻求那些看起来确实能够给予人们深刻见解的原则。</li>
<li>Newton 风格的核心内容是：目前的科学水平下，世界本身是不可理解的，科学研究所要做的最好的事情就是努力构建可以被理解的理论。</li>
</ul>
</li>
<li>生成语法的目的是构建关于人类语言的理论，而不是描写语言的各种事实和现象。即探索和发现语言事实和现象背后的本质和原则。</li>
<li>NLP 中的理性主义方法是一种基于规则的方法，或符号主义的方法。基本根据是 “物理符号系统假设”，该假设主张：人类的智能行为可以使用武物理符号系统来模拟，物理符号系统包含一些物理符号的模式，这些模式可以用来构建各种符号表达式以表示符号的结构。</li>
<li>基于规则的理性主义方法适合处理深层次的语言现象和长距离依存关系，多使用演绎法。</li>
</ul>
</li>
<li>20 世纪 50 年代末到 60 年代中期，经验主义兴盛起来<ul>
<li>学者普遍认为：语言学的研究必须以语言事实作为根据，才有可能得出比较可靠的结论。</li>
<li>基于统计的语言概率模型表现出色，取得很大成功。</li>
<li>适合于处理浅层次的语言现象和近距离的依存关系，多使用归纳法。</li>
<li>50 年代后期，Bayes 方法被应用于解决最优字符识别的问题。</li>
<li>60 年代，统计方法在语音识别算法研制中取得成功，特别是 HMM 模型和噪声信道模型。</li>
<li>各种语料库建立并在商业上获得成功。</li>
</ul>
</li>
<li>20 世纪 60 年代至 80 年代初期，Chomsky 仍然是主流</li>
<li>20 世纪 90 年代开始，随着大规模语料库的建立，NLP 进入一个新的阶段，概率和数据驱动的方法几乎成为 NLP 的标准方法</li>
</ul>
<h2 id="理性主义方法和经验主义方法的利弊得失"><a href="#理性主义方法和经验主义方法的利弊得失" class="headerlink" title="理性主义方法和经验主义方法的利弊得失"></a>理性主义方法和经验主义方法的利弊得失</h2><p>基于规则的理性主义方法的优点：</p>
<ul>
<li>规则是语言学规则，描述和生成能力很强</li>
<li>可以有效处理长距离依存关系</li>
<li>通常明白易懂、表达清晰</li>
<li>本质上没有方向性，既可用于分析也可用于生成</li>
<li>可以在语言知识各个平面上使用，可以在不同维度上应用</li>
<li>与计算机科学的一些高效算法兼容</li>
</ul>
<p>基于规则的理性主义方法的缺点：</p>
<ul>
<li>鲁棒性差</li>
<li>往往需要多领域专家配合研究，不能通过机器学习自动获得，也不能自动泛化</li>
<li>针对性比较强，难以进一步升级</li>
<li>实际应用效果不如基于统计的经验主义方法</li>
</ul>
<p>基于统计的经验主义方法的优点：</p>
<ul>
<li>从数据中获取语言的统计知识，效果良好</li>
<li>数据越多效果越好</li>
<li>容易与基于规则的方法结合起来处理语言的约束问题</li>
<li>适合模拟有细微差别的、不精确的、模糊的概念</li>
</ul>
<p>基于统计的经验主义方法的缺点：</p>
<ul>
<li>运行时间与统计模式中包含的符号类别多少成比例线性增长</li>
<li>语料库的质量决定了效果，但获取数据费时费力，而且很难避免出错</li>
<li>容易出现数据稀疏问题</li>
</ul>
<p>对 NLP 现状的一些呼吁：</p>
<ul>
<li>Lori Levin：在计算语言学研究中，语言学在整体上是缺位的，已经失去了它应该有的位置。</li>
<li>Shuly Wintner：计算语言学领域会议的主要文章绝大多数都是工程型的，讨论的都是实际问题的工程解决方案，几乎不再有人讨论基础性的语言学问题，NLP 工程师怎么能够不研究语言学呢？</li>
<li>Kenneth Church：基于统计方法的 “钟摆” 已经摆的太远了，需要依靠深层的语言学知识去摘取高枝上的果实。他建议深入研究语言学中的规律和各种规则，将其融合到统计方法中，将两种方法有效地结合起来。</li>
</ul>
<h2 id="探索理性主义方法和经验主义方法结合的途径"><a href="#探索理性主义方法和经验主义方法结合的途径" class="headerlink" title="探索理性主义方法和经验主义方法结合的途径"></a>探索理性主义方法和经验主义方法结合的途径</h2><p>培根：历来处理科学的人，不是实验家，就是教条者。实验家像蚂蚁，只会采集和使用；推论家像蜘蛛，只凭自己的材料来织成丝网。而蜜蜂采取中道，从花朵中采集材料，用自己的能力加以变化和消化。哲学的真正任务正是这样，它既非完全或主要依靠心的能力，也非只把从自然历史和机械实验收来的材料原封不动、囫囵吞枣地累置于记忆当中，而是把它们变化过和消化过放置在理解力之中。</p>
<p>本章主要从哲学层面探讨了自然语言处理，其实，工业界关注工程完全可以理解，学术界因为容易出成果也可以理解，我想任何人都不会否认 “无论使用什么方法，效果好是最重要的”。不过哲学中的一些观点还是很有趣的，也不光适用于自然语言处理。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/">
    <time datetime="2019-04-08T15:46:00.000Z" class="entry-date">
        2019-04-08
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Philosophy/">Philosophy</a></li></ul>

    </footer>
</article>





  
    <article id="post-2019-03-31-Nabokov-Favorite-Word" class="post-2019-03-31-Nabokov-Favorite-Word post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/31/2019-03-31-Nabokov-Favorite-Word/">《纳博科夫最喜欢的词》读书笔记与思考</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/31/2019-03-31-Nabokov-Favorite-Word/" data-id="cjtx2yw7o0032vccc6svu1zb8" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>作家们确实有各自的风格，而且是可以进行预测的。事实证明，所有书籍的作者都在写作中不断重复自己的遣词造句和行文方式。</p>
        
          <p class="article-more-link">
            <a href="/2019/03/31/2019-03-31-Nabokov-Favorite-Word/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/31/2019-03-31-Nabokov-Favorite-Word/">
    <time datetime="2019-03-31T15:23:00.000Z" class="entry-date">
        2019-03-31
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Thinking/">Thinking</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Style/">Style</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing" class="post-NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/">自然语言计算机形式分析的理论与方法笔记(Ch15)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/" data-id="cju9u9zbu002k5occ7h5h5xlz" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第十五章：语音自动处理的形式模型"><a href="#第十五章：语音自动处理的形式模型" class="headerlink" title="第十五章：语音自动处理的形式模型"></a>第十五章：语音自动处理的形式模型</h1><p>语音自动处理主要包括：自动语音识别(ASR) 和文本-语音转换(TTS)。</p>
<h2 id="语音和音位的形式描述方法"><a href="#语音和音位的形式描述方法" class="headerlink" title="语音和音位的形式描述方法"></a>语音和音位的形式描述方法</h2><p>词的发音模拟为表示音子和语段的符号串。音子就是言语的发音，用语音符号表示。可以用三套不同的字母符号——IPA，ARPAbet，SAMPA 描述音子。</p>
<ul>
<li>IPA：国际音标，字母表+标音原则</li>
<li>ARPAbet：使用 ASCII 字符</li>
<li>SAMPA：计算机可读的语音学符号</li>
</ul>
<p>语音链描述了语音传送过程。语音的发出是发音语音学问题，发音器官包括：鼻腔、鼻咽腔、软腭、口咽腔、喉头盖、咽腔、假声带、喉腔、声带、食管、唇、甲状软骨和气管等部分。</p>
<p>语音可分为辅音和元音两大类。辅音以某种方式限制和阻挡气流的运动，从而形成浊音或清音。元音受到的阻挡较小，一般为浊音。元音比辅音响亮，延续时间较长。</p>
<ul>
<li><p>根据发音部位不同，辅音分为：唇音、齿音、齿龈音、上颚音、软腭音、喉音等。汉语普通话的辅音（22 个）按发音方法分：</p>
<ul>
<li>塞音：b d g p t k</li>
<li><p>塞擦音：z c zh ch j q</p>
</li>
<li><p>擦音：f s sh x h r</p>
</li>
<li>鼻音：m n ng</li>
<li>边音：l</li>
</ul>
</li>
<li><p>根据发音方法不同，可以分为：塞音、鼻音、擦音（与塞擦音）、半元音、颤音。汉语普通话的辅音按发音部位分：</p>
<ul>
<li>双唇音：b p m</li>
<li>唇齿音：f</li>
<li>舌面前音：d t n l</li>
<li>舌面音：j q x</li>
<li>舌根音：g k h</li>
<li>舌尖前音：z c s</li>
<li>舌尖后音：zh ch sh r</li>
</ul>
</li>
</ul>
<p>元音也可以通过发音部位描述，有两个重要参数：一个是发元音时舌位的高低（大致相当于舌头最高部分所处的位置）；另一个是嘴唇的形状。</p>
<p>舌位处于前面的元音叫前元音，后面的叫后元音；最高点相对高的元音叫高元音，最高点的值处于中部或低部的元音分别叫中元音或低元音。汉语普通话的元音 (42) 个分为三种：</p>
<ul>
<li>单元音：13 个，i, a, o, ü 等</li>
<li>复合元音：13 个，ai, ei, ao, ou, iao, iou, uai, uei, ia, ie, ua, uo, üe 等</li>
<li>复鼻韵尾音：16 个，ian, uan, uen, iong, üan, iang, uang, ueng 等</li>
</ul>
<p>辅音和元音结合成音节，粗略地说，音节是一个元音之类的音和它周围的一些联系非常紧密的辅音结合而成的。把单词分割为音节的工作叫音节切分。汉语声母和韵母构成的音节形式表可参见 P. 665。汉语普通话的声母和韵母之间存在着十分严格的配合关系。</p>
<p>一般把某个音子一类的发音变化叫作一个抽象的类别——音位，是对不同实际语音的一种归纳或抽象。音位和它的变体之间的关系可以用 “音位规则” 表示。</p>
<h2 id="声学语音学和信号"><a href="#声学语音学和信号" class="headerlink" title="声学语音学和信号"></a>声学语音学和信号</h2><p>声学分析的基础是正弦余弦函数。语音处理的第一步是把模拟信号转为数字信号，又分抽样和量化两个步骤。</p>
<ul>
<li>抽样度量信号在特定时刻的振幅，抽样率是每秒钟提取的样本数，每周期至少正负两个样本，给定抽样率的最大频率叫 Nyquist 频率。</li>
<li>振幅的测量结果一般以整数存储，该过程叫量化。量化后可以以不同的格式存储，这些格式的参数之一是抽样率和抽样范围，如麦克风语音数据以 16Hz 抽样，16 位样本存储；另一个参数是频道数；还有一个参数是个体抽样存储，可以采用线性存储或压缩存储。<ul>
<li>电话语音使用的一个常见的压缩格式是 μ-律（对数压缩算法），直觉是：人类的听觉在音强较小时比较大时更加敏感。</li>
<li>非对数的线行值通常指线性 PCM 值。</li>
</ul>
</li>
</ul>
<p>声带振动产生声波，这个振动的频率叫作基音频率(基频)，记为 F。</p>
<p>除了任何时刻的振幅，还需知道某一个时间段内的平均振幅，为了避免正负抵消，一般使用振幅的均方根(RMS)，叫 RMS 振幅：<script type="math/tex">\sqrt {\frac {1}{N} \sum_{i=1}^{N} x_i^2}​</script> (N 为样本数)</p>
<p>信号的强度与振幅的平方有关：<script type="math/tex">\frac {1}{N} \sum_{i=1}^{N} x_i^2</script> (N 为样本数)</p>
<p>音强是对于人类听觉阈值的强度归一化，用分贝度量，如果阈值的压强 <script type="math/tex">P_0 = 2 * 10^5 Pa​</script> ，那么音强为：</p>
<script type="math/tex; mode=display">10 \lg \frac{1}{NP_0} \sum_{i=1}^{N} x_i^2</script><p>振幅就是声音的压强水平。</p>
<p>音高和响度是两个重要的感知特性，与频率和音强有关。</p>
<ul>
<li><p>一般在 100-1000 Hz 范围内，音高与频率是线性相关的，对于音高感知的计量采用 “美”：如果一对语音在感知上它们的音高听起来是等距离的，就是以相同数目的 “美” 被分开的，”美” 的频率 m 可以根据粗糙的声学频率计算：<script type="math/tex">m=1127 \ln (1 + \frac{f}{700})</script></p>
</li>
<li><p>响度与信号强度感知有关，振幅大的听起来会觉得响，但不是线性关系</p>
</li>
</ul>
<p>抽取 F0 叫作基音抽取，有自相关方法、基于倒谱特征的方法等。</p>
<p>根据傅里叶变换，可以用声谱表示频率，声谱中最容易看到的声谱峰是区别不同语音的最明显的特征；声谱峰是音子的声谱特征的 “签名”。</p>
<p>声谱表示某一时刻的声波的频率成分，频谱表示这些不同的频率是怎样使波形随时间改变而改变的，相当于把声波的三个维（时间、频率和振幅）可视化。频谱横轴为时间，纵轴为频率，沿纵轴亮度不同的条纹就是声音压强水平，每一个条纹就相当于一个声谱图。频谱中每一个暗色条纹（条纹中的一段）叫作共振峰。</p>
<p>不同元音在特征位置具有不同的共振峰，因此可以把元音区别开来。不同的元音之所以具有不同的声谱特征，因为口腔是一个滤波器，改变声腔形状能使不同频率的声音得到放大。</p>
<h2 id="语音自动合成的方法"><a href="#语音自动合成的方法" class="headerlink" title="语音自动合成的方法"></a>语音自动合成的方法</h2><p>语音自动合成就是把文本映射为波形，有两步：</p>
<ul>
<li>文本分析：将输入文本转换为语音内部表示</li>
<li>波形合成：将内部表示转换为波形</li>
</ul>
<p>波形合成有三个区别很大的范式：毗连合成、共振峰合成和发音合成。本章以毗连合成为主，语音样本先被切分为碎块，存储后结合起来重新组合，造出新句子。</p>
<p>文本分析又分为文本归一化、语音分析、韵律分析等部分；波形合成又分为单元选择、单元数据库等部分。</p>
<ul>
<li>文本归一化<ul>
<li>任务一：句子的词例还原，把文本片段切分成彼此分开的话段。关键部分是小圆点排歧，使用有监督学习训练一个分类器判断是不是句子结尾。</li>
<li>任务二：处理非标准词，数字、首字母缩写词、普通缩写词等，因为它们的读音存在不同读法，可以看成是一种歧义问题。处理至少有三个步骤：词例还原、分类、扩充。<ul>
<li>词例还原用于分割和识别潜在的非标准词，可以用词典和简单的启发式算法</li>
<li>分类用于给非标准词标上读音类型，可以用正则或机器学习分类器，可利用的特征包括组成成分的字母特征、相邻单词辨识等等</li>
<li>扩充用于把每一个类型的非标准词转换为标准词的符号串。一般需要借助缩写词词典，并使用同形异义词排歧算法处理歧义问题。同形异义词可以利用词类信息排歧。</li>
</ul>
</li>
</ul>
</li>
<li><p>针对已经归一化的单词符号串中的每一个单词，产生出单词的发音。最重要的一个组成部分是大规模发音词典。但有两个方面需要加强，一方面是命名实体，一方面是未知词。</p>
<ul>
<li>词典：<ul>
<li>英文的 CELEX，CMU，PROLEX，UNISYN 等发音词典</li>
<li>英文的 TIMIT（波形与音子序列对齐），Switchboard（波形与音节序列对齐），Bukeye 等语音标注语料库。</li>
</ul>
</li>
<li>命名实体：包括人名、地理名称、商业机构名称等，使用发音词典</li>
<li><p>未知词：很多现代系统采用 “字位-音位转换” 的方法来处理未知的名称，通常需要两个预测系统：一个是系统预测名称，一个是系统预测非名称。把字母序列转换成音子序列的过程叫作 “字位-音位转换”，简称 g2p。</p>
<ul>
<li>早期的算法都是 Chomsky-Halle 重写规则，这样的规则通常叫 字母-语音规则 (LTS 规则)，LTS 规则按照顺序使用，仅当前面规则的上下文条件不符合时才可以使用下面一条规则。</li>
<li>1984 年，Lucassen 和 Mercer 把该问题形式化：对于一个给定的字母序列 L，搜索概率最大的音子序列 P。<ul>
<li>把观察范围扩大到围绕字母前后的一个窗口 (就是 N-gram) </li>
<li>把已经正确识别的前面的音子的信息加入到概率模型</li>
<li>单词的词类标记，甚至字母的类别信息</li>
</ul>
</li>
<li>大多数语音合成系统分别建立两个字位-音位分类器，一个用于未知的人名，一个用于其他的未知词</li>
</ul>
</li>
</ul>
</li>
<li><p>韵律分析：一般在比语音更长的语言单位上起作用，也叫超音段现象，三个主要的音系学特征：</p>
<ul>
<li>突显度：音节的一个特征，描述通常是相对的，只是说明一个音节比另一个更突出。有助于突出话语重点</li>
<li>结构：某些词似乎自然地结合在一起或分开。通常用韵律短语来描述韵律结构，具有同样韵律短语结构的话语应该具有同样的句法结构。</li>
<li>调：话语的语调节律。具有区别意义作用。<ul>
<li>可以分解，其中最重要的组成部分是音高重音。</li>
<li>音高重音分类最流行的模式是 Pierrehumbert 模式或 ToBI  模式，模式指出，英语中共有六种音高重音，由高调 H 和低调 L 两个简单的调按不同方式组合而成：<code>H* 重音高调</code>，<code>L* 重音低调</code>，<code>L+H* 低调+重音高调</code>，<code>L*+H 重音低调+高调</code>，<code>H+L*</code>，<code>H*+L</code>。此外还有两个短语重音 <code>L-</code> 和 <code>H-</code>，两个边界调 <code>L%</code> 和 <code>H%</code>，用于短语的结尾以控制语调的升降。</li>
<li>其他语调模型与 ToBI 的不同在于，它们不使用离散的音位类别来表示语调重音。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>突显度、结构和调三个音位因素相互作用，并且在各种不同的语音和声学现象中被实现。突现的音节一般比非突现的音节读的重一些、长一些；韵律的短语边界通常有停顿，边界之前的音节变长，有时边界处音高变低；语调的不同规则表现为基频曲拱（F0）的差异。</p>
<p>TTS 的主要任务是生成韵律的适当的语言表示，并且从这样的语言表示出发，生成适当的声学模式，而这样的声学模式将表现为输出语音的波形。这样的一个韵律成分在 TTS 系统中的输出就是音子的一个序列，每个音子都具有一个音延的值和一个音高的值。每个音子的音延与语音上下文有关。F0 的值受到上面讨论过的各种因素的影响，包括词重音、句子的重读或焦点成分以及话语的语调。</p>
<p>20 世纪 50 年代初人们提出了波形合成的三种主要的泛型：毗连合成、共振峰合成、发音合成。</p>
<ul>
<li>1953 年，Harris 的方法是把与音子对应的磁带片段按照字面的顺序拼接在一起，是建立在单音子基础上的。对每一个音子都要存储多个复本，并且使用连接代价进行选择（选择转移到相邻单元时具有最为平滑的共振峰的那些单元）。</li>
<li>1958 年，Peterson 提出要使用双音子和数据库，对每一个音子都要存储多个具有不同韵律的复本，且每一个复本都要标注韵律特征，如 F0、重音、时延等，并且还要使用基于 F0 和相邻单元共振峰距离的连接代价进行选择。纯理论模型。</li>
<li>1992 年，学者们提出单元选择合成技术理论，包括非均匀长度大单元的理论，使用目标代价的理论，后来形式化后变成了形式模型。</li>
<li>1996 年，Donovan 把语音识别中使用的决策树聚类算法引入语音合成。</li>
<li>共振峰合成试图建立规则来生成人工声谱，其中包括生成共振峰的规则。发音合成试图直接给声道和发音过程的物理机制建模。</li>
</ul>
<h2 id="语音自动识别的方法"><a href="#语音自动识别的方法" class="headerlink" title="语音自动识别的方法"></a>语音自动识别的方法</h2><p>语音自动识别（ASR）研究的目标是用计算机来建立语音识别系统，把声学信号映射为单词串。</p>
<p>语音自动理解（ASU）研究的目标除了单词还要产生句子，并在某种程度上理解这些句子。</p>
<p>语音识别参数：</p>
<ul>
<li>词汇量大小</li>
<li>语音流畅度和自然度</li>
<li>信道和噪声</li>
<li>说话人的语音特征</li>
</ul>
<p>ASR 目前的重点是大词汇量连续语音识别 (LVCSR)，占统治地位的范式是 HMM。语音识别可以分为三个阶段：</p>
<ul>
<li>特征抽取：获取观察值 O，信号处理阶段，语音的声学波形按照音片的时间框架抽样，把音片的时间框架转换成声谱特征，每一个时间框架的窗口用矢量表示，每一个矢量大约包括 39 个特征，用以表示声谱的信息以及能量大小和声谱变化的信息。</li>
<li>声学建模：获取观察似然读 P(O|W) 和先验概率 P(W)，音子识别阶段，对于给定的语言单位 (单词、音子、次音子) 计算观察到的声谱特征矢量的似然度。</li>
<li>解码：获取文本 W，声学模型 AM + HMM 单词发音词典 + 语言模型 LM 输出最可能的单词序列，HMM 发音词典就是单词的发音表，其中每一个发音用一个音子串表示。</li>
</ul>
<h3 id="特征抽取"><a href="#特征抽取" class="headerlink" title="特征抽取"></a>特征抽取</h3><p>最普通的方法是 mel 频率倒谱系数，MFCC 是建立在倒谱的思想上的。</p>
<ul>
<li>把模拟信号转为数字信号<ul>
<li>抽样<ul>
<li>每秒钟抽取的样本数叫抽样率</li>
<li>每一轮抽样至少两个样本，一个测量正侧，一个测量负侧</li>
<li>可能测量的最大频率的波是频率等于抽样率一半的波，给定抽样率的最大频率叫 Nyquist 频率</li>
</ul>
</li>
<li>量化<ul>
<li>把实数值表示为整数的过程叫量化</li>
</ul>
</li>
</ul>
</li>
<li>抽取 MFCC 特征<ul>
<li>预加重：加重高频段的能量，使用滤波器进行。原因是声谱斜移 (频率高能量下降的现象)。</li>
<li>加窗：语音的统计特性在时间上不恒定，因此从一个小窗口抽取波形，并假定在该区域内是恒定的。<ul>
<li>三个参数：窗口的宽度、连续窗口之间的偏移、窗口的形状</li>
<li>从每一个窗口抽出的语音叫作一帧，帧持续的时间叫作帧长，连续窗口的左边沿之间相距的时间叫作帧移</li>
<li>矩形窗的边界处会支离破碎地切掉一些信号使得信号不连续，所以普遍使用汉明窗，在边界处把信号值收缩到零</li>
</ul>
</li>
<li>离散傅里叶变换 (DFT)：对抽样的离散时间信号的离散频带，抽取其声谱信息 (频率和振幅的关系)，常用的算法是快速傅里叶变换 (FFT)。</li>
<li>mel 滤波器组和对数表示：因为人类听觉对高频不敏感，所以把 DFT 输出的频率改为 “美” 标度。<ul>
<li>如果一对语音在感知上的音高听起来是等距离的，就可以用相同数目的 “美” 分开。</li>
<li>低于 1000 Hz 时，频率与 “美” 标度之间的映射是线性关系；高于 1000 Hz 时是对数关系。</li>
<li>通过建立一个滤波器组来实现这样的直觉。</li>
<li>最后使用对数表示 mei 声谱的值。</li>
<li>一般情况下，人类对于信号级别的反应是按照对数计算的，振幅高的阶段，人类对于振幅的轻微差别的敏感性比在振幅低的阶段低得多。</li>
</ul>
</li>
<li>倒谱：逆向傅里叶变换（iDFT）<ul>
<li>可以实现把声源和滤波器分开（由于人发音的特性，对于探测音子最有用的信息在于滤波器，即声腔）。</li>
<li>倒谱是对数声谱的声谱（_这里涉及到信号处理，比较复杂，没看太懂_）。</li>
<li>一般只取头 12 个倒谱值，它们仅表示滤波器的信息。</li>
<li>不同倒谱系数之间的方差倾向于不相关（相对的不同频带上的声谱系数是相关的），声学模型不需要表示各个 MFCC 特征之间的协方差，降低了参数数目。</li>
</ul>
</li>
<li>Delta 特征与能量<ul>
<li>一帧的能量是该帧在某一时段内的样本幂的总和</li>
<li>从一帧到另一帧语音信号不是恒定的，可以加上倒谱特征中与时间变化有联系的特征，可以对之前的 13 个（12 个倒谱值+能量）特征加上 Delta 或速度特征，以及双 Delta 或加速度特征。</li>
<li>13 个 Delta 特征的每一个特征表示在相应的倒谱/能量特征中帧与帧之间的变化</li>
<li>13 个 双 Delta 特征中的每一个特征表示在相应的 Delta 特征中帧与帧之间的变化</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="声学建模"><a href="#声学建模" class="headerlink" title="声学建模"></a>声学建模</h3><p>词类标注中，每一个观察序列是一个离散符号（单词），可以直接统计训练集中的次数计算概率；但是在语音识别中，MFCC 矢量是实数，每一个矢量几乎都是唯一的，因此需要进行矢量的量化。</p>
<p>其中一个方法是把输入矢量映射为离散符号，叫作矢量量化（VQ）。一个矢量量化系统使用三个特征刻画：码本、聚类算法、距离测度。</p>
<ul>
<li>码本是可能类别的表（类似于词表），对于码本中的每一个代码列出的模型矢量叫作码字，码字是一个特定的特征矢量。</li>
<li>使用聚类算法建立码本，把训练集中所有的特征矢量聚类为 256 个（或其他特定数量个）类别，然后从中选择一个有代表性的特征矢量，并将其作为这个聚类的模型矢量或码字。</li>
<li>使用距离测度选择与输入特征向量最接近的模型矢量，用模型矢量替换输入矢量。<ul>
<li>欧几里得距离：假定特征矢量的每个维度同样重要</li>
<li>Mahalanobis 距离：考虑每一个维度不同的方差</li>
</ul>
</li>
</ul>
<p>接下来的过程就和词类标注一样，只需统计训练集中不同音子下每个特征矢量的概率（似然度）。当给定新的特征矢量时，P(音子|特征矢量) = P(特征矢量|音子) × P(音子)，计算出每一个音子对应的值，选择最大的即可。</p>
<p>矢量量化中数量很小的码字不足以捕捉变化多端的语音信号，而且语音现象也不是一个简单地符号化的过程，因此现代语音识别一般是直接根据实数值的、连续的输入特征矢量计算观察概率，即计算连续空间的概率密度函数。</p>
<h3 id="解码阶段"><a href="#解码阶段" class="headerlink" title="解码阶段"></a>解码阶段</h3><p>声学模型 AM + HMM 发音词典 + LM 输出最可能的单词序列。</p>
<p>语音处理的另一个重要领域是说话人识别，一般分为两个子领域：一个是说话人检验（判断是不是 X），另一个是说话人认同（从 N 个判定中选择一个）。</p>
<p>实际应用中涉及的主要技术：</p>
<ul>
<li><p>快速 Fourier 变换、倒谱处理</p>
</li>
<li><p>使用动态规划处理处理翘曲变形（对输入的语音进行模板匹配，动态时间翘曲变形）</p>
</li>
<li>基于统计的 HMM + Bayes<ul>
<li>Viterbi 动态规划解码</li>
<li>Jelinek 的栈解码</li>
</ul>
</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li><p>语音和音位的形式描述方法</p>
<ul>
<li>词的发音模拟为表示音子和语段的符号串。音子就是言语的发音，用语音符号表示。</li>
<li>语音可分为辅音和元音两大类。辅音以某种方式限制和阻挡气流的运动，从而形成浊音或清音。元音受到的阻挡较小，一般为浊音。元音比辅音响亮，延续时间较长。</li>
<li>辅音和元音结合成音节，粗略地说，音节是一个元音之类的音和它周围的一些联系非常紧密的辅音结合而成的。把单词分割为音节的工作叫音节切分。汉语普通话的声母和韵母之间存在着十分严格的配合关系。</li>
<li>一般把某个音子一类的发音变化叫作一个抽象的类别——音位，是对不同实际语音的一种归纳或抽象。音位和它的变体之间的关系可以用 “音位规则” 表示。</li>
</ul>
</li>
<li><p>声学语音学和信号</p>
<ul>
<li>声学分析的基础是正弦余弦函数。语音处理的第一步是把模拟信号转为数字信号，又分抽样和量化两个步骤。</li>
<li>音高和响度是两个重要的感知特性，与频率和音强有关。<ul>
<li>声带振动产生声波，这个振动的频率叫作基音频率(基频)，记为 F。</li>
<li>音强是对于人类听觉阈值的强度归一化，用分贝度量，如果阈值的压强 <script type="math/tex">P_0 = 2 * 10^5 Pa</script> ，那么音强为：<script type="math/tex">10 \lg \frac{1}{NP_0} \sum_{i=1}^{N} x_i^2</script> (振幅就是声音的压强水平)</li>
</ul>
</li>
<li>一般在 100-1000 Hz 范围内，音高与频率是线性相关的，对于音高感知的计量采用 “美”：如果一对语音在感知上它们的音高听起来是等距离的，就是以相同数目的 “美” 被分开的，”美” 的频率 m 可以根据粗糙的声学频率计算：<script type="math/tex">m=1127 \ln (1 + \frac{f}{700})</script></li>
<li>响度与信号强度感知有关，振幅大的听起来会觉得响，但不是线性关系</li>
<li>根据傅里叶变换，可以用声谱表示频率，声谱中最容易看到的声谱峰是区别不同语音的最明显的特征；声谱峰是音子的声谱特征的 “签名”。</li>
<li>声谱表示某一时刻的声波的频率成分，频谱表示这些不同的频率是怎样使波形随时间改变而改变的，相当于把声波的三个维（时间、频率和振幅）可视化。频谱横轴为时间，纵轴为频率，沿纵轴亮度不同的条纹就是声音压强水平，每一个条纹就相当于一个声谱。频谱中每一个暗色条纹（条纹中的一段）叫作共振峰。</li>
</ul>
</li>
<li>语音自动合成的方法<ul>
<li>语音自动合成就是把文本映射为波形，有两步：<ul>
<li>文本分析：将输入文本转换为语音内部表示</li>
<li>波形合成：将内部表示转换为波形</li>
</ul>
</li>
<li>文本分析又分为文本归一化、语音分析、韵律分析等部分<ul>
<li>文本归一化：句子的词例还原，把文本片段切分成彼此分开的话段；处理非标准词，数字、首字母缩写词、普通缩写词等。</li>
<li>语音分析：针对已经归一化的单词符号串中的每一个单词，产生出单词的发音。最重要的一个组成部分是大规模发音词典。但有两个方面需要加强，一方面是命名实体（包括人名、地理名称、商业机构名称等，使用发音词典），一方面是未知词（采用 “字位-音位转换” 的方法）。</li>
<li>韵律分析：一般在比语音更长的语言单位上起作用，也叫超音段现象，三个主要的音系学特征：突显度、结构、调。突现的音节一般比非突现的音节读的重一些、长一些；韵律的短语边界通常有停顿，边界之前的音节变长，有时边界处音高变低；语调的不同规则表现为基频曲拱（F0）的差异。</li>
<li>TTS 的主要任务是生成韵律的适当的语言表示，并且从这样的语言表示出发，生成适当的声学模式，而这样的声学模式将表现为输出语音的波形。这样的一个韵律成分在 TTS 系统中的输出就是音子的一个序列，每个音子都具有一个音延的值和一个音高的值。每个音子的音延与语音上下文有关。F0 的值受到上面讨论过的各种因素的影响，包括词重音、句子的重读或焦点成分以及话语的语调。</li>
</ul>
</li>
<li>波形合成又分为单元选择、单元数据库等部分。<ul>
<li>20 世纪 50 年代初人们提出了波形合成的三种主要的泛型：毗连合成、共振峰合成、发音合成。</li>
<li>共振峰合成试图建立规则来生成人工声谱，其中包括生成共振峰的规则。发音合成试图直接给声道和发音过程的物理机制建模。</li>
</ul>
</li>
</ul>
</li>
<li>语音自动识别的方法<ul>
<li>语音识别参数：<ul>
<li>词汇量大小</li>
<li>语音流畅度和自然度</li>
<li>信道和噪声</li>
<li>说话人的语音特征</li>
</ul>
</li>
<li>ASR 目前的重点是大词汇量连续语音识别 (LVCSR)，占统治地位的范式是 HMM。语音识别可以分为三个阶段：特征抽取、声学建模、解码。</li>
<li>特征抽取：获取观察值 O，信号处理阶段，语音的声学波形按照音片的时间框架抽样，把音片的时间框架转换成声谱特征，每一个时间框架的窗口用矢量表示，每一个矢量大约包括 39 个特征，用以表示声谱的信息以及能量大小和声谱变化的信息。最普通的方法是 mel 频率倒谱系数，MFCC 是建立在倒谱的思想上的。<ul>
<li>预加重：加重高频段的能量，使用滤波器进行。原因是声谱斜移 (频率高能量下降的现象)。</li>
<li>加窗：语音的统计特性在时间上不恒定，因此从一个小窗口抽取波形，并假定在该区域内是恒定的。</li>
<li>离散傅里叶变换 (DFT)：对抽样的离散时间信号的离散频带，抽取其声谱信息，常用的算法是快速傅里叶变换 (FFT)。</li>
<li>mel 滤波器组和对数表示：因为人类听觉对高频不敏感，所以把 DFT 输出的频率改为 “美” 标度。</li>
<li>倒谱：逆向傅里叶变换（iDFT）。</li>
<li>Delta 特征与能量生成 39 个特征。</li>
</ul>
</li>
<li>声学建模：获取观察似然读 P(O|W) 和先验概率 P(W)，音子识别阶段，对于给定的语言单位 (单词、音子、次音子) 计算观察到的声谱特征矢量的似然度。<ul>
<li>把输入矢量映射为离散符号，叫作矢量量化（VQ）。一个矢量量化系统使用三个特征刻画：码本、聚类算法、距离测度。</li>
<li>矢量量化中数量很小的码字不足以捕捉变化多端的语音信号，而且语音现象也不是一个简单地符号化的过程，因此现代语音识别一般是直接根据实数值的、连续的输入特征矢量计算观察概率，即计算连续空间的概率密度函数。</li>
</ul>
</li>
<li>解码：获取文本 W，声学模型 AM + HMM 单词发音词典 + 语言模型 LM 输出最可能的单词序列，HMM 发音词典就是单词的发音表，其中每一个发音用一个音子串表示。</li>
</ul>
</li>
</ul>
<p>这章内容有点繁琐，信号处理那块还有些难度，不是很容易理解。不过语音处理也是 NLP 的一个重要组成领域，作为一个入门的了解应该是足够了。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/">
    <time datetime="2019-03-29T03:32:00.000Z" class="entry-date">
        2019-03-29
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Model/">Formal Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-03-22-Ch14-HMM" class="post-NLPFA/2019-03-22-Ch14-HMM post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/">自然语言计算机形式分析的理论与方法笔记(Ch14)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/" data-id="cju9u9z9z00015occ90tdju3f" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第十四章：隐-Markov-模型"><a href="#第十四章：隐-Markov-模型" class="headerlink" title="第十四章：隐 Markov 模型"></a>第十四章：隐 Markov 模型</h1><h2 id="HMM-概述"><a href="#HMM-概述" class="headerlink" title="HMM 概述"></a>HMM 概述</h2><p>Markov 链也就是加权自动机。HMM 增加了要求：</p>
<ul>
<li>HMM 有一个观察符号的集合 O，这个集合中的符号不是从状态集合 Q 中的字母抽取的</li>
<li>HMM 中观察似然度函数 B 的值不只限于 1 或 0，概率 bi(ot) 可以取 0-1 之间的任何值</li>
</ul>
<p>HMM 要求的参数如下：</p>
<ul>
<li>状态序列：Q = (q1, q2, …, qn)</li>
<li>观察序列：O = (o1, o2, …, on)</li>
<li>转换概率：A = (a01, a02, …, ann)</li>
<li>观察似然度：B = bi(ot)，表示从状态 i 生成观察值 ot 的概率</li>
<li>初始状态概率分布：π，πi 是 HMM 在状态 i 开始时的概率</li>
<li>接收状态：合法的接收状态集合</li>
</ul>
<p>需要求解的是 A B π，因此一般使用 λ = {A, B, pain} 来定义一个 HMM 模型，模型对外表现出来的是观察序列，状态序列不能直接观察到，被称为 “隐变量”。</p>
<p>三个基本问题：</p>
<ul>
<li>评估问题：给定观察序列 O 和模型 λ，如何计算由该模型产生该观察序列的概率 P(O|λ)</li>
<li>解码问题：给定观察序列 O 和模型 λ，如何获取在某种意义下最优的状态序列 Q，一般使用 Viterbi 算法</li>
<li>训练问题：如何选择或调整模型参数 λ，使得在该模型下产生观察序列 O 的概率 P(O|λ) 最大</li>
</ul>
<h2 id="HMM-在语音识别中的应用"><a href="#HMM-在语音识别中的应用" class="headerlink" title="HMM 在语音识别中的应用"></a>HMM 在语音识别中的应用</h2><p>口语理解分为自动语音识别（ASR）和自动语音理解（ASU），这里重点关注 “大词汇量连续语音识别” 这个领域：</p>
<ul>
<li>大词汇量：5000-60000 个单词的词汇</li>
<li>连续：所有单词自然地、一块说出来</li>
</ul>
<p>可以把语音识别系统看作一个噪声信道模型，建立噪声信道模型需要解决两个问题：</p>
<ul>
<li>为了挑选出与噪声输入匹配的最佳的句子，需要对 “最佳匹配” 有一个完全的度量</li>
<li>所有英语句子的集合非常大，需要一个有效的算法使得只搜索那些有机会与输入匹配的句子<ul>
<li>Viterbi 或动态规划解码算法</li>
<li>A* 或栈解码算法</li>
</ul>
</li>
</ul>
<p>语音识别的概率噪声信道模型总体结构目标如下：“对于给定的某个声学输入 O，在语言 L 的所有句子中，寻找一个最可能的句子。”</p>
<ul>
<li>可以把 O 看成音片串，把句子看成单词串</li>
<li>单词通常根据正词法定义，比如 oak 和 oaks 不同，can 和 can’t 相同</li>
</ul>
<script type="math/tex; mode=display">\hat{W} = \arg \max_{W \in L} P(W|O) = \arg \max_{W \in L} P(O|W) P(W)</script><ul>
<li>假定输入序列是音子祖列，而不是声学观察序列</li>
<li>向前算法对于给定音子序列的观察，能够产生出对于给定单词的这些音子的观察概率，是 HMM 的特殊情况；进一步扩充就可以对给定的一个句子计算出音子序列的概率。</li>
</ul>
<p>关于 HMM 的扩展资料可以查看：<a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/PrababilisticGraphModel/HMM/HMM-Tutorial.ipynb" target="_blank" rel="noopener">HMM Tutorial</a></p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/">
    <time datetime="2019-03-22T03:32:00.000Z" class="entry-date">
        2019-03-22
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HMM/">HMM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Markov/">Markov</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing" class="post-NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/">自然语言计算机形式分析的理论与方法笔记(Ch13)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/" data-id="cju9u9za700035occ7pzjn2ga" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第十三章：N-元语法和数据平滑"><a href="#第十三章：N-元语法和数据平滑" class="headerlink" title="第十三章：N 元语法和数据平滑"></a>第十三章：N 元语法和数据平滑</h1><h2 id="N-元语法"><a href="#N-元语法" class="headerlink" title="N 元语法"></a>N 元语法</h2><p>N 元语法模型利用前面 N-1 个单词来预测下一个词。一些特殊情况：标点、大小写、屈折变化等。</p>
<p>一个单词的概率只依赖于它前面一个单词的这种假设叫作 Markov 假设，这样的模型叫 Bi-gram，即二元语法模型，也叫一阶 Markov 模型。</p>
<p>N 元语法模型可以使用训练语料库 “归一化” 得到。</p>
<script type="math/tex; mode=display">p(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n)}{\sum_w C(w_{n-1}w)}​</script><p>以 <script type="math/tex">w_{n-1}</script> 开头的二元语法计数必定等于 <script type="math/tex">w_{n-1}</script> 这个单词的计数，于是：</p>
<script type="math/tex; mode=display">p(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n)}{C(w_{n-1})}</script><p>一般化 N 元语法的参数估计：</p>
<script type="math/tex; mode=display">p(w_n|w_{n-N+1}^{n-1}) = \frac {C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}</script><p>两个重要事实：</p>
<ul>
<li>N 增加时，精确度相应增加，同时生成句子的局限性增加（可选的下个词减少）</li>
<li>严重依赖于语料库</li>
</ul>
<h2 id="数据平滑"><a href="#数据平滑" class="headerlink" title="数据平滑"></a>数据平滑</h2><p>N 元语法在特定语料中会有大量零概率的情况，但实际上可能并非如此，因此需要给某些零概率和低概率的 N 元语法重新赋值，这就是 “平滑”。</p>
<ul>
<li>加一平滑<ul>
<li>给取出来的二元语法的计数全部加一，即：(ci + 1)/(N+V)，V 是词表大小，因为每个都加一，所以分母加 V，<script type="math/tex">p*(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V}​</script>。</li>
<li>一个很大的问题是，每个二元语法计数加一，对应的一元语法加 V，如果某个一元语法本来出现的次数很少的话，会使其概率发生急剧变化。比如：“蝴蝶 吃饭”，我们假设它在语料中出现了 1 次，“蝴蝶” 出现了 2 次，本来的概率是 1/2=0.5，如果词表大小是 1000，概率就会变成 2/(1000+2)。当然，如果 “蝴蝶 吃饭” 没有出现在语料中，那这样的概率是可以接受的。问题就在于那些出现概率本身不高的词。</li>
<li>另外一个问题是可能会给未登录词过高的概率，尤其词表非常大的时候。这相当于人为稀释了所有在训练数据中词的概率（意味着提高了未登录词的概率）。</li>
</ul>
</li>
<li>Witten-Bell 打折法<ul>
<li>思想：看一个零概率 N 元语法的概率可以用首次看一个 N 元语法的概率模拟，即借助 “第一次看过的事物” 的数量估计 “从没见过的事物” 的概率。</li>
</ul>
</li>
<li>Good-Turing 打折法</li>
<li>Backoff</li>
<li>Interpolation</li>
</ul>
<p>更多关于 N-gram 的可以参阅：<a href="https://nbviewer.jupyter.org/github/hscspring/Note_NLP/blob/master/CMU-NeuralNetworksforNLP2017/02-lm/02-lmNote.ipynb" target="_blank" rel="noopener">LM</a></p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/">
    <time datetime="2019-03-15T03:32:00.000Z" class="entry-date">
        2019-03-15
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ngram/">Ngram</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Smoothing/">Smoothing</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-03-11-Ch12-Bayes-and-Dynamic-Programming" class="post-NLPFA/2019-03-11-Ch12-Bayes-and-Dynamic-Programming post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/11/NLPFA/2019-03-11-Ch12-Bayes-and-Dynamic-Programming/">自然语言计算机形式分析的理论与方法笔记(Ch12)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/11/NLPFA/2019-03-11-Ch12-Bayes-and-Dynamic-Programming/" data-id="cju9u9zas000r5occ9m4ula4v" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第十二章：Bayes-公式与动态规划算法"><a href="#第十二章：Bayes-公式与动态规划算法" class="headerlink" title="第十二章：Bayes 公式与动态规划算法"></a>第十二章：Bayes 公式与动态规划算法</h1><h2 id="拼写错误的检查与更正"><a href="#拼写错误的检查与更正" class="headerlink" title="拼写错误的检查与更正"></a>拼写错误的检查与更正</h2><p>1992 年，Kukich 把这个领域分解为三大问题：</p>
<ul>
<li>非词错误检查</li>
<li>孤立词错误更正</li>
<li>依赖于上下文的错误检查和更正<ul>
<li>打字操作时的错误：插入、脱落、改变位置等</li>
<li>书写错误地拼写同音词和准同音词</li>
</ul>
</li>
</ul>
<p>1964 年 Damerau 发现 80% 的错误由 “单个错误” 引起的：</p>
<ul>
<li>插入：the→ther</li>
<li>脱落：the→th</li>
<li>替代：the→thw</li>
<li>换位：the→hte</li>
</ul>
<p>Kukich 把打字错误分为两类：</p>
<ul>
<li>打字操作错误：一般与键盘有关</li>
<li>认知错误：不知道如何拼写</li>
</ul>
<p>OCR 错误一般分为五类：替代、多重替代、空白脱落、空白插入和识别失败。</p>
<p>检查方法最通常的做法是使用词典，为了表示能产生的屈折变化和派生，词典一般还包括形态分析模式。</p>
<blockquote>
<p>中文和英文情况不太一样，最大的不同是中文不会出现打出一个不存在的字这种情况。根据我们之前<a href="http://aiwriter.cn/home" target="_blank" rel="noopener">项目</a>的经验，中文的拼写错误主要包括以下几种：</p>
<ul>
<li><p>拼音输入法导致的同音、近音错误，比如 “拼音” 打成 “朋友”，“打成” 打成 “达成” 或 “调查” 之类</p>
</li>
<li><p>五笔输入法导致的形近错误，比如 “土” 打成 “士” 之类</p>
</li>
<li><p>键盘邻近键位导致的错误，比如 “爱情” 打成 “矮穷”，就是把 “情” 的 i 打成了 o 之类</p>
</li>
</ul>
<p>上面的分法是从输入的角度看，每种其实又有两个维度：</p>
<ul>
<li>输入的词不成词，比如：“维吾尔族” 打成 “维吾尔组”</li>
<li>输入的词在上下文语境中不对，比如 “体现出XXX” 打成 “体系出XXX”</li>
</ul>
<p>第一个维度用词典可以，第二个维度则需要借助其他一些方法结合词典才能起作用。</p>
<p>由于项目时间很短，我们重点做了第一种错误，此外还做了很多词法、语法、语义方面的检查和更正，后面的这几种错误主要是本章所说的认知错误，就是用户不知道那个是错的，比如 “过早夭折” 这个词的错误叫 “书面语重复”，因为 “夭折” 就包含了 “过早” 的意思，再比如 “定金” 和 “订金” 这类属于 “易混淆词”，必须要根据句子的语义才能做出正确的判断。</p>
</blockquote>
<h2 id="Bayes-公式与噪声信道模型"><a href="#Bayes-公式与噪声信道模型" class="headerlink" title="Bayes 公式与噪声信道模型"></a>Bayes 公式与噪声信道模型</h2><p>”噪声信道“ 这个比喻来自 20 世纪 70 年代 IBM 实验室应用于语音识别的模型，F. JelineK 在 1976 年把这样的模型叫作 ”噪声信道模型“，是 Bayes 推理的一个特殊情况。</p>
<p>用 w* 表示 ”对正确单词 w 的估计“，O 表示 ”观察序列，从中选出最优单词的等式为：</p>
<script type="math/tex; mode=display">w^{*} = arg_{w\in V} max P(w|O)</script><p>P(w|O) 无法直接计算，可以通过贝叶斯公式将上式变为：</p>
<script type="math/tex; mode=display">w^* = arg_{w \in V} max \frac {P(O|w)P(w)}{P(O)} = arg_{w \in V} max{P(O|w)P(w)}</script><p>P(O) 很难估计但却不影响等式，P(w) 是先验概率，可以在语料中统计出来，P(O|w) 是似然度也可以通过语料估计出来。</p>
<p>P577 页的例子比较详细，不过是关于单词拼写错误的，但和我们平时了解的单词或字级别的也没有太多区别。比如我们要对 “我 书 兔” 进行纠错，问题可以转换为：“我后面出现 书 的概率”：</p>
<p>P(书|我） = P(我书)/P(我)，后面的两个概率都可以通过语料计算出来，语料足够大时，可近似认为频率等于概率，于是我们就获得 P(书|我）的概率估计。我们同样可以估计出 P（属|我）、P（输|我）……等等的概率，这些都是 “书” 可能被打错（这里假设根据拼音输入法）的情况，然后我们找出最高概率的情况，就可以当作把它当作是正确的。这种做法在 NLP 中有个名词叫 N-gram，如果只考虑上一个词就是 Bi-gram，考虑上两个词的是 Tri-gram，更加详细的介绍可以阅读：<a href="https://nbviewer.jupyter.org/github/hscspring/Note_NLP/blob/master/CMU-NeuralNetworksforNLP2017/02-lm/02-lmNote.ipynb" target="_blank" rel="noopener">Language Model</a>。</p>
<h2 id="最小编辑距离算法"><a href="#最小编辑距离算法" class="headerlink" title="最小编辑距离算法"></a>最小编辑距离算法</h2><p>Wagner 和 Fischer 1974 年提出，两个符号串的最小编辑距离就是指把一个符号串转换为另一个符号串时所需要的最小编辑操作的次数。最小编辑距离使用动态规划来计算。</p>
<p>关于最小编辑距离大家可以阅读：<a href="">最小编辑距离</a>。</p>
<h2 id="发音问题研究中的-Bayes-方法"><a href="#发音问题研究中的-Bayes-方法" class="headerlink" title="发音问题研究中的 Bayes 方法"></a>发音问题研究中的 Bayes 方法</h2><p>发音变异有两类：词汇变异和音位变异。前者用来表示词表中单词的语音片段的差异，后者是单独的语音片段在不同的上下文中的发音差异。</p>
<ul>
<li>词汇变异的一个重要来源是社会语言变异。由语言外的因素引起，如社会背景等。<ul>
<li>方言变异是一种社会语言变异</li>
<li>另一个原因是语域或风格</li>
</ul>
</li>
<li>音位变异发生在表层形式，反映了语音因素和发音因素。每一条音位变体规则都依赖于一组十分复杂的因素，这些因素需要在概率上加以解释。很多这样的规则都是模拟协同发音的。协同发音是由相邻语音片段的发音运动而引起的某一语音片段发音的改变。英语的大多数音位变体规则有这几种类型：<ul>
<li>同化：把一个语音片段改变得更像它邻接的语音片段。普遍的同化是腭化（跨语言同化现象）。</li>
<li>异化</li>
<li>脱落</li>
<li>闪音化：闪音化时，前面的元音发的高一些，后面的元音倾向于非重读；但并不总是必要的。</li>
<li>元音弱化：很多非重读音节中的原因变为弱化元音。最常见的弱化元音是混元音。是否弱化取决于很多因素，如不同场合、单词的频率、是否为短语最后一个元音、语言习惯等。</li>
<li>增音</li>
</ul>
</li>
</ul>
<p>使用 Bayes 的基本原理是，发音为 y 的单词（可能有很多个）的前一个单词是 X 的时候，这个发音最可能对应的单词。即：p(w) * p(y|w)，p(w) 为 X 的先验概率，p(y|w) 为单词 X 下个发音为 y 的概率。</p>
<h2 id="发音变异的决策树模型"><a href="#发音变异的决策树模型" class="headerlink" title="发音变异的决策树模型"></a>发音变异的决策树模型</h2><p>1984 年，Breiman 等使用分类回归树（一种决策树），从标注语料库中自动地推导出词汇到表层发音的映射关系。决策树提取由特征集所描述的情况，并把这种情况分类为范畴和相关的概率。</p>
<p>在发音问题研究中，可以训练决策树来提取一个词汇音子和它的各种上下文特征（包围的音子、重音、音节结构信息等），并选择适合的表层音子（一个概率分布）来实现它。</p>
<h2 id="加权自动机"><a href="#加权自动机" class="headerlink" title="加权自动机"></a>加权自动机</h2><p>为了提高效率通常把编辑好的发音变异存储在词表中，这种词表的两种最普遍的方式是 trie 和加权有限状态自动机（概率有限状态自动机）。Markov 链是加权自动机的一种特殊情况（也包括 Ngram 语法）。任何一个加权自动机，只要其中的输入序列不是唯一地指定其状态序列，就可以被模拟为一个 HMM（隐马尔科夫模型）。</p>
<p>关于自动机的大家可以查看：<a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/Automata/FSM_Tutorial.ipynb" target="_blank" rel="noopener">FSM</a></p>
<h2 id="向前算法"><a href="#向前算法" class="headerlink" title="向前算法"></a>向前算法</h2><p>对于给定的加权自动机计算出音子串对于某一单词的似然读，这个算法叫 ”向前算法“。</p>
<p>向前算法是另外一种动态规划算法，可以看成是最小编辑距离算法的轻度泛化。二者相同的是，当用它求观察序列的概率的时候，要使用一个表来存储中间值；不同的是，向前算法在列上所标记的不总是以线性顺序排列的状态，而且还隐含着一个状态图，在这个状态图中，从一个状态到另一个状态之间存在着多条路径。在最小编辑距离算法中，从周围的三个单元计算每一个单元的值，并把结果填到矩阵中；向前算法中，可能会有其他的状态进入一个状态中。最小编辑距离算法只需要计算概率的最小值，而向前算法要计算能够观察的所有可能路径的概率总和。</p>
<p>关于向前算法大家可以阅读：<a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/PrababilisticGraphModel/HMM/HMM-Tutorial.ipynb" target="_blank" rel="noopener">HMM-Tutorial</a></p>
<h2 id="Viterbi-算法"><a href="#Viterbi-算法" class="headerlink" title="Viterbi 算法"></a>Viterbi 算法</h2><p>Viterbi 算法是向前算法的特殊情况，向前算法把前面所有的路径总和放到当前单元中，Viterbi 算法则把前面所有路径中最大的放到当前单元中。</p>
<p>Viterbi 算法在 NLP 中的一个重要应用是分词，分词就是找到词语的边界，Sproat 等人提出一个非常简单的算法：选择包含最高频率的单词的切分为正确的切分。换言之，把每一种潜在切分中每一个单词的概率相乘，选择最大的作为最终的切分。</p>
<p>具体实现时，要把表示汉语词表的加权有限状态转录机（FST）和 Viterbi 结合。FST 词表中每个单词表示为弧的一个序列，序列中的每个弧代表一个汉字，概率通过熵来计算，选择最小熵的切分。</p>
<p>关于 Viterbi 算法大家可以阅读：<a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/PrababilisticGraphModel/HMM/HMM-Tutorial.ipynb" target="_blank" rel="noopener">HMM-Tutorial</a></p>
<p>这章内容涉及到的算法对大部分 NLPer 来说都比较熟悉了，Bayes、自动机、向前算法、Viterbi 等，书中讲的比较简单，也没有代码，所以很有必要扩充阅读。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/11/NLPFA/2019-03-11-Ch12-Bayes-and-Dynamic-Programming/">
    <time datetime="2019-03-11T03:32:00.000Z" class="entry-date">
        2019-03-11
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Backward/">Backward</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Bayes/">Bayes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/FSM/">FSM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Forward/">Forward</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spell-Check/">Spell Check</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Viterbi/">Viterbi</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-03-01-Ch11-Probabilistic-Grammar" class="post-NLPFA/2019-03-01-Ch11-Probabilistic-Grammar post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/01/NLPFA/2019-03-01-Ch11-Probabilistic-Grammar/">自然语言计算机形式分析的理论与方法笔记(Ch11)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/01/NLPFA/2019-03-01-Ch11-Probabilistic-Grammar/" data-id="cju9u9z9v00005occp2q2fulm" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第十一章：概率语法"><a href="#第十一章：概率语法" class="headerlink" title="第十一章：概率语法"></a>第十一章：概率语法</h1><p>基于规则的句法剖析主要使用 Chomsky 的上下文无关语法。之前的自顶向下、自底向上、左角、CYK、Earley、线图分析法等都对歧义无能为力，于是有了新的改进：一方面是给上下文无关语法的规则加上概率，另一方面是除了加上概率外，还考虑规则的中心词对于规则概率的影响。这些称为 “概率语法”。</p>
<h2 id="概率上下文无关语法与句子的歧义"><a href="#概率上下文无关语法与句子的歧义" class="headerlink" title="概率上下文无关语法与句子的歧义"></a>概率上下文无关语法与句子的歧义</h2><p>上下文无关语法（CFG）定义为四元组：G={N, Σ, P, S}，N 是非终极符号集合，Σ 是终极符号集合，S 是初始符号，P 是重写规则。规则形式为 A→β，A 是单独的非终极符号，β 可以由终极符号组成，也可以由非终极符号组成。</p>
<p>在句法分析时，由于词的词性和词义的多样性会导致歧义，于是学者们试图通过基于统计的方法，计算上下文无关语法重写规则的使用概率。</p>
<h2 id="概率上下文无关语法的基本原理"><a href="#概率上下文无关语法的基本原理" class="headerlink" title="概率上下文无关语法的基本原理"></a>概率上下文无关语法的基本原理</h2><p>概率上下文无关语法（Probabilistic Context-Free Grammar，PGFG）由 Booth 1967 年最早提出。在每一个重写规则 A→β 上增加一个条件概率 p：A→β[p]，于是上下文无关语法可定义为五元组：G={N, Σ, P, S, D}，D 是给每一个规则指派概率 p 的函数。这个函数表示某个非终极符号 A 重写为符号 β 时的概率 p。规则可写为：p(A→β) 或 p(A→β|A)，从 A 写为 β 时应考虑所有情况且其概率和为 1。</p>
<p>如果句子有歧义，每一个树形图都会有一个概率，一个树形图 T 的概率等于从每一个非终极符号的结点 n 扩充的规则 r 的概率的乘积：</p>
<script type="math/tex; mode=display">P(T) = \prod_{n \in T} p(r(n))</script><p>其中，n 表示非终极符号的结点，r 表示由该非终极符号扩充的规则，小写字母 p 表示规则 r 的概率，P 表示整个树形图 T 的概率。从句子 S 的若干个树形图中选出最好的 T(S) = arg max P(T)。</p>
<p>用于 CYK 算法叫作 “概率 CYK 算法”，同样也有 Earley 算法、概率线图分析法等。</p>
<p>如何获取概率？</p>
<ul>
<li>使用句子已经的到剖析的语料库：树库，对其中的树形图进行统计</li>
<li>通过未加工的语料库自动学习语法规则，采用 “向内向外算法”（Baker 1979 年提出）</li>
</ul>
<h2 id="概率上下文无关语法的三个假设"><a href="#概率上下文无关语法的三个假设" class="headerlink" title="概率上下文无关语法的三个假设"></a>概率上下文无关语法的三个假设</h2><ul>
<li>假设一：位置无关性假设：子结点的概率与该子结点所直接管辖的字符串在句子中的位置无关</li>
<li>假设二：上下文无关性假设：子结点的概率与不受该子结点直接管辖的其他符号串无关</li>
<li>假设三：祖先结点无关性假设：子结点的概率与支配该结点的所有祖先结点的概率无关</li>
</ul>
<p>概率上下文无关语法存在结构依存和词汇依存的问题。</p>
<ul>
<li>结点上规则的转写与节点在树形图中的位置是有关的（如代词）</li>
<li>PP 附着（PP 可以做 VP 的状语，或它前面 NP 的修饰语）问题</li>
<li>并列结构的歧义</li>
</ul>
<h2 id="概率词汇化上下文无关语法"><a href="#概率词汇化上下文无关语法" class="headerlink" title="概率词汇化上下文无关语法"></a>概率词汇化上下文无关语法</h2><p>Charniak 1997 年提出，剖析树的每一个结点要标上该结点的中心词，因此规则数目比概率上下文无关语法多很多，因为结点上除了语法标记还有词汇，也就是说统计的时候不是统计树库某一类标记的概率，而是某个具体的词的概率。</p>
<p>除此之外，还有一些附加条件，以及各种回退和平滑算法。还有一些剖析器包括更多的因素，比如，区分论元成分与附属成分，对于树形图中那些比较接近的词汇依存关系比那些比较疏远的词汇依存关系给以更大的权重，考虑在给定成分中的三个最左的词类，考虑一般的结构依存关系等等。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>概率上下文无关语法与句子的歧义<ul>
<li>基于统计的方法，计算上下文无关语法重写规则的使用概率。</li>
</ul>
</li>
<li>概率上下文无关语法的基本原理<ul>
<li>概率上下文无关语法（Probabilistic Context-Free Grammar，PGFG）由 Booth 1967 年最早提出。在每一个重写规则 A→β 上增加一个条件概率 p：A→β[p]，于是上下文无关语法可定义为五元组：G={N, Σ, P, S, D}，D 是给每一个规则指派概率 p 的函数。</li>
<li>如果句子有歧义，每一个树形图都会有一个概率，一个树形图 T 的概率等于从每一个非终极符号的结点 n 扩充的规则 r 的概率的乘积。</li>
</ul>
</li>
<li>概率上下文无关语法的三个假设<ul>
<li>三个假设：<ul>
<li>位置无关</li>
<li>上下文无关</li>
<li>祖先结点无关</li>
</ul>
</li>
<li>问题：<ul>
<li>结点上规则的转写与节点在树形图中的位置是有关的（如代词）</li>
<li>PP 附着（PP 可以做 VP 的状语，或它前面 NP 的修饰语）问题</li>
<li>并列结构的歧义</li>
</ul>
</li>
</ul>
</li>
<li>概率词汇化上下文无关语法<ul>
<li>Charniak 1997 年提出，剖析树的每一个结点要标上该结点的中心词，因此规则数目比概率上下文无关语法多很多，因为结点上除了语法标记还有词汇，也就是说统计的时候不是统计树库某一类标记的概率，而是某个具体的词的概率。</li>
</ul>
</li>
</ul>
<p>这章内容非常简单，其思想与隐马尔科夫有些类似，不过后面的改进都没有提到基于贝叶斯的推理。可能是个不错的改进方向。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/01/NLPFA/2019-03-01-Ch11-Probabilistic-Grammar/">
    <time datetime="2019-03-01T03:32:00.000Z" class="entry-date">
        2019-03-01
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-02-27-Ch10-Formal-Model-of-Pragmatic-Automatic-Processing" class="post-NLPFA/2019-02-27-Ch10-Formal-Model-of-Pragmatic-Automatic-Processing post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/02/27/NLPFA/2019-02-27-Ch10-Formal-Model-of-Pragmatic-Automatic-Processing/">自然语言计算机形式分析的理论与方法笔记(Ch10)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/02/27/NLPFA/2019-02-27-Ch10-Formal-Model-of-Pragmatic-Automatic-Processing/" data-id="cju9u9zbt002i5occ0w9024df" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第十章：语用自动处理的形式模型"><a href="#第十章：语用自动处理的形式模型" class="headerlink" title="第十章：语用自动处理的形式模型"></a>第十章：语用自动处理的形式模型</h1><p>语用学是对语言与使用环境之间关系的研究。使用环境包括像人和物这样的本体，也包括话语的上下文。研究主要涉及修辞结构理论、文本连贯、言语行为理论和会话智能代理等方面。</p>
<h2 id="Mann-和-Thompson-的修辞结构理论"><a href="#Mann-和-Thompson-的修辞结构理论" class="headerlink" title="Mann 和 Thompson 的修辞结构理论"></a>Mann 和 Thompson 的修辞结构理论</h2><p>1987 年，W. Mann 和 S. Thompson 提出修辞结构理论（RST），一种基于文本局部之间关系的关于文本组织的描述理论。</p>
<p>RST 对语言使用的性质和如何理解这种性质的基本观点：</p>
<ul>
<li>如果想说明话语本身，就必须对说话者和听话者的参与有明确的解释</li>
<li>话语的结构比其他任何事物都更反映说话者的意图，意图是有层次的</li>
<li>注意和意图被认为是文本中相互独立又相互作用的方面；语言形式、语言功能和话语结构互相联系的方式是一种松散的相互制约的方式，而不是某种类似于 “一一映射” 的方式。因此并不总有什么特定的词汇或语法形式唯一地标记结构特征。</li>
</ul>
<p>RST 的核心是修辞关系的概念，修辞关系是存在于两个互不重叠的文本跨段之间的关系，一个叫 “核心单元”，一个叫 “卫星单元”，这种区分来自经验观察。根据文本分析经验，他们对文本结构做了基本假设：组织性、统一性和连贯性、统一和连贯的功能目的、层级性、层级的同质性、关系组合、关系的非对称性、关系性质的 “修辞” 功能。他们总结出了 25 种修辞关系，分为 “核心-卫星关系”（21 种）和 “多核心关系”（4 种），分别表示为 N-S 和 N-N(…N)。用来描写文本结构的 RST 只辨识结构的三种主要类型：整体结构、关系结构和句法结构，并主要研究中间一层的关系结构。</p>
<p>常用的 RST 关系：</p>
<ul>
<li>详述：卫星单元给出的是与核心单元内容有关的一些额外细节</li>
<li>对比：核心单元表示的事物尽管在某些方面具有相似性，但在某些重要方面又不同，这种关系具有多个核心</li>
<li>条件：卫星单元给出的某些事件必须在核心单元给出的情形出现之前就已经发生</li>
<li>目的：卫星单元给出的是实施核心单元所表示行为的目的</li>
<li>序列：这种关系是多核心的</li>
</ul>
<p>RST 提出了文本的树结构模型，从根节点开始的树形图可以代表整个文本的修辞关系结构。RST 将各种修辞关系概括成五种基本图式：</p>
<p><img src="http://qnimg.lovevivian.cn/book-2017-fengzhiwei-12.jpeg" alt=""></p>
<p>他们认为对文本的一个典型分析是应用一套图式使下列限制成立：</p>
<ul>
<li>完整性：有一个图式（根节点）可以覆盖整个文本</li>
<li>联系性：除根节点外，每一个分析中的文本跨段要么是一个最小单元，要么是分析中另一个图式应用的一个成分</li>
<li>唯一性：每一个图式应用都涉及不同的一套文本跨段</li>
<li>邻接性：每一个图式应用的文本跨段都组成一个邻近的文本跨段</li>
</ul>
<p>以上条件使 RST 分析形成树形结构，成为一个树图，用来代表文本的 RST 结构。在树图中，每一条竖线从被图式分解的文本跨段中延伸下来，一直到这一图式应用的核心单元（详见 P505 例子）。树图中各子结点代表各个互不重叠但又相互邻接的文本跨段。一个文本跨段是任何一部分从文本组织的角度上看有功能整体性的一个文本片段。关系存在于两个不重叠的文本跨段之间，由关系定义来确认。文本结构的概念是用一层层更大的文本跨段的网络关系来定义的。</p>
<p>RST 文本分析一般采用自底向上剖析过程：</p>
<ul>
<li>将一个文本切分成多个单元：大小任意，理论上中立性，功能上整体性</li>
<li>确定跨段和关系：要确定作者写作意图是否适用于关系定义</li>
<li>除去非良构的图：根据上面的四个限制</li>
<li>进行排歧</li>
</ul>
<p>RST 提供了一种讨论书面独白文本的联系性和整体性，以及文本如何为作者的目的服务的方法。从渊源上，它采用的是语用功能主义思想，受 Halliday 系统功能语法影响很大。</p>
<p>利用 RST 开发出文本自动剖析器的：</p>
<ul>
<li>Marcu：用 13 条公里描写了该系统运作中的各种限制，并使用数字和谓词逻辑的传统运算符对由每个文本跨段的四类信息所组成的结构进行运算，推导出一个文本的所有可能的修辞-意图树。</li>
<li>Corston-Oliver：通过将提示短语与回指、指示词和指称连贯整合，改进了算法。</li>
<li>Reitter：使用 SVM 指派文本片段间的修辞关系和核心单元。</li>
<li>O’Donnell：<a href="http://www.wagsoft.com/RSTTool/index.html" target="_blank" rel="noopener">RSTTool</a></li>
</ul>
<p>RST 并不具有显著的跨语言可转移性，它的优点是提供了完整的分析而不是选择性解释，可以应用于很多种不同的文本，允许不考虑语类对文本结构做一种统一的描写，有助于区分文本中那些真正是语类特殊的方面与那些相对而言更独立于语类的方面。不足包括：没有对各种关系如何实现做出系统描写；没有将它的理论与各种文本特性的理论（如信息流、主题结构、词汇结构等）联系起来。</p>
<p>汉语研究中，2002 年香港城市大学的卫真道利用 RST 进行了案例分析；2000 年同校的邹嘉彦等利用 RST 对 DM 自动标注系统做了改进。</p>
<h2 id="文本连贯中的常识推理技术"><a href="#文本连贯中的常识推理技术" class="headerlink" title="文本连贯中的常识推理技术"></a>文本连贯中的常识推理技术</h2><p>语言通常并不是由孤立无关的句子组成，而是由搭配在一起的相关句子群组成的，这种句子群称为  ”话语“。话语包括独白、对话和人机交互。本节主要讨论独白，即参与者是一个说话人和一个或多个听话人的单向交流。</p>
<p>话语的话段之间所有可能的连接称为连贯关系的集合。常见的连贯关系（S0 和 S1 分别表示两个相关句子的意义）：</p>
<ul>
<li>结果：S0 所声明的状态或事件导致或可能导致 S1 所声明的状态或事件</li>
<li>说明：S1 所声明状态或事件导致或可能导致 S0 所声明的状态或事件</li>
<li>平行：S0 所声明的 P（a1，a2……）和 S1 所声明的 P（b1，b2……）对所有 i a b 是类似的</li>
<li>详述：S0 和 S1 所声明的是同一命题</li>
<li>时机：推测从 S0 所声明的状态到 S1 所声明的最终状态的状态变化，或反过来</li>
</ul>
<p>每个连贯关系与一个或多个约束有关，符合约束才能维持连贯关系。如何应用这些约束？推理。演绎（向前推出隐含的关系）是一种可靠的推理，但在许多语言理解系统中所依赖的推理是不可靠的，这类推理方法称为 ”溯因推理“（从结果中寻找可能的原因）。</p>
<p>一个给定的结果可能有许多潜在原因，通常需要最佳解释。有三种策略可以采用：</p>
<ul>
<li>概率模型：计算正确空间和缺少相关语料时会有问题</li>
<li>启发式策略：比如优先选择假设最少的解释，过于脆弱和有限</li>
<li>基于代价策略：上面两者结合</li>
</ul>
<p>在确定话段间最合理的连贯关系时，需要利用世界知识和领域知识，但这是个 AI 完全问题（NP 完全），因为需要人类拥有的所有知识。详见 P514-517 的例子。</p>
<p>句子间连贯关系导致话语结构，一组局部连贯话段的节点被称为 ”话语片断“。话语结构的计算所涉及的 ”话语语法“ 只牵扯两个规则：<strong>把一个片断改写为两个较小的片断的规则，以及判断一个句子就是一个片断的规则。</strong>话语结构对于所指判定很有用，代词常常表现出一种 ”新近“ 的优先关系，即更倾向于指向附近的对象。根据话语层级结构的 ”新近“ 判定代词所指效果比根据话语线性顺序的 ”新近“ 判定好得多。</p>
<h2 id="言语行为理论和会话智能代理"><a href="#言语行为理论和会话智能代理" class="headerlink" title="言语行为理论和会话智能代理"></a>言语行为理论和会话智能代理</h2><p>语用学中关于言语行为的理论由哲学家提出。</p>
<h3 id="哲学基础"><a href="#哲学基础" class="headerlink" title="哲学基础"></a>哲学基础</h3><p>维特根斯坦：</p>
<ul>
<li>哲学的本质是语言</li>
<li>哲学的本质应该在日常生活中解决</li>
<li>凡是能够说的事情都能够说清楚，凡是不能说的事情就应该保持沉默</li>
<li>意义即使用</li>
</ul>
<p>奥斯汀：</p>
<ul>
<li>把言语行为分为 ”施行式“ 和 ”表述式“，施行式的话语有得体和不得体之分。得体的六个条件：<ul>
<li>必须存在公认的、确实有约定效果的约定程序，该程序包括由一定的人在一定的情境中说出的一定的话语。违反的不切当叫作 ”无用“。</li>
<li>在某一确定场合，那些特定的人和情景对于被援引的特定的程序的执行必须是合适的。违反的不切当叫作 ”误用“。</li>
<li>所有的话语参与者必须正确实施这样的程序。违反的不切当叫作 ”缺陷“。</li>
<li>所有的话语参与者必须完全实施这样的程序。违反的不切当叫作 ”障碍“。</li>
<li>参与者或实施者，必须具备一定思想或感情，并且自己有意去执行。违反的不切当叫作 ”非诚“。</li>
<li>参与者自己后来确实这样去执行。违反的不切当叫作 ”背诺“。</li>
</ul>
</li>
<li>对话具有一个重要特征：对话中的话段是一种由说话人实施的行为。但他没找到关于施行式话语的统一标准，于是又从 ”说事“ 和 ”做事“ 角度分析言语行为。他认为，在真实的言语中，发出的任何句子不外乎三种言语行为：<ul>
<li>以言表意行为：发出一个带有特殊意义的句子</li>
<li>以言行事行为：发出一个句子时带有询问、回答、承诺等行为</li>
<li>以言取效行为：发出一个句子对听话人的感情、信念或行为产生一种特定效果</li>
</ul>
</li>
<li>区分这三种言语行为是为了强调以言行事行为。他认为，如果要检验明显的施行式动词，最好区分出那些说出施行式话语时具有明显行事语力的动词。于是，根据不同的行事能力，以言行事分为五类：<ul>
<li>判定式：说话人对某事发出的裁决、估计、推断或评价，是判断的运用。</li>
<li>执行式：说话人对某事做出的执行、命令、指导或催促，是返回影响或运用能力。</li>
<li>承诺式：说话人对于某一行动方案做出的承诺、保证、意向或支持，是承担义务或表明意向。</li>
<li>表态式：说话人对于他人的行为做出的反应，如道歉、祝贺、感谢等，是表明态度。</li>
<li>阐述式：说话人阐述观点、做出论证、说明用法，是阐明原因、论点或意见。</li>
</ul>
</li>
<li>言语行为通常用于描述以言行事行为</li>
</ul>
<p>塞尔：</p>
<ul>
<li>对奥斯汀的言语行为分类进行了修改，分为五类：<ul>
<li>断言式：说话人对某事是某种情形的表态（建议、提出、宣誓、自夸、推断等）</li>
<li>指令式：说话人的目的是使听话人做某事（询问、命令、要求、邀请、建议、乞求等）</li>
<li>承诺式：说话人对将来的行为做出承诺（承诺、计划、发誓、打赌、反对等）</li>
<li>表情式：表达说话人对一些事情的心理状态（感谢、道歉、欢迎、悲痛等）</li>
<li>宣告式：由说话人说出而使外在世界产生新情景</li>
</ul>
</li>
</ul>
<h3 id="三个特性"><a href="#三个特性" class="headerlink" title="三个特性"></a>三个特性</h3><p>自然语言处理中，人与计算机之间的对话与会话叫作 ”会话智能代理“，包括六个组件：语音识别组件、自然语言理解组件、自然语言生成组件、文本-语音合成组件、对话管理组件和任务管理组件。</p>
<p>会话分析与书面独白三个不同的重要特性：话轮转换、会话的共同基础和会话中的隐涵。</p>
<ul>
<li>话轮轮换<ul>
<li>1974 年，Sacks 等指出，至少在美国英语中，话轮转换的行为是受一组话轮转换规则制约的。这些规则被应用于 ”合适转换位置“（Transition-Relevance Place，TRP）。话轮转换规则由三个子规则组成：<ul>
<li>如果在该话轮，目前说话人已经选择 A 为下一说话人，则下一个讲话人一定是 A</li>
<li>如果当前说话人没有选择下一说话人，其他说话人可以在下一轮说话</li>
<li>如果没有其他人参加下一个话轮，当前的说话人可以接着参加下一个话轮</li>
</ul>
</li>
<li>话段切分要根据边界线索设计：<ul>
<li>线索词：倾向于出现在话段首尾</li>
<li>N 元词或词性标记序列：特定单词或词性标记序列往往预示边界所在</li>
<li>韵律</li>
</ul>
</li>
</ul>
</li>
<li>会话的共同基础<ul>
<li>对话是说话人和听话人共同的行为，共同基础就是被双方都认可的事物的集合</li>
<li>1998 年，Clark 和 Schaefer 讨论了五种接续的方法，按从弱到强为：<ul>
<li>继续关注</li>
<li>相关邻接贡献</li>
<li>确认</li>
<li>表明</li>
<li>展示</li>
</ul>
</li>
<li>对会话的理解不仅仅是字面意义：说话人交流的信息大于话段单词的字面所给出的信息</li>
</ul>
</li>
<li>会话隐含<ul>
<li>会话隐含意味着在会话中对允许的推理需要进行特殊分类。Grice 提出听话人之所以能够推出结论，是因为会话都需要遵循一套普遍准则，这些通用的启发式准则在会话话段的解释中起指导作用。</li>
<li>四个普遍准则：<ul>
<li>数量准则：提供与需求正好一致的信息（一定不要提供多于需求的信息）</li>
<li>质量准则：尽可能提供真实的信息（不要提供你认为虚假或缺乏足够证据的信息）</li>
<li>相关准则：提供切题的信息</li>
<li>方式准则：提供清楚的信息（避免模糊表达；避免歧义；简短，避免啰嗦；有序）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="对话行为多层置标语言"><a href="#对话行为多层置标语言" class="headerlink" title="对话行为多层置标语言"></a>对话行为多层置标语言</h3><p>对话行为（或会话行动）建立在奥斯汀和塞尔的言语行为理论基础之上，是被丰富了的言语行为。对话行为多层置标语言（Dialogue Act Markup in Several Layers，DAMSL）是一个对话行为标注方案，包括向前功能和向后功能，是对言语行为的扩展，扩展了毗邻对这样的对话结构概念以及对话共同基础和对话修复的概念。</p>
<p>如何确定一个给定的输入是 QUESTION，STATEMENT，还是 SUGGEST，还是 ACKNOWLEDGEMENT 呢？</p>
<ul>
<li>一种解决方法是使用连续的习语集。但有个问题是：实施一个间接请求的方式可能有很多种，而每种方式表面的语法结构可能会有细微的差别，于是不得不把 REQUEST 的意义添加给许多不同的表达。</li>
<li>另一种方法是使用推理。通过推理方法解释对话行为有两种模型：基于计划推理和基于习语提示。</li>
</ul>
<h3 id="BDI-模型"><a href="#BDI-模型" class="headerlink" title="BDI 模型"></a>BDI 模型</h3><p>Gordon 和 Lakoff 以及 Searle 最早提出，他们注意到存在一种对应于事件类型的结构，通过这些事件类型说话人可以发出间接请求。特别是，说话人可以提及或询问所期望行为的各种非常特定的属性以发出间接请求。</p>
<p>关于<strong>推理方法的建模问题</strong>，Allen，Cohen 和 Perrault 提出了 ”信念-期望-意图模型“（Belief，Desire and Intention Model，简称 BDI 模型），一个建立在言语行为理论基础上的语用自动处理形式模型。BDI 对信念和期望的形式化定义如下：</p>
<ul>
<li>信念<ul>
<li>把 ”S 相信命题 P“ 表示为二元谓词 B(S, P)，B 即为信念。</li>
<li>知识被看成 ”真的信念“，知识就是一种信念：S 具有知识就等于 S 知道某种命题，这样就可以用 ”知道“ 描述信念。”S 知道命题 P“ 表示为 KNOW(S, P)，含义是 ”存在某个命题 P，并且 S 相信 P“。即：KNOW(S, P) ＝ PΛB(S, P)。</li>
<li>”知道是不是“ 定义为：KNOWIF(S, P) = KNOW(S, P) V KNOW(S, !P)</li>
</ul>
</li>
<li>期望<ul>
<li>用 WANT 或 W 定义，”S 期望 P 为真“ 可定义为：WANT(S, P)，P 可以是行为的状态或实施动作，如 W(S, ACT(H)) 表示 ”S 想要让 H 实施行为 ACT“。</li>
</ul>
</li>
</ul>
<p>BDI 需要对行为和计划进行形式化描述，因此需要使用公理化方案，最简单的公理化方案要根据 ”行动方案“ 来建立。每个行动方案都有一个参数集，包括对于每个变量类型的 ”约束“ 以及行为的 ”前提“、”效果“ 和 ”实体“。这样的行动方案既是 ”以言表意“ 的言语行为，也是 ”以言行事“ 和 ”以言取效“ 的言语行为。这是 ”言语行为理论“ 的形式化描述。详见 P538 的例子（包括 INFORM，INFORMIF 和 REQUEST 三种与间接请求有关的言语行为方案的形式化定义）。</p>
<ul>
<li>前提（precondition）：表示成功地实施某种言语行为必须为真的那些条件</li>
<li>效果（effect）：表示成功地实施某种言语行为之后结果变为真的那些条件</li>
<li>实体（body）：表示在实施某种言语行为的过程中，必须达到的部分有序的目标</li>
</ul>
<h3 id="基于计划推理"><a href="#基于计划推理" class="headerlink" title="基于计划推理"></a>基于计划推理</h3><p>以上定义只涉及 ”表面层的行为“，还需要推导出隐藏在 ”字面意义“ 之后的 ”真实意义“。推导过程中的推理链是一种 ”貌似合理的推理链“，采用基于 ”计划推理“ 的启发式规则来实现。具体规则如下：</p>
<ul>
<li>行为-效果规则（Action-Effect，PI.AE）：对所有行为人 S 和 H，如果 Y 是行为 X 的效果，并且如果 H 相信 S 想实施 X，则 H 相信 S 想获得 Y 是合理的</li>
<li>前提-行为规则（Precondition-Action，PI.PA）：对所有行为人 S 和 H，如果 X 是行为 Y 的前提，并且如果 H 相信 S 想获得 X，则 H 相信 S 想实施 Y 是合理的</li>
<li>实体-行为规则（Body-Action，PI.BA）：对所有行为人 S 和 H，如果 X 是 Y 的实体的一部分，并且如果 H 相信 S 想实施 X，则 H 相信 S 想实施 Y 是合理的</li>
<li>知道-期望规则（Know-Desire，PI.KD）：对所有行为人 S 和 H，如果 H 相信 S 想 KNOWIF(P)，则 H 相信 S 相认 P 为真</li>
<li>扩展推理规则（Extended Inference，EI）：如果 B(H, W(S, X)) =&gt; B(H, W(S, Y)) 是一个 PI 规则，那么 B(H, W(S, B(H, (W(S, X))))) =&gt;B(H, W(S, B(H, (W(S, Y))))) 也是一个 PI 规则。意味着可以把 B(H, W(S)) 放在任何计划推理规则之前。</li>
</ul>
<h3 id="基于习语提示"><a href="#基于习语提示" class="headerlink" title="基于习语提示"></a>基于习语提示</h3><p>计划推理虽然强大，但耗费太高，实际上是 AI 完全问题。对许多应用来说，较简单的数据驱动方法就能满足需求，其中一种就是基于习语提示的对话行为解释模型。基于提示的模型之间的不同在于辨别对话行为时采用提示知识源的不同，比如可以采用词汇、搭配、句法、韵律或对话结构等不同的提示知识源。</p>
<blockquote>
<p>其实就是我们平时用的有监督模型，准确来说是分类模型。</p>
</blockquote>
<p>比如 Jurafsky 等的对话行为解释系统使用了如下信息源：</p>
<ul>
<li>单词和搭配：如 please 或 would you 是 RQUEST 的有效提示</li>
<li>韵律：比如上升音高 YES-NO-QUESTION（是非疑问句）是有效提示</li>
<li>会话结构：比如 SUGGEST 之后的 yeah 可能是 AGREEMENNT</li>
</ul>
<h3 id="意图方法"><a href="#意图方法" class="headerlink" title="意图方法"></a>意图方法</h3><p>可以把意图方法看成是 BDI 模型的一个组成部分。根据意图方法，话段被理解为言语行为，需要听话人推测基于计划的说话人的意图，这是确立连贯关系的基础。意图方法主要应用于研究对话。</p>
<p>Grosz 和 Sidner 认为话语可以表示为三个相互关联的部分：语言结构、关注状态、意图结构。话语是这三个部分组成的混合体。</p>
<ul>
<li>语言结构包括话语中的话段，分成话语片断的层级结构。</li>
<li>关注状态是话语在每一点具有显著性的对象、属性和关系的动态变化的模型</li>
<li>意图结构的基本思想是把话语与该话语行为的发起人所持有的潜在目的联系在一起，这个目的称为 ”话语目的“（DP）；话语中每个话语片断也有一个相应的目的，称为 ”话语片断目的“（DSP）。</li>
</ul>
<p>Grosz 和 Sidner 给出的一些可能的 DP/DSP：</p>
<ul>
<li>某些行为人企图实施某些实际任务的意图</li>
<li>某些行为人相信某些事实的意图</li>
<li>某些行为人相信一个事实支持另外一个事实的意图</li>
<li>某些行为人企图识别一个对象（物理对象、虚构对象、计划、事件和事件序列）的意图</li>
<li>某些行为人知道一个对象的某些属性的意图</li>
</ul>
<p>Grosz 和 Sidner 只用了两种连贯关系：支配、优先满足。如果满足 DSP2 的目的是为 DSP1 的满足提供部分基础，就说 DSP1 支配 DSP2；如果 DSP1 必须在 DSP2 之前被满足，就说 DSP1 优先满足于 DSP2。</p>
<p>对话可以据此导出为话语结构，话语片断分别与意图联系，形成 ”意图集“。意图集及它们之间的关系要根据它们在被推理出的说话人在整个计划中所充当的角色，才能够形成连贯的话语。</p>
<p>辅助话语片断也叫 ”子对话“：</p>
<ul>
<li>其中 ”知识前提子对话“ 是被行动者发起以帮助实现满足更高层目标的前提，也叫信息共享子对话。</li>
<li>另一种类型是 ”修正子对话“，又叫 ”磋商子对话“。</li>
</ul>
<p>对话中意图结构的推理算法与对话行为的推理算法很相似，很多都是 BDI 模型的变体。</p>
<p>除了意图连贯还有信息连贯，意图连贯取决于对话参与者识别彼此的意图并使这些意图适合于他们的计划的能力；信息连贯取决于确立话段之间内容所承担的某些类型关系的能力。Moore 和 Pollack 认为意图连贯和信息连贯两个层次的分析必须共存，它们相辅相成。</p>
<h3 id="会话智能代理系统"><a href="#会话智能代理系统" class="headerlink" title="会话智能代理系统"></a>会话智能代理系统</h3><p>最简单的对话管理组件的架构是基于有限状态自动机的。许多基于语音的问答系统都是以用于旅行航线的计划 GUS 系统为基础的，并引入了基于框架或模板的较新的 ATIS 系统以及其他的旅行和饭店向导系统。基于模板的系统本质上是一个生成规则的系统，不同类型的输入可以激发不同的生成规则，每个生成能够灵活地填入不同的模板。生成规则能够基于一些因素进行转换控制，这些因素包括用户的输入和一些简单的对话历史。</p>
<p>数理逻辑学家 R. Carnap 在 1942 年提出要区分语言研究的三个领域：</p>
<ul>
<li>如果在一种语言中，明确地指称涉及说话人（或语言的使用者），可归于语用学领域</li>
<li>如果抽去语言的使用者，仅仅分析语言的表达形式及其所指，就处于语义学领域</li>
<li>如果再抽去所指，仅仅分析语言表达形式之间的关系，就处于逻辑句法学的领域</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>Mann 和 Thompson 的修辞结构理论<ul>
<li>1987 年，W. Mann 和 S. Thompson 提出修辞结构理论（RST），一种基于文本局部之间关系的关于文本组织的描述理论。</li>
<li>RST 的核心是修辞关系的概念，修辞关系是存在于两个互不重叠的文本跨段之间的关系，一个叫 “核心单元”，一个叫 “卫星单元”，这种区分来自经验观察。</li>
<li>他们总结出了 25 种修辞关系，分为 “核心-卫星关系”（21 种）和 “多核心关系”（4 种），分别表示为 N-S 和 N-N(…N)。</li>
<li>用来描写文本结构的 RST 只辨识结构的三种主要类型：整体结构、关系结构和句法结构，并主要研究中间一层的关系结构。</li>
<li>RST 提出了文本的树结构模型，从根节点开始的树形图可以代表整个文本的修辞关系结构。他们认为对文本的一个典型分析是应用一套图式使下列限制成立：<ul>
<li>完整性：有一个图式（根节点）可以覆盖整个文本</li>
<li>联系性：除根节点外，每一个分析中的文本跨段要么是一个最小单元，要么是分析中另一个图式应用的一个成分</li>
<li>唯一性：每一个图式应用都涉及不同的一套文本跨段</li>
<li>邻接性</li>
</ul>
</li>
<li>RST 文本分析一般采用自底向上剖析过程，它提供了一种讨论书面独白文本的联系性和整体性，以及文本如何为作者的目的服务的方法。从渊源上，它采用的是语用功能主义思想，受 Halliday 系统功能语法影响很大。</li>
<li>RST 并不具有显著的跨语言可转移性，它的优点是提供了完整的分析而不是选择性解释，可以应用于很多种不同的文本，允许不考虑语类对文本结构做一种统一的描写，有助于区分文本中那些真正是语类特殊的方面与那些相对而言更独立于语类的方面。不足包括：没有对各种关系如何实现做出系统描写；没有将它的理论与各种文本特性的理论（如信息流、主题结构、词汇结构等）联系起来。</li>
</ul>
</li>
<li>文本连贯中的常识推理技术<ul>
<li>语言通常并不是由孤立无关的句子组成，而是由搭配在一起的相关句子群组成的，这种句子群称为  ”话语“</li>
<li>话语的话段之间所有可能的连接称为连贯关系的集合。常见的连贯关系有结果、说明、平行、详述和时机</li>
<li>每个连贯关系与一个或多个约束有关，符合约束才能维持连贯关系。如何应用这些约束？推理，语言系统由于所依赖的推理是不可靠的，这类推理方法称为 ”溯因推理“（从结果中寻找可能的原因）。</li>
<li>在确定话段间最合理的连贯关系时，需要利用世界知识和领域知识，但这是个 AI 完全问题，因为需要人类拥有的所有知识。</li>
<li>句子间连贯关系导致话语结构，一组局部连贯话段的节点被称为 ”话语片断“。话语结构的计算所涉及的 ”话语语法“ 只牵扯两个规则：把一个片断改写为两个较小的片断的规则，以及判断一个句子就是一个片断的规则。</li>
</ul>
</li>
<li>言语行为理论和会话智能代理<ul>
<li>哲学基础：维特根西坦、奥斯汀、塞尔</li>
<li>会话分析与书面独白三个不同的重要特性：话轮转换、会话的共同基础和会话中的隐涵。</li>
<li>对话行为（或会话行动）建立在奥斯汀和塞尔的言语行为理论基础之上，是被丰富了的言语行为。对话行为多层置标语言是一个对话行为标注方案，包括向前功能和向后功能，是对言语行为的扩展，扩展了毗邻对这样的对话结构概念以及对话共同基础和对话修复的概念。</li>
<li>通过推理方法解释对话行为有两种模型：基于计划推理和基于习语提示。</li>
<li>关于推理方法的建模问题，Allen，Cohen 和 Perrault 提出了 ”信念-期望-意图模型“（Belief，Desire and Intention Model，简称 BDI 模型），一个建立在言语行为理论基础上的语用自动处理形式模型。BDI 需要对行为和计划进行形式化描述，因此需要使用公理化方案，最简单的公理化方案要根据 ”行动方案“ 来建立。每个行动方案都有一个参数集，包括对于每个变量类型的 ”约束“ 以及行为的 ”前提“、”效果“ 和 ”实体“。这样的行动方案既是 ”以言表意“ 的言语行为，也是 ”以言行事“ 和 ”以言取效“ 的言语行为。这是 ”言语行为理论“ 的形式化描述。</li>
<li>除此涉及 ”表面层的行为“ 之外（通过 BDI），还需要推导出隐藏在 ”字面意义“ 之后的 ”真实意义“。推导过程中的推理链是一种 ”貌似合理的推理链“，采用基于 ”计划推理“ 的启发式规则来实现。具体规则包括：行为-效果规则（Action-Effect，PI.AE）、前提-行为规则（Precondition-Action，PI.PA）、实体-行为规则（Body-Action，PI.BA）、知道-期望规则（Know-Desire，PI.KD）和扩展推理规则（Extended Inference，EI）。</li>
<li>计划推理虽然强大，但耗费太高，实际上是 AI 完全问题。对许多应用来说，较简单的数据驱动方法就能满足需求，其中一种就是基于习语提示的对话行为解释模型。基于提示的模型之间的不同在于辨别对话行为时采用提示知识源的不同，比如可以采用词汇、搭配、句法、韵律或对话结构等不同的提示知识源。</li>
<li>可以把意图方法看成是 BDI 模型的一个组成部分。根据意图方法，话段被理解为言语行为，需要听话人推测基于计划的说话人的意图，这是确立连贯关系的基础。意图方法主要应用于研究对话。Grosz 和 Sidner 认为话语可以表示为三个相互关联的部分：语言结构、关注状态、意图结构。话语是这三个部分组成的混合体。Grosz 和 Sidner 只用了两种连贯关系：支配、优先满足，对话可以据此导出为话语结构，话语片断分别与意图联系，形成 ”意图集“。意图集及它们之间的关系要根据它们在被推理出的说话人在整个计划中所充当的角色，才能够形成连贯的话语。</li>
<li>会话智能代理系统主要基于有限状态自动机和基于模板的，都有很明显的局限性。</li>
</ul>
</li>
</ul>
<p>本章主要介绍了语用自动处理的形式模型，主要有两个模型：RST 和 BDI。</p>
<p>第三节通过会话智能代理系统说明 BDI 的使用，我的理解是基于 DAMSL，BD+ 计划推理是一类，习语提示是一类，I（意图）+ BD是一类。第一类 BD 主要负责字面意义，计划推理负责真实意义；第二类直接通过有监督学习确定；第三类意图为核心，BD 起辅助作用。（注：BD 实际指基于它的行动方案）。</p>
<p>理解不一定正确，最后一节内容感觉有点混乱。感觉各种形式模型貌似都是理论上看起来不错，实际操作还是有监督学习，虽然简单粗暴但效果并不差。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/02/27/NLPFA/2019-02-27-Ch10-Formal-Model-of-Pragmatic-Automatic-Processing/">
    <time datetime="2019-02-27T03:32:00.000Z" class="entry-date">
        2019-02-27
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Model/">Formal Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-02-21-Ch09-System-Function-Syntax" class="post-NLPFA/2019-02-21-Ch09-System-Function-Syntax post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/02/21/NLPFA/2019-02-21-Ch09-System-Function-Syntax/">自然语言计算机形式分析的理论与方法笔记(Ch09)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/02/21/NLPFA/2019-02-21-Ch09-System-Function-Syntax/" data-id="cju9u9zbm00235occ5evy6xhe" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第九章：系统功能语法"><a href="#第九章：系统功能语法" class="headerlink" title="第九章：系统功能语法"></a>第九章：系统功能语法</h1><h2 id="系统功能语法的基本概念"><a href="#系统功能语法的基本概念" class="headerlink" title="系统功能语法的基本概念"></a>系统功能语法的基本概念</h2><p>英国语言学家 M. A. K. Halliday 提出，他继承并发扬了他的老师 Firth 为代表的伦敦语言学派的功能主义理论。</p>
<p>Firth 的功能主义语言理论要点：</p>
<ol>
<li><p>语言除了具有语言内部的上下文之外，还具有情景上下文</p>
<p> Manlinowski 的观点：语言根据社会的特定要求进化，性质及使用都反映了该社会的具体特性，语言的环境对于理解语言必不可少。只有在 “文化上下文”（说话者生活在其中的社会文化），尤其只能在 “情境上下文”（说话时实际发生的事情） 中才能对一段话语的意义做出评估。</p>
<p> Firth 对 “情境上下文” 做了更确切的定义，认为语言行为包括三个方面范畴：</p>
<ul>
<li>参与者的有关特征：哪些人，有什么样的人格，什么样的有关特征<ul>
<li>参与者的言语行为</li>
<li>参与者的言语行为之外的行为</li>
</ul>
</li>
<li>有关的事物和非语言性、非人格性的事件</li>
<li><p>语言行为的效果</p>
<p>他认为要把语言作为一种 “社会过程” 来看，即语言是 “人类生活的一种形式，并非仅仅是一套约定俗成的符号和记号”，所以要提出各种 “限制性语言”（人们按各自的身份说出来的得体的话）。</p>
</li>
</ul>
</li>
<li><p>语言既有情景意义，又有形式意义</p>
<p> 语言学的目的是说明意义，意义分两种：情景意义和形式意义。后者是 Firth 受 Ferdinand de Sausurre（索绪尔）关于语言符号具有价值这一观点的启发而提出。形式意义可表现在三个层上：搭配层、语法层、语音层。</p>
</li>
<li><p>语言有结构和系统两个方面</p>
<ul>
<li><p>结构是语言成分的组合性排列，横向的</p>
</li>
<li><p>系统是一组能在结构里的一个位置上互相替换的 “类聚性单位”，纵向的</p>
<p>语法、语音和搭配层都存在结构和系统，详见 P484 的例子。</p>
</li>
</ul>
</li>
<li><p>音位的多系统理论和跨音段理论</p>
<ul>
<li>在音位学中的系统就是在某个结构中的一个位置所能出现的若干个可以互换的语音的总称。多系统分析法建立两个辅音系统：词首和词末辅音系统。</li>
<li>在一种语言里，区别性语音特征不能都归纳在一个音段位置上。跨音段成分可以横跨一个音节的一部分，或整个音节，或一个词，或一个短语，或一个句子。跨音段成分除了语调，还有音高、音强、音长、元音性、软腭性等。音位单位减去跨音段成分留下来的是 “准音位单位”。</li>
</ul>
</li>
</ol>
<p>Halliday 的系统功能语法主要在三个方面继承和发展了 Firth 的功能主义学说：</p>
<ul>
<li><p>发展了 “情境上下文” 理论，提出了 “语域” 概念</p>
<p>  语言的情境由场景、方式和交际者三部分组成：</p>
<ul>
<li>场景是话语在其中行使功能的整个事件，以及说话者的目的，包括话语的主题</li>
<li>方式是事件中话语的功能，包括语言采用的渠道（临时或有准备的说或写），以及风格或修辞手段（叙述、说教、劝导、应酬）</li>
<li><p>交际者指交集中的角色类型，即话语的参与者之间的一套永久性或暂时性的相应的社会关系</p>
<p>语言的语义分为概念功能、人际功能和语篇功能：</p>
</li>
<li><p>概念功能表示说话的内容，分为经验功能（与说话的内容发生关系，是说话者对外部环境反映的再现，是说话者关于各种现象的外部世界和自我意识的内部世界的经验）和逻辑功能（间接地从经验中取得抽象的逻辑关系的表达）。</p>
</li>
<li>人际功能是一种角色关系，涉及说话者在语境中所充当的角色以及给其他参与者所分派的角色。</li>
<li><p>语篇功能使说话者所说的话在语言环境中起作用，反映语言使用前后连贯的需要。是一种给予效力的功能，没有它前两种功能无法实现。</p>
<p>当语言情境特征反映到语言结构中时：</p>
<p><img src="http://qnimg.lovevivian.cn/book-2017-fengzhiwei-10.jpeg" alt=""></p>
<p>语域是语言使用中由于语言环境的改变而引起的语言变异。语言环境的场景、交际者和方式三个组成部分都可以产生新的语域。</p>
</li>
<li><p>场景不同产生科技英语、非科技英语等，<strong>差异主要表现在词汇、及物关系和语言各结构等级上逻辑关系的不同</strong></p>
</li>
<li>交际者不同产生正式英语、非正式英语及介于二者之间的等，<strong>差异主要表现在语气、情态以及单词中所表达的说话者的态度的不同</strong></li>
<li><p>方式不同产生口头英语和书面英语等，差异主要表现在句题结构（主题、述题）、信息结构（新信息、旧信息）和连贯情况（如参照、替代、省略、连接等）的不同</p>
<p>实际的语域变异通常不是由一种语言环境因素的改变而引起的，三种类型的变化共同作用的结果便产生了各式各样的语域，<strong>所谓语言，只不过是一个高度抽象化的概念。</strong></p>
</li>
</ul>
</li>
<li><p>发展了关于 “结构” 和 “系统” 的理论</p>
<p>  从系统和功能两大角度研究语言。Firth 认为 “系统或选择是在语言的结构内部进行，因而结构是第一性的”。Halliday 提出 “系统的概念适用于级的自上而下的各个层次，在语言深层中存在的是系统而不是结构”，主张 “系统存在于所有层次，于是从 ’阶‘ 和 ’范畴‘ 的语法 ” 过渡到 “系统语法”。他提出的系统语法理论包括四个范畴：</p>
<ul>
<li><p>单位：语言的单位形成一个层级体系，同时又是一个分类体系；每个单位包含一个或多个紧跟在它下面的单位。如英语中的单位：句子、小句、词组、词和语素，一个单位的级就是在层级体系中的位置。</p>
</li>
<li><p>结构：为了说明连续事实间的相似性而设立的范畴叫作 “结构”。结构是符号的线性排列，结构中的每个单位，由一个或多个比它低一级的单位组成。如小句由主语、谓语、补语和附加语组成。</p>
</li>
<li><p>类别：一定单位的一群成员，根据它们在上一级单位结构中的作用，可以定出它们的 “类别”。如词组可定出名词词组（可用于小句中主语和补语）、动词词组（可用于小句中谓语）、副词词组（可用于小句中附加语）等。</p>
<p>  类别和结构都同单位相连，类别始终是一定单位的成员的类别。类别和结构关系一般不变，类别总是按照上一级单位的结构来定；结构总是按照下一级单位的类别来定。</p>
</li>
<li><p>系统：由一组特点组成的网。如果进入该系统的条件得到满足，就选出且只选出一个特点。从外部形式上看，就是一份可供说话者有效地进行选择的清单。</p>
<p>  系统存在于所有的语言层（语义、语法、音位），它们都有各自的系统来表示本层次的语义潜势。</p>
<p>从系统与法的观点看，言语行为就是从数量巨大、彼此有关、可供选择的各种成分中，同时进行选择的过程。美国人工智能专家维诺拉德在 1974 年研制了 SHRDLU。</p>
</li>
</ul>
</li>
<li><p>提出了语法分析的三个尺度——级、幂、细度</p>
<p>  Halliday 把语法分析的尺度叫作阶，为了把范畴相互联系起来要采用三种抽象的阶：级、幂、细度。</p>
<ul>
<li>级的阶上，排列着从句子到语素的各层单位，按逻辑顺序从最高单位排列到最低单位。句子的描写只有当语素的描写完备后才是完备的，反之亦然。</li>
<li>幂的阶是抽象程度的阶梯，它把语法中的概念同实际材料联系起来。从比较抽象的概念向具体的材料推进，就是沿着幂的阶下降。</li>
<li><p>细度的阶则反映结构和类别的细分程度。细度是一个渐进系，是潜在的带有无限分度的连续体。它的范围，一头是结构和类别两大范畴中的基本程度，另一头是理论上这样的一个点，过了这个点就得不出新的语法关系。</p>
<p>Halliday 认为对一个语言项目进行分类时，应该按照细度的阶，由一般逐步趋向特殊，对每一个选择点上的可选项给以近似值。在每一个选择点上，可选项的选择要考虑概率，进一步细分时，对多重或交叉的标准，要给不同的参考值适当调整。</p>
</li>
</ul>
</li>
</ul>
<p>系统功能语法，除了研究语言符号系统的构成及其内部各个子系统，以及这些子系统运作的方式外，还研究语言在使用过程中所发挥的作用，以及如何发挥这些作用，包括 “系统语法” 和 “功能语法”：</p>
<ul>
<li>系统语法着重说明：语言作为系统的内部底层关系，它是与意义相关联的可供人们不断选择的若干个子系统组成的系统网络，又称 “意义潜势”。语言在表述说话人的语义时，必然要在语言的各个语义功能部分进行选择。</li>
<li>功能语法着重说明：语言是社会交往的工具，语言系统是人们长期交往中为了实现各种不同的语义功能而逐渐形成起来的；人们在交往中需要在语言系统中选择时，也是根据所要实现的功能进行有动因的活动。</li>
</ul>
<p>Halliday 系统功能语法的核心思想：</p>
<ul>
<li><p>元功能的思想</p>
<p>  Halliday 认为语言的性质决定人们对语言的要求，即语言所必须完成的功能。这种功能具有无限可能，但其中有若干个有限的抽象功能是语言本身所固有的，这就是 “元功能”。三种元功能：</p>
<ul>
<li>概念元功能：包括经验功能或关于所说 “内容” 的功能和逻辑功能，与表达的 “命题内容” 有关</li>
<li>人际元功能：由建立和维护说话人与听话人之间的交互关系的那些功能组成。能反映人与人不同的地位与关系。</li>
<li>语篇元功能：与适合于当前话语的表达方式有关。包括主题化以及所指等问题。</li>
</ul>
</li>
<li><p>系统的思想</p>
<p>  Halliday 不同意 Sausurre 等把语言仅看成一套符号的集合，他认为：</p>
<ul>
<li>对语言的解释要用有意义的有规则的源泉（意义潜势）解释，语言并不是所有合乎语法的句子集合</li>
<li>结构是过程的底层关系，是从潜势中衍生的，潜势可以更好地用聚合关系表达，语言系统是一种可进行语义选择的网络，当有关系统的每个步骤一一实现后，就可以产生结构</li>
<li>系统存在于所有语言层次中，各个层次系统有表示自己层次的意义潜势</li>
</ul>
</li>
<li><p>层次的思想</p>
<p>  Halliday 认为语言是一种多层次的系统结构，包括内容、表达和实体三个层次，各层次间相互联系。</p>
<ul>
<li>语言是有层次的，至少包括语义层、词汇语法层和音系层</li>
<li>各层次间存在 “实现” 关系，“意义（语义层）” 的选择体现于对 “形式（词汇语法层）” 的选择，进而体现于对 “实体（音系层）” 的选择</li>
<li>语言系统是一个多重代码系统，可由一个系统代入另一个系统，然后再代入其他系统</li>
<li>采用层次概念可以使人们对语言的理解扩展到语言外部，语义层实际上是语言系统对语境即行为层或社会符号层的体现</li>
</ul>
</li>
<li><p>功能的思想</p>
<p>  Halliday 的功能思想属于语义概念，这里的功能是形式化的意义潜势的离散部分，是构成一个语义系统的起具体作用的语义成分，词汇语法的成分或结构只是它的表达格式。</p>
</li>
<li><p>语境的思想</p>
<p>  Halliday 认为如果人们把语言当作一个整体来看，就必须从外部来确定区别语义系统的标准，即依靠语境确定属于同一语义类型的语言材料是否具有同一意义的标记。社会语境也是语义的一部分。</p>
</li>
<li><p>近似或概率的思想</p>
<ul>
<li>语言固有的特征之一是概率性，在人们选择词汇时表现的最为明显</li>
<li>人们只能从相对概率来掌握语言的使用范围，将该原则推广到语法系统的描写时，各种句型的使用也有一个概率的问题，要掌握不同形式语言项目的使用，必须精确区别语义与特定情景语境的关系</li>
<li>不同语域之间的差别可能就是它们在词汇语法层面上的概率不同而形成的，这种概率与所要表达的不同语义的确切程度有关</li>
</ul>
</li>
</ul>
<h2 id="系统功能语法在自然语言处理中的应用"><a href="#系统功能语法在自然语言处理中的应用" class="headerlink" title="系统功能语法在自然语言处理中的应用"></a>系统功能语法在自然语言处理中的应用</h2><p>在 NLP 中，采用包含 “与/或” 逻辑关系的非循环有向图表示语法，这样的语法称为 “系统网络”。系统功能语法采用 “实现语句” 建立语法指定的特征（如指示语、祈使句）与句法形式之间的映射。网络中每个特征都具有一个实现语句集，并据此指定对最终表达形式的约束。采用的运算符号包括：</p>
<ul>
<li>+X：插入功能 X</li>
<li>X=Y：合并功能 X 和 Y</li>
<li>X&gt;Y：将功能 X 置于功能 Y 之前的某一位置</li>
<li>X/A：将功能 X 与词汇或语法特征 A 划为一类</li>
</ul>
<p>对一个给定的系统网络，生成的处理程序是：</p>
<ul>
<li>从左到右遍历网络，选择正确的特征并执行相关的实现语句</li>
<li>建立中间表示，该表示满足遍历期间执行的实现语句所施加的约束</li>
<li>对于任何没有完全指定的功能通过在较下层递归调用该语法而加以指定</li>
</ul>
<p>简单来说就是将分析得到的 “谓词—论元” 结构通过系统网络得到概念意义、人际意义和语篇意义三个不同层次的功能结构，然后递归执行直到完全指定短语、词典项以及词形，从而生成句子。</p>
<p>系统功能语法把语言看作在上下文中表示意义的资源，把句子表示为功能的集合以及这些功能与外在的语法形式之间的映射规则。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>基本概念<ul>
<li>英国语言学家 M. A. K. Halliday 提出，他继承并发扬了他的老师 Firth 为代表的伦敦语言学派的功能主义理论。</li>
<li>Halliday 的系统功能语法主要在三个方面继承和发展了 Firth 的功能主义学说，其核心思想是：<ul>
<li>元功能的思想：Halliday 认为语言的性质决定人们对语言的要求，即语言所必须完成的功能。这种功能具有无限可能，但其中有若干个有限的抽象功能是语言本身所固有的，这就是 “元功能”，包括概念元功能、人际元功能和语篇元功能。</li>
<li>系统的思想：Halliday 认为对语言的解释要用有意义的有规则的源泉（意义潜势）解释，语言并不是所有合乎语法的句子集合，语言系统是一种可进行语义选择的网络，当有关系统的每个步骤一一实现后，就可以产生结构。结构是过程的底层关系，是从潜势中衍生的，潜势可以更好地用聚合关系表达。系统存在于所有语言层次中，各个层次系统有表示自己层次的意义潜势。</li>
<li>层次的思想：Halliday 认为语言是一种多层次的系统结构，包括内容、表达和实体三个层次，各层次间相互联系。</li>
<li>功能的思想：Halliday 的功能思想属于语义概念，这里的功能是形式化的意义潜势的离散部分，是构成一个语义系统的起具体作用的语义成分，词汇语法的成分或结构只是它的表达格式。</li>
<li>语境的思想：Halliday 认为如果人们把语言当作一个整体来看，就必须从外部来确定区别语义系统的标准，即依靠语境确定属于同一语义类型的语言材料是否具有同一意义的标记。社会语境也是语义的一部分。</li>
<li>近似或概率的思想：语言固有的特征之一是概率性，在人们选择词汇时表现的最为明显，各种句型的使用也有一个概率的问题，要掌握不同形式语言项目的使用，必须精确区别语义与特定情景语境的关系。不同语域之间的差别可能就是它们在词汇语法层面上的概率不同而形成的，这种概率与所要表达的不同语义的确切程度有关。</li>
</ul>
</li>
</ul>
</li>
<li>应用<ul>
<li>系统功能语法采用 “实现语句” 建立语法指定的特征（如指示语、祈使句）与句法形式之间的映射。网络中每个特征都具有一个实现语句集，并据此指定对最终表达形式的约束。</li>
<li>对一个给定的系统网络，生成的处理程序是：<ul>
<li>从左到右遍历网络，选择正确的特征并执行相关的实现语句</li>
<li>建立中间表示，该表示满足遍历期间执行的实现语句所施加的约束</li>
<li>对于任何没有完全指定的功能通过在较下层递归调用该语法而加以指定</li>
</ul>
</li>
<li>系统功能语法把语言看作在上下文中表示意义的资源，把句子表示为功能的集合以及这些功能与外在的语法形式之间的映射规则。</li>
</ul>
</li>
</ul>
<p>本章主要介绍了韩礼德的系统功能语法及其应用，篇幅虽然短小，但其思想却发人深省。就我个人而言甚至觉得它超过之前所有的理论。它的核心思想和中间表达非常符合人类认知思维，应用中的难点在于系统网络的构建。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/02/21/NLPFA/2019-02-21-Ch09-System-Function-Syntax/">
    <time datetime="2019-02-21T03:32:00.000Z" class="entry-date">
        2019-02-21
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-02-15-Ch08-Formal-Model-of-Semantic-Automatic-Processing" class="post-NLPFA/2019-02-15-Ch08-Formal-Model-of-Semantic-Automatic-Processing post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/02/15/NLPFA/2019-02-15-Ch08-Formal-Model-of-Semantic-Automatic-Processing/">自然语言计算机形式分析的理论与方法笔记(Ch08)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/02/15/NLPFA/2019-02-15-Ch08-Formal-Model-of-Semantic-Automatic-Processing/" data-id="cju9u9zc7003a5occucuq4flz" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第八章：语义自动处理的形式模型"><a href="#第八章：语义自动处理的形式模型" class="headerlink" title="第八章：语义自动处理的形式模型"></a>第八章：语义自动处理的形式模型</h1><p>关于语义与语法分析的关系，有两种方式：先句法后语义和句法语义一体化。</p>
<h2 id="义素分析法"><a href="#义素分析法" class="headerlink" title="义素分析法"></a>义素分析法</h2><p>20 世纪 40 年代初期，结构主义丹麦学派代表人数 L. Hjelmslev 提出义素分析法的设想；20 世纪 50 年代，美国人类学家 F. G. Lounsbury 和 W. H. Goodenough 在研究亲属词的含义时提出了义素分析法；20 世纪 60 年代初，美国语言学家 J. J. Katz 和 J. A. Fodor 提出了解释语义学，将义素分析法引入语言学，为生成转换语法提供语义特征。</p>
<p>义素是意义的基本要素，它就是词的理性意义的区别特征。词的理性意义是一束语义特征（义素）的总和。一组词的义素可以用义素矩阵表示，纵坐标表示词，横坐标表示义素，纵横坐标相交点注以 + 或 - 号或其他表示方法。</p>
<p>采用义素分析法建造机器词典可以解决利用辞典直接存储每个词意义（义项）的问题（占用空间大；难于判断别同义词、近义词在意义上的差别；难以确定词与词之间的搭配关系）：</p>
<ul>
<li>机器词典中词条以义素存储，可以使用较少的义素对大量的词义做形式化描述</li>
<li>通过对机器词典中不同义素集合内的各个义素的分析比较，计算机容易找出不同单词在词义上的细微差别</li>
<li>通过义素分析法，计算机可以了解词与词搭配时在语义上要受到什么样的限制</li>
</ul>
<p>隐喻在修辞学中属于一种 “辞格”，一个完整的隐喻一般由 “喻体” 和 “本体” 构成，喻体通常是我们熟悉的、比较具体直观、容易理解的一些概念范畴，本体则是我们后来才认识的、抽象的、不易理解的概念范畴。在认知语言学中，喻体叫作 “始源域”，本体叫作 “目标域”。隐喻的认知力量就在于将始源域的图式结构映射到目标域上，使人们对目标域有更清晰的认识。<strong>认知语言学认为，隐喻不但是一种修辞手段，而且还是人的一种思维方式普遍存在于人们的各种认知活动中。</strong></p>
<h2 id="语义场"><a href="#语义场" class="headerlink" title="语义场"></a>语义场</h2><p>要进行义素分析，首先要对该语言的词汇体系建立起 “语义场”。</p>
<p>1924 年，德国学者 G. Ipsen 提出该术语，20 世纪 30 年代初，德国学者 J. Trier 提出系统的语义场理论，1992 年，北大贾彦德在《汉语语义学》中系统提出了汉语的语义场理论，北京语言大学张普提出 “场型” 的概念。</p>
<p>语义场是词义形成的系统，它是基于概念的关系场，是词义与词义之间构成的一种完全虚化、非物质的空间领域。若干个意义上紧密相联的词义，通常归属于一个总称之下，就构成了语义场。</p>
<p>语义场可进一步分为词汇场和联想场。词汇场是静态的，表现为词义与词义之间的组合关系。这里的语义场主要指词汇场。</p>
<p>汉语的场型（不同类型的语义场）包括：</p>
<ul>
<li>分类场型：基本场型，一般是多层次的，特点如下：<ul>
<li>上下词义之间存在着领属关系，上位表示领域，下位表示分类</li>
<li>下位可以继承上位的基本义素</li>
</ul>
</li>
<li>构件场型：基本场型，下位是上位的构件，特点如下：<ul>
<li>上下位之间是整体和构件的关系</li>
<li>不是下位继承上位义素而是上位抽取下位的某些义素来集成</li>
</ul>
</li>
<li>有序场型：基于分类场型和构件场型的特殊场型，所有平位是有序的，特点如下：<ul>
<li>同一层次的词义排列是有序的，反应了客观世界的有序性</li>
<li>一些有序的词义是封闭型的，封闭型的词义可以循环</li>
</ul>
</li>
<li>对立场型：特殊场型，平位的词义之间存在对立关系，特点如下：<ul>
<li>一些对立场型中的平位只有两个</li>
<li>一些对立场型的平位不止两个，之间还有中间状态，这种对立叫作两极对立</li>
</ul>
</li>
<li>同义场型：特殊场型，同一场型中，同位和变位的理性意义完全相同，只是附属于理性意义的风格、色彩等方面义素不同</li>
</ul>
<p>场与场之间的关系有：</p>
<ul>
<li>嵌套关系：同一类场型之间的关系</li>
<li>交叉关系：不同场型之间的关系</li>
<li>传递关系：不同场型之间的关系</li>
<li>联想关系：不同场型之间和同一场型不同子场之间都可以产生联想关系</li>
</ul>
<h2 id="语义网络"><a href="#语义网络" class="headerlink" title="语义网络"></a>语义网络</h2><p>1968 年美国心理学家 M. R. Quillian 研究人类联想记忆时提出的，1972 年，美国人工智能专家 R. F. Simmons 和 J. Slocum 首先将语义网络用于自然语言理解系统中。1977 年，美国人工智能学者 G. Hendrix 提出了分块语义网络的思想，把语义的逻辑表示与 “格语法” 结合起来，把复杂问题分解为若干简单的子问题，每个子问题以一个语义网络表示。</p>
<p>语义网络可用有向图表示，一个语义网络就是由一些以有向图表示的三元组：（结点1，弧，结点2）连接而成的。三元组可写成二元谓词：P（个体1，个体2）</p>
<p>在人工智能中，语义网络内各概念之间的关系主要（常见的关系）由 ISA, PART-OF, IS 等谓词来表示，分别是 “具体-抽象” 关系（隶属关系）、“整体-构件” 关系（包含关系）、“一个结点是另一个结点的属性”。</p>
<p>语义网络可表示一个事件，事件由若干个概念组合所反映的客观现实，可以分为叙述性事件、描述性事件和表述性事件三种。语义网络表述事件时，结点之间的关系还可以为施事（AGENT）、受事（OPATIENT）、位置（LOCATION）、时间（TIME）等。</p>
<p>语义网络的推理机制一般基于网络的匹配。根据提出的问题构建局部网络，查询解答的过程就是查询局部网络到网络知识库的匹配操作。</p>
<p>知识图谱描述真实世界中存在的各种实体或概念，每个属性-值偶对用来刻画实体的内在特性，而关系用来连接两个实体。可以看作一个巨大的语义网络。</p>
<p>知识图谱不仅需要在数据层构建（自底向上），还需要在模式层上构建（自顶向下），模式是对知识的提炼，遵循预先给定的模式，有助于知识的标准化。</p>
<h2 id="Montague-语法"><a href="#Montague-语法" class="headerlink" title="Montague 语法"></a>Montague 语法</h2><p>Montague 语法采用内涵逻辑的方法描述句子语义内容，1970 年前后美国数理逻辑学家 R. Montague 等把内涵逻辑应用于自然语言的研究，并把生成语法与内涵逻辑这两个领域的研究集中提炼为 Montague 语法。</p>
<p>Montague 认为自然语言与高度形式化的逻辑语言没有区别，他将 Frege 原理（一个句子的整体意义是它各部分意义和组合方式的函数）中的 “意义” 扩展到 “结构”：一个句子的整体结构是它各部分的结构和组合方式的函数。</p>
<p>因此在 Montague 语法里，一个句子的句法形式、内涵逻辑表达式和语义所指都是从基本单位开始，通过句法规则、转译规则和语义规则，从小到大逐段确定的。句法、转译和语义三大部分是同态的。有一条句法规则就有一条转译规则把它处理的短语转译成内涵逻辑表达式，然后再由一条语义规则来确定这个内涵逻辑表达式的语义。歧义问题通过不同的组合方式和运用不同的句法、语义规则来解决，这是 Montague 语法的 “规则对规则假说”。</p>
<ul>
<li>句法：包括一套语类和一套句法规则。功能是把来自词库的词语组成句子。<ul>
<li>语类给基本词语规定一个句法范畴<ul>
<li>语类由基本语类 e 和 t 以及它们之间关系的一组集合，e 和 t 是基本语类，其他是派生语类。</li>
<li>语类 e 表示自然界某类事物中的个体词语或实体词语，并不等于传统语法中的名词或名词短语。</li>
<li>语类 t 表示具有真值的语言单位，叫作真值词语或陈述语句。</li>
</ul>
</li>
<li>句法规则把基本词语变成短语，然后再把较小片段短语合成较大片段短语</li>
</ul>
</li>
<li>转译：包括一套转译规则，把短语转译成内涵逻辑表达式。</li>
<li>语义：以内涵逻辑为基础建立的。内涵逻辑包括句法和语义两方面：<ul>
<li>句法：由一套义类系统和句法规则组成，主要解决内涵逻辑结构成分的结合问题。<ul>
<li>义类由对应函数从该词项的语类中求得</li>
<li>句法规则规定各种成分结合以后的义类</li>
</ul>
</li>
<li>语义：解决语义所指问题，有一套语义规则，运用这套规则可以求出内涵逻辑表达式在特定模型中的语义所指。</li>
</ul>
</li>
</ul>
<p>Montague 语法有两个来源：N. Chomsky 的生成转换语法 和 Louis 的内涵逻辑学。采用内涵逻辑学来描述句子的深层结构，在句子的每一个层次上都可得出一个相应的内涵逻辑表达式，并以此来表示该句子深层结构的逻辑含义。从 (λx…x…)a 出发得到 …a… 这一性质称为 “λ-变换”， <strong>λ-变换是 Montague 语法转译计算的关键</strong>，生成语法得出的完全相同的树形图在 λ-变换后，可以得出不同的内涵逻辑表达式。</p>
<p>Montague 通过真值条件语义学、模型论语义学和可能世界语义学（Montague 语法的语义理论的三个特点），把自然语言所表现出来的意义介入内涵逻辑学中，从而建立了 Montague 语法的语义理论。由于将句法和语义结合，使得任何一个通过句法分析得到的表示句子的句法结构的树形图，都可以用 Montague 语法解释为相应的内涵逻辑表达式，从而表现出句子的语义内容。</p>
<p>Montague 在《普通英语中量化的特定处理》中提出了 PTQ 系统用以计算句子的语义值，步骤如下：</p>
<ul>
<li>选出有限片段的英语，从中提炼出包含 9 个派生语类的词典和 17 条句法规则，根据组合原则，从最简单的成分词汇开始，逐层组合成复杂成分</li>
<li>片段英语的 17 条句法规则中的每一条都相应地对应着一条转译规则，将片段英语中的每一个语言成分转译成内涵逻辑语言中的一个内涵逻辑表达式，最后将复杂的语言表达式转译成复杂的内涵逻辑表达式</li>
<li>根据内涵逻辑的语义解释规则，将内涵逻辑表达式在给定的模型下求出其语义值，即为该片段英语表达式的一个语义解释</li>
</ul>
<h2 id="Wilks-的优选语义学"><a href="#Wilks-的优选语义学" class="headerlink" title="Wilks 的优选语义学"></a>Wilks 的优选语义学</h2><p>1874 年，Y. A. Wilks 在研制英法机器翻译系统的基础上，提出了 “优选语义学”，它共有五种语义单位：义素、义式、裸模板、模板、超模板，并有从较小的单位到较大的单位的构造规则。义素构成义式以描写单词的语义，由义式构成裸模板和模板以描述简单句的语义，再由超模板描写更大的文句单位一直到句子的语义。</p>
<ul>
<li>义素：80 个语义单元，用以表示语义实体、状态、性质和动作，分为 五大类：语义实体、动作、性状、种类、格</li>
<li>义式：由义素及左右圆括号构成，最重要的在最右端，称为义式的首部，直接或间接支配义式中其他义素。</li>
<li>裸模板：由一个行为主体义式<strong>首部</strong>、一个动作义式<strong>首部</strong>和一个客体义式<strong>首部</strong>组成的能够直观解释的通的序列。<ul>
<li>裸模板提出了句子的主要成分——主语、谓语和直接宾语的语义类</li>
<li>如果谓语是不及物动词，裸模板中宾语的位置用一个虚构节点 DTHIS 代替，叫作哑元</li>
</ul>
</li>
<li>模板：如果义式的<strong>首部能组成裸模板</strong>，那么这些义式可能依附于其上的其他义式所组成的序列，就称为该原文片段的一个模板。模板并不仅仅包括义式的首部，实际上是义式组成的网络，首部是其核心部分。</li>
<li>超模板：把模板结合起来就形成超模板。两种结合方式：<ul>
<li>利用虚构的结点</li>
<li>找出指代和照应关系</li>
</ul>
</li>
</ul>
<p>采用优选语义学进行语言自动分析的过程：</p>
<ul>
<li>切分：根据关键词（标点、连接词和介词）</li>
<li>匹配：找出与切分段匹配的裸模板</li>
<li>扩展：把裸模板扩展为模板的网络，切分段内部以模板为框架建立词与词之间的关系，如果上一步匹配到不止一个裸模板，那么在建立关系时要根据各个裸模板语义联系程度的不同情况进行优选</li>
<li>捆绑：在各个模板之间建立联系，把模板捆绑为超模板，在切分段外部（切分段之间）建立联系。主要任务：<ul>
<li>建立模板之间的深层格的联系</li>
<li>建立哑元与它所替代的词之间的联系</li>
<li>解决遗留的歧义问题</li>
<li>解决代词的指代问题</li>
</ul>
</li>
</ul>
<p>优选语义学特点：</p>
<ul>
<li>语言分析不经过形态和句法分析，都通过语义信息表示出来，摆脱了传统的句法分析框框</li>
<li>各个片段的语义描写都可以用义素和括号统一进行</li>
</ul>
<h2 id="Schank-的概念依存理论"><a href="#Schank-的概念依存理论" class="headerlink" title="Schank 的概念依存理论"></a>Schank 的概念依存理论</h2><p>1973 年美国计算语言学家 R. Schank 提出，用于描述自然语言中的短语和句子的意义。</p>
<p>三条重要原理：</p>
<ul>
<li>意义相同的句子，无论属于什么语言，语义表达式只有一个</li>
<li>蕴涵在一个句子里的任何为理解所必需的信息都应该在概念依存理论中得到显式表达，一般使用概念依存表达式：由若干个语义基元组成，语义基元分为基本行为和基本状态两种。<ul>
<li>基本行为 11 个：PTRANS（物体物理位置转移）、ATRANS（占有、物主或控制等抽象关系转移）、INGEST（使某种东西进入动物体内）、PROPEL（在某物上使用体力）、MTRANS（人与人或一个人身上的精神信息转移）、MBUILD（人根据旧信息加工成新信息）、MOVE、GRASP、EXPEL、SPEAK、ATTEND；基本行为的概念之间的关系叫做依存。基本状态数量很多。</li>
<li>概念依存理论建立了五条推导因果关系的规则：<ul>
<li>行为可能引起状态改变</li>
<li>状态可以使行为成为可能</li>
<li>状态可以使行为成为不可能</li>
<li>状态可以激发一个精神事件，行为也可以激发一个精神事件</li>
<li>精神事件可以成为行为的原因</li>
</ul>
</li>
</ul>
</li>
<li>在句子的意义表达式中，必须把隐晦地存在于句子中的信息尽量显现出来</li>
</ul>
<p>概念依存表达式一般不依赖于句法，Schank 认为，概念依存理论具有一定的心理学效应，反映了人们认知活动的知觉概念。在概念依存理论原理的基础上，Schank 等提出了一些更高层次的知识结构：脚本、计划、目的和主题。</p>
<ul>
<li><p>脚本：用来描述人们活动的一种标准化事件序列，是人们对特定场合下可能出现的一些事件的固定顺序特有的一种集装知识。有两个项目特别重要：</p>
<ul>
<li>关键事件：首先要匹配关键事件，以便开始分析句子</li>
<li>主要概念：脚本所叙述的故事的目的</li>
</ul>
</li>
<li><p>计划、目的和主题：在指定情况为了达到某个目标而必须（或可能）采取的行动序列，是另一类更一般化的集装知识。</p>
<ul>
<li>计划是故事中的角色为实现其目的（如去电影院）所采取的手段（如走到车站）；计划还可以包括为实现某个一般目的（如去某地）所采取的手段（如开车、步行等）。计划集中起来构成计划库，计划库中存储着有关各种目的以及手段的信息。</li>
<li>采用计划理解故事的过程：<ul>
<li>首先确定角色目的</li>
<li>再确定导致主要目的之 D-目的</li>
<li>然后再把角色的行动同存储着 D-目的之打算库相匹配，从而获得对故事的一定理解</li>
</ul>
</li>
<li>主题是我们的预见所赖以建立起来的背景信息，背景信息中一定包含着角色的某种目的。一个主题要列举一系列角色，说明这些角色所处情况以及为了处理这种包含于主题中的情况而必须采取的行动，而主题的目的是完成这些行动。</li>
<li>Schank 和 Abelson 提出了七种类型的目的，主要有：<ul>
<li>A-目的或达成性目的</li>
<li>P-目的或保护性目的</li>
<li>C-目的或紧急性目的</li>
</ul>
</li>
<li>用计划、目的和主题这类知识结构理解一个故事的过程：<ul>
<li>用计划和主题这类知识结构去识别故事的目的</li>
<li>利用计划找到满足该目的之子目的以及相应的实施行动</li>
<li>在故事的相继输入中寻找上述子目的和行动，并据此对故事做出解释</li>
</ul>
</li>
</ul>
</li>
<li>脚本、计划、目的和主题之间的关系：<ul>
<li>主题引起目的</li>
<li>当目的被认出，并且其行动与该目的之实现相一致时，就可以引起计划</li>
<li>脚本是事件的标准化模式</li>
<li>脚本是特殊的，而计划是一般的</li>
<li>计划的来源是脚本</li>
<li>计划是表示人的目的之一种方法，这些目的隐含在脚本中，它们只表示行动</li>
<li>脚本中有一个关键事件，要与输入的句子进行模式匹配；计划中没有关键事件，每一个计划归入一个目的之下。</li>
</ul>
</li>
</ul>
<p>采用脚本、计划、目的和主题，Schank 等先后建立了 MARGIE、SAM、PAM、MOP、FRUMP、IPP 等系统。</p>
<ul>
<li>MARGIE 分为三部分<ul>
<li>概念分析程序：把英文句子转换成概念依存表达式</li>
<li>推理程序：接收依存表达式，根据系统存储器中有关信息进行推理，推演出大量事实。推理知识在存储器中用语义网络表示</li>
<li>文本生成程序：把概念依存表达式转为英文句子输出，两种方法<ul>
<li>分辨网络：用于区分不同词义，可以根据上下文选择恰当的单词</li>
<li>扩充转移网络：把概念依存表达式转为线性单词符号序列，输出句子表层线性结构</li>
</ul>
</li>
</ul>
</li>
<li>SAM 和 PAM 采用脚本和计划理解简单故事。区别在建立概念依存表达式之后的处理过程。<ul>
<li>SAM 采用故事与脚本相匹配的办法理解故事，匹配完成后就可以对故事做出总结</li>
<li>PAM 建立在计划基础上的，它的方法是确定故事中任务的目的，并且把而后的行动解释为实现这些行动的目的和 D-目的（直接目的）</li>
</ul>
</li>
<li>人们在理解故事中存在另一类知识结构，能同时容纳集装知识和抽象知识，叫作记忆组织包（MOP），MOP 中，各种不同环境共享的抽象知识存放在一个地方，以便不同的 MOP 调用。概念依存表达式、脚本、计划等属于静态记忆，而 MOP 则是具有自修改能力的动态记忆，它能把具体事件中获得的经验升华为抽象的一般性经验，反映了人类学习的过程。</li>
<li>Schank 认为传统分离的句法分析是不必要的，也不符合人们理解语言的心理过程。主张把句法和语义知识结合在一起，一次就把输入语句转换成某种机器的内部表示。这种分析方法叫作一体化的概念分析模型。MARGIE 的概念分析程序体现了这种思想，该程序经过扩充后成为 ELI（English Language Interpreter），充当许多故事理解系统的公共前端：把输入语句直接映射为概念依存表达式。ELI 采用的基本手段是 ”期望“：当一个人谈到或听到一个词时，他会预见某些别的词或者已经出现，或者即将相继出现。这种期望根据迄今已被理解的内容以及有关语言和世界的知识建立起来的。人们在阅读过程中，不断根据这样的期望预见下一步可能会读到什么，并利用它们来排除歧义和理解正读入的文本。<ul>
<li>第一个一体化程序是 FRUMP，通过对输入文本浏览来寻找它感兴趣的东西，这些东西往往是系统打算在故事的总结中陈述的那些重要信息。FRUMP 采用梗概脚本作为它的知识表示模型。整个理解程序是期望驱动的，附加在指定梗概脚本上的期望指明了脚本希望寻找的信息是什么。</li>
<li>FRUMP 的故事理解是根据实现设计好的梗概脚本的内容进行的，如果脚本设计时忽略了某些重要情节，理解就会出错。IPP 克服了这个缺陷，它采用 MOP 作为知识结构，抽象程度较高，并能够从读入的故事中自动地归纳出一般的结论来。它的词典中的词条分为立即分析的词、暂时跳过的词和完全忽略的词，处理故事时只可能忽略单词，不会证据忽略，这样就能发现和处理事先某些没有预见到的重要情节，有效克服了 FRUMP 的缺陷。IPP 论文：<a href="https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog0701_1" target="_blank" rel="noopener">Generalization From Natural Language Text*</a></li>
<li>其他系统：<ul>
<li>自动编写简单故事：<a href="https://www.cs.utah.edu/nlp/papers/talespin-ijcai77.pdf" target="_blank" rel="noopener">TALE-SPIN</a></li>
<li>模拟人们在国际政治事件意识形态上的理解：<a href="https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0201_3" target="_blank" rel="noopener">POLITICS: Automated Ideological Reasoning* - Carbonell - 1978 - Cognitive Science - Wiley Online Library</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>本节后面提到的几个系统在内部的模块设计理念上非常具有启发性。MOP 在具体事件中获得一般性经验，其理念甚至超过了知识图谱；ELI 利用期望的方法，也很符合人类认知；FRUMP 和 IPP 利用梗概脚本提取信息在 NLP 中有非常广泛的应用场景。</p>
</blockquote>
<h2 id="Mel’chuk-的意义-lt-gt-文本理论"><a href="#Mel’chuk-的意义-lt-gt-文本理论" class="headerlink" title="Mel’chuk 的意义&lt;=&gt;文本理论"></a>Mel’chuk 的意义&lt;=&gt;文本理论</h2><p>A. K. Zolkovski 和 I. A. Mel’chuk 20 世纪 60 年代提出。主张自然语言是建立意义和文本对应的逻辑工具。</p>
<p>三个基本假设：</p>
<ul>
<li>自然语言的意义和文本之间的对应是多对多的</li>
<li>自然语言的意义和文本之间的对应可以采用形式化的逻辑关系来描述，这个逻辑工具应当反映自然的说话人的语言活动</li>
<li>意义和文本之间关系复杂，所以在话语过程中，必须区分一些中间层次<ul>
<li>七个层次：语义表示、深层句法表示、表层句法表示、深层形态表示、表层形态表示、深层音位表示、表层音位表示</li>
<li>六个模块：语义模块、深层句法模块、表层句法模块、深层形态模块、表层形态模块、音位模块</li>
</ul>
</li>
</ul>
<p><strong>意义到文本的转化过程是从多维的图，经过二维的树，最后转化到一维的串的过程。</strong></p>
<ul>
<li>语义表示：多维有向图<ul>
<li>语义素：结点上的标记，是一个语义单位，它的函项叫作语义行动元</li>
<li>语义名：一般是具体名词，没有函项</li>
<li>语义依存：语义素和它的行动元之间用箭头相连，这种连接叫作语义依存</li>
</ul>
</li>
<li>表层句法表示：二维依存关系树<ul>
<li>严格区分句法结构和形态结构</li>
<li>句法结构只表示二维依存关系而不表示一维的前后线性关系</li>
<li>与依存语法一致，与短语结构语法有差别</li>
</ul>
</li>
<li>表层形态表示：一维的符号串<ul>
<li>单词有顺序</li>
<li>代词带有语法信息</li>
</ul>
</li>
</ul>
<p>意义&lt;=&gt;文本模型不是 ”生成“ 装置，而是 ”转换“ 装置，这是与 Chomsky 生成语法最重要的区别。主要工作原理是进行同义转换，在各个层次上生成大量同义结构，再经过各种过滤装置，筛选出合格文本。八种过滤器：</p>
<ul>
<li>一般类型过滤器：剔除语义合成结果中包含人造虚构词的深层句法结构</li>
<li>同类过滤器：剔除同义结构中所有包含 ”空位“ 关键词的深层结构语法</li>
<li>保障语义配价和句法配价饱和的过滤器：剔除不满足配价的深层句法结构</li>
<li>限制单词或词组的组合性能的过滤器：剔除不合规则的词汇组合</li>
<li>词序规则过滤器：剔除在一定语言环境下不合格的词序</li>
<li>限制表层句法成分的过滤器：剔除在表层句法结构中不合格的句法成分</li>
<li>限制形态或构词的过滤器：剔除在形态或构词上不合格的句法成分</li>
<li>优化文本的过滤器：剔除在修辞上不合格的文本</li>
</ul>
<p>巴黎第七大学的 S. Kahane 根据本理论提出了 ”转构语法“，主要功能是把意义-文本模型中不同层次上的结构集合对应起来，是一种 ”元语法的形式化模型“。</p>
<blockquote>
<p>本理论从多维的意义到二维的句法再到一维的符号这种结构非常具有启发性，尤其是多维的意义图让多维的语言有了一种呈现方式，也许自然语言的表示本身就是图结构的。</p>
</blockquote>
<h2 id="词义排歧方法"><a href="#词义排歧方法" class="headerlink" title="词义排歧方法"></a>词义排歧方法</h2><p>排歧涉及上下文因素、语义因素、语境因素以及生活中的常识，是 NLP 最棘手的问题。常用的方法有：</p>
<ul>
<li>选择最常见义项：没有排歧功能</li>
<li>利用词类：不同的词义往往属于不同的词类</li>
<li>基于选择限制：“观其伴而知其意”<ul>
<li>使用两方面知识<ul>
<li>论元的语义类型分类</li>
<li>论元对于谓词的选择限制</li>
</ul>
</li>
<li>局限性<ul>
<li>当选择限制一般性太强时，很难决定有关词的选择限制范围：what kind of dishes? (碟子 or 菜)</li>
<li>在否定句子的时候，否定关系违反选择限制，但语义合法：You can’t eat gold.</li>
<li>不寻常事件违反限制但句子合法：He ate glass in the trial.</li>
<li>有比喻或借喻：He wants to kill the USA.</li>
</ul>
</li>
<li>改进：<ul>
<li>1987 年，Hirst 建议把选择限制看作一种优选关系</li>
<li>1997 年，Resnik 提出 “选择联想”：谓词与该谓词所支配论元的类别之间的联想强度的概率测度</li>
</ul>
</li>
</ul>
</li>
<li>自立的鲁棒法：依靠词类标注工作，力求把对于信息的要求减到最低限度<ul>
<li>步骤如下：<ul>
<li>选择相关语言学特征</li>
<li>根据算法要求对特征进行形式化描述</li>
</ul>
</li>
<li>用来训练的语言学特征分类：<ul>
<li>搭配特征：对目标词左右的上下文编码</li>
<li>共现特征：不考虑相邻词的位置信息，单词本身可作为特征</li>
</ul>
</li>
</ul>
</li>
<li>有指导的学习方法（监督）<ul>
<li>朴素 Bayes 分类法：在给定上下文环境下，计算多义词的各个义项中概率最大的义项</li>
<li>决策表分类法：根据共现词的等价类的不同制定决策表，表中项目的排列根据训练语料的特征决定</li>
</ul>
</li>
<li>自举法（半监督）：不需要训练大量语料，每一个词目的每一个义项都依靠少量的标记好的实例来判别，以这些实例作为种子，采用有监督训练得到初始分类，再利用初始分类从未训练的语料中抽取训练语料，反复进行<ul>
<li>1991 年，Hearst 用简单的手工标记方法获得一个小的实例集合</li>
<li>1995 年，Yarowsky 提出 “每个搭配一个义项” 的原则：为每一个义项选择一个合理的标示词作为种子。选择种子的途径有：机器可读词典；统计方法根据搭配关系选择。</li>
</ul>
</li>
<li>无指导的方法（无监督）：根据相似度（从共现次数的分布可以看出）对语料聚类。<ul>
<li>凝聚法是常用的方法，语料中每个实例被指派给一个类聚，自底向上陆续把两个最相似的类聚结合成新的类聚，直到达到预期目标为止</li>
<li>不足：<ul>
<li>训练语料中无法得知什么是正确的义项</li>
<li>所得类聚往往与训练实例的义项在性质上差别很大</li>
<li>类聚的数量几乎总与需要消歧的目标词的义项的数量不一致</li>
</ul>
</li>
</ul>
</li>
<li>基于词典的方法：词典提供义项及义项的定义上下文，是一种利用既存知识源，判断两个词亲和程度时，比较它们在词典的定义中同时出现的词语情况。排歧时，把多义词的各个义项的定义进行比较，选择具有最大覆盖上下文的义项作为正确的义项。如 pine cone，cone 是多义词，把词典中 pine 的定义分别与 cone 的定义比较，选择重合最多的义项作为 cone 的义项。</li>
</ul>
<blockquote>
<p>这一节虽然内容有点相对古老，不过思想依旧有意义。</p>
</blockquote>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章都是关于语义（即如何理解自然语言）的形式模型，主要包括：</p>
<ul>
<li>义素分析法<ul>
<li>20 世纪 50 年代，美国人类学家 F. G. Lounsbury 和 W. H. Goodenough 在研究亲属词的含义时提出了义素分析法；20 世纪 60 年代初，美国语言学家 J. J. Katz 和 J. A. Fodor 提出了解释语义学，将义素分析法引入语言学，为生成转换语法提供语义特征。</li>
<li>义素是意义的基本要素，它就是词的理性意义的区别特征。词的理性意义是一束语义特征（义素）的总和。</li>
<li>一组词的义素可以用义素矩阵表示，纵坐标表示词，横坐标表示义素，纵横坐标相交点注以 + 或 - 号或其他表示方法。</li>
</ul>
</li>
<li>语义场<ul>
<li>20 世纪 30 年代初，德国学者 J. Trier 提出系统的语义场理论，1992 年，北大贾彦德在《汉语语义学》中系统提出了汉语的语义场理论，北京语言大学张普提出 “场型” 的概念。</li>
<li>语义场是词义形成的系统，它是基于概念的关系场，是词义与词义之间构成的一种完全虚化、非物质的空间领域。若干个意义上紧密相联的词义，通常归属于一个总称之下，就构成了语义场。语义场可进一步分为词汇场和联想场。词汇场是静态的，表现为词义与词义之间的组合关系。这里的语义场主要指词汇场。</li>
<li>汉语的场型（不同类型的语义场）包括：<ul>
<li>分类场型：基本场型，一般是多层次的</li>
<li>构件场型：基本场型，下位是上位的构件</li>
<li>有序场型：基于分类场型和构件场型的特殊场型，所有平位是有序的</li>
<li>对立场型：特殊场型，平位的词义之间存在对立关系</li>
<li>同义场型：特殊场型，同一场型中，同位和变位的理性意义完全相同，只是附属于理性意义的风格、色彩等方面义素不同</li>
</ul>
</li>
<li>场与场之间的关系有：嵌套关系、交叉关系、传递关系、联想关系</li>
</ul>
</li>
<li>语义网络<ul>
<li>1972 年，美国人工智能专家 R. F. Simmons 和 J. Slocum 首先将语义网络用于自然语言理解系统中。1977 年，美国人工智能学者 G. Hendrix 提出了分块语义网络的思想，把语义的逻辑表示与 “格语法” 结合起来，把复杂问题分解为若干简单的子问题，每个子问题以一个语义网络表示。</li>
<li>语义网络可用有向图表示，一个语义网络就是由一些以有向图表示的三元组：（结点1，弧，结点2）连接而成的。三元组可写成二元谓词：P（个体1，个体2）</li>
<li>语义网络可表示一个事件，事件由若干个概念组合所反映的客观现实，可以分为叙述性事件、描述性事件和表述性事件三种。语义网络表述事件时，结点之间的关系还可以为施事（AGENT）、受事（OPATIENT）、位置（LOCATION）、时间（TIME）等。</li>
<li>语义网络的推理机制一般基于网络的匹配。根据提出的问题构建局部网络，查询解答的过程就是查询局部网络到网络知识库的匹配操作。知识图谱描述真实世界中存在的各种实体或概念，每个属性-值偶对用来刻画实体的内在特性，而关系用来连接两个实体。可以看作一个巨大的语义网络。</li>
</ul>
</li>
<li>Montague 语法<ul>
<li>1970 年前后美国数理逻辑学家 R. Montague 等把内涵逻辑应用于自然语言的研究，并把生成语法与内涵逻辑这两个领域的研究集中提炼为 Montague 语法。</li>
<li>在 Montague 语法里，一个句子的句法形式、内涵逻辑表达式和语义所指都是从基本单位开始，通过句法规则、转译规则和语义规则，从小到大逐段确定的。句法、转译和语义三大部分是同态的。有一条句法规则就有一条转译规则把它处理的短语转译成内涵逻辑表达式，然后再由一条语义规则来确定这个内涵逻辑表达式的语义。歧义问题通过不同的组合方式和运用不同的句法、语义规则来解决，这是 Montague 语法的 “规则对规则假说”。<ul>
<li>句法：包括一套语类和一套句法规则。功能是把来自词库的词语组成句子。</li>
<li>转译：包括一套转译规则，把短语转译成内涵逻辑表达式。</li>
<li>语义：以内涵逻辑为基础建立的。内涵逻辑包括句法和语义两方面</li>
</ul>
</li>
<li>Montague 语法有两个来源：N. Chomsky 的生成转换语法 和 Louis 的内涵逻辑学。采用内涵逻辑学来描述句子的深层结构，在句子的每一个层次上都可得出一个相应的内涵逻辑表达式，并以此来表示该句子深层结构的逻辑含义。从 (λx…x…)a 出发得到 …a… 这一性质称为 “λ-变换”， <strong>λ-变换是 Montague 语法转译计算的关键</strong>，生成语法得出的完全相同的树形图在 λ-变换后，可以得出不同的内涵逻辑表达式。</li>
<li>Montague 通过真值条件语义学、模型论语义学和可能世界语义学（Montague 语法的语义理论的三个特点），把自然语言所表现出来的意义介入内涵逻辑学中，从而建立了 Montague 语法的语义理论。由于将句法和语义结合，使得任何一个通过句法分析得到的表示句子的句法结构的树形图，都可以用 Montague 语法解释为相应的内涵逻辑表达式，从而表现出句子的语义内容。</li>
</ul>
</li>
<li>Wilks 优选语义学<ul>
<li>1874 年，Y. A. Wilks 在研制英法机器翻译系统的基础上，提出了 “优选语义学”，它共有五种语义单位：义素、义式、裸模板、模板、超模板，并有从较小的单位到较大的单位的构造规则。</li>
<li>采用优选语义学进行语言自动分析的过程：<ul>
<li>切分：根据关键词（标点、连接词和介词）</li>
<li>匹配：找出与切分段匹配的裸模板</li>
<li>扩展：把裸模板扩展为模板的网络，切分段内部以模板为框架建立词与词之间的关系，如果上一步匹配到不止一个裸模板，那么在建立关系时要根据各个裸模板语义联系程度的不同情况进行优选</li>
<li>捆绑：在各个模板之间建立联系，把模板捆绑为超模板，在切分段外部（切分段之间）建立联系。</li>
</ul>
</li>
</ul>
</li>
<li>Schank 概念依存理论<ul>
<li>1973 年美国计算语言学家 R. Schank 提出，用于描述自然语言中的短语和句子的意义。</li>
<li>三条重要原理：<ul>
<li>意义相同的句子，无论属于什么语言，语义表达式只有一个</li>
<li>蕴涵在一个句子里的任何为理解所必需的信息都应该在概念依存理论中得到显式表达，一般使用概念依存表达式：由若干个语义基元组成，语义基元分为基本行为和基本状态两种。</li>
<li>在句子的意义表达式中，必须把隐晦地存在于句子中的信息尽量显现出来</li>
</ul>
</li>
<li>概念依存表达式一般不依赖于句法，Schank 认为，概念依存理论具有一定的心理学效应，反映了人们认知活动的知觉概念。在概念依存理论原理的基础上，Schank 等提出了一些更高层次的知识结构：脚本、计划、目的和主题。<ul>
<li>脚本：用来描述人们活动的一种标准化事件序列，是人们对特定场合下可能出现的一些事件的固定顺序特有的一种集装知识。</li>
<li>计划、目的和主题：在指定情况为了达到某个目标而必须（或可能）采取的行动序列，是另一类更一般化的集装知识。</li>
<li>计划是故事中的角色为实现其目的（如去电影院）所采取的手段（如走到车站）；计划还可以包括为实现某个一般目的（如去某地）所采取的手段（如开车、步行等）。计划集中起来构成计划库，计划库中存储着有关各种目的以及手段的信息。</li>
<li>采用计划理解故事的过程：首先确定角色目的，再确定导致主要目的之 D-目的，然后再把角色的行动同存储着 D-目的之打算库相匹配，从而获得对故事的一定理解。</li>
<li>主题是我们的预见所赖以建立起来的背景信息，背景信息中一定包含着角色的某种目的。一个主题要列举一系列角色，说明这些角色所处情况以及为了处理这种包含于主题中的情况而必须采取的行动，而主题的目的是完成这些行动。</li>
<li>Schank 和 Abelson 提出了七种类型的目的。</li>
<li>用计划、目的和主题这类知识结构理解一个故事的过程：<ul>
<li>用计划和主题这类知识结构去识别故事的目的</li>
<li>利用计划找到满足该目的之子目的以及相应的实施行动</li>
<li>在故事的相继输入中寻找上述子目的和行动，并据此对故事做出解释</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Mel’chuk 意义&lt;=&gt;文本理论<ul>
<li>A. K. Zolkovski 和 I. A. Mel’chuk 20 世纪 60 年代提出。主张自然语言是建立意义和文本对应的逻辑工具。</li>
<li>三个基本假设：<ul>
<li>自然语言的意义和文本之间的对应是多对多的</li>
<li>自然语言的意义和文本之间的对应可以采用形式化的逻辑关系来描述，这个逻辑工具应当反映自然的说话人的语言活动</li>
<li>意义和文本之间关系复杂，所以在话语过程中，必须区分一些中间层次</li>
</ul>
</li>
<li>意义到文本的转化过程是从多维的图，经过二维的树，最后转化到一维的串的过程。</li>
<li>意义&lt;=&gt;文本模型不是 ”生成“ 装置，而是 ”转换“ 装置，这是与 Chomsky 生成语法最重要的区别。主要工作原理是进行同义转换，在各个层次上生成大量同义结构，再经过各种过滤装置，筛选出合格文本。八种过滤器：一般类型过滤器、同类过滤器、保障语义配价和句法配价饱和的过滤器、限制单词或词组的组合性能的过滤器、词序规则过滤器、限制表层句法成分的过滤器、限制形态或构词的过滤器、优化文本的过滤器。</li>
</ul>
</li>
<li>词义排歧方法<ul>
<li>排歧涉及上下文因素、语义因素、语境因素以及生活中的常识，是 NLP 最棘手的问题</li>
<li>选择最常见义项：没有排歧功能</li>
<li>利用词类：不同的词义往往属于不同的词类</li>
<li>基于选择限制：“观其伴而知其意”，使用两方面知识：论元的语义类型分类；论元对于谓词的选择限制</li>
<li>自立的鲁棒法：依靠词类标注工作，力求把对于信息的要求减到最低限度，用来训练的语言学特征分类分为：搭配特征和共现特征</li>
<li>有指导的学习方法（监督），包括朴素 Bayes 分类法决策表分类法</li>
<li>自举法（半监督）：每一个词目的每一个义项都依靠少量的标记好的实例来判别，以这些实例作为种子，采用有监督训练得到初始分类，再利用初始分类从未训练的语料中抽取训练语料，反复进行</li>
<li>无指导的方法（无监督）：根据相似度（从共现次数的分布可以看出）对语料聚类</li>
<li>基于词典的方法：词典提供义项及义项的定义上下文，排歧时，把多义词的各个义项的定义进行比较，选择具有最大覆盖上下文的义项作为正确的义项</li>
</ul>
</li>
</ul>
<p>本章的很多思想非常具有启发性，比如语义网络的构建和推理（涉及知识图谱）、基于概念依存理论的 MOP 和 ELI、意义&lt;=&gt;文本理论的多维意义到二维句法再到一维符号的结构表达，最后的词义排歧方法也是涵盖全面，虽然时间上早了些，但方法却依然在使用。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/02/15/NLPFA/2019-02-15-Ch08-Formal-Model-of-Semantic-Automatic-Processing/">
    <time datetime="2019-02-15T03:32:00.000Z" class="entry-date">
        2019-02-15
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Model/">Formal Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a></li></ul>

    </footer>
</article>





  
  
    <nav id="pagination">
      <nav id="page-nav">
        <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
      </nav>
    </nav>
  
</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">25</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">8</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/">自然语言计算机形式分析的理论与方法笔记(Ch18)</a>
          </li>
        
          <li>
            <a href="/2019/03/31/2019-03-31-Nabokov-Favorite-Word/">《纳博科夫最喜欢的词》读书笔记与思考</a>
          </li>
        
          <li>
            <a href="/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/">自然语言计算机形式分析的理论与方法笔记(Ch15)</a>
          </li>
        
          <li>
            <a href="/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/">自然语言计算机形式分析的理论与方法笔记(Ch14)</a>
          </li>
        
          <li>
            <a href="/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/">自然语言计算机形式分析的理论与方法笔记(Ch13)</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a><span class="tag-list-count">28</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backward/">Backward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Business/">Business</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calculus/">Calculus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DataClearing/">DataClearing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DataScience/">DataScience</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/">DeepLearning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependence/">Dependence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Diary/">Diary</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Economics/">Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/">Entropy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Evaluation/">Evaluation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FSM/">FSM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Model/">Formal Model</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forward/">Forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gan/">Gan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HMM/">HMM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Industry/">Industry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalism/">Lexicalism</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LinearAlgebra/">LinearAlgebra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine/">Machine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MachineLearning/">MachineLearning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markov/">Markov</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">23</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ngram/">Ngram</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Orientation/">Orientation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ReinforcementLearning/">ReinforcementLearning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simon/">Simon</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SimpsonParadox/">SimpsonParadox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smoothing/">Smoothing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spell-Check/">Spell Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Style/">Style</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/">System</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valence/">Valence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VirtualBox/">VirtualBox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Viterbi/">Viterbi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZhouZhihua/">ZhouZhihua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zipf/">Zipf</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Business/" style="font-size: 10px;">Business</a> <a href="/tags/C/" style="font-size: 11.67px;">C</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/DataClearing/" style="font-size: 10px;">DataClearing</a> <a href="/tags/DataScience/" style="font-size: 13.33px;">DataScience</a> <a href="/tags/DeepLearning/" style="font-size: 13.33px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 10px;">Diary</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Entropy/" style="font-size: 10px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10px;">Evaluation</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Formal-Model/" style="font-size: 16.67px;">Formal Model</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/LinearAlgebra/" style="font-size: 10px;">LinearAlgebra</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/MachineLearning/" style="font-size: 10px;">MachineLearning</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Math/" style="font-size: 11.67px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/NLP/" style="font-size: 18.33px;">NLP</a> <a href="/tags/Ngram/" style="font-size: 10px;">Ngram</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Philosophy/" style="font-size: 11.67px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/ReinforcementLearning/" style="font-size: 10px;">ReinforcementLearning</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/SimpsonParadox/" style="font-size: 10px;">SimpsonParadox</a> <a href="/tags/Smoothing/" style="font-size: 10px;">Smoothing</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/System/" style="font-size: 11.67px;">System</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10px;">Viterbi</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2019 Yam
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>