<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
<meta property="og:type" content="website">
<meta property="og:title" content="Yam">
<meta property="og:url" content="http://www.yam.gift/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yam">
<meta name="twitter:description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="http://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main">
  
    <article id="post-SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification" class="post-SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/05/05/SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification/">Naive Bayes and Sentiment Classification</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/05/05/SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification/" data-id="cjvad15dm0000n3ccu9u2v7r4" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p><strong>Text categorization</strong>, the task of assigning a label or category to an entire text or document.</p>
<ul>
<li>sentiment analysis</li>
<li>spam detection</li>
<li>subject category or topic label</li>
</ul>
<p><strong>Probabilistic classifier</strong> additionally will tell us the probability of the observation being in the class.</p>
<p><strong>Generative classifiers</strong> like naive Bayes build a model of how a class could generate some input data. </p>
<p><strong>Discriminative classifiers</strong> like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. </p>
<h2 id="Naive-Bayes-Classifiers"><a href="#Naive-Bayes-Classifiers" class="headerlink" title="Naive Bayes Classifiers"></a>Naive Bayes Classifiers</h2><p><strong>Bayesian inference</strong>: </p>
<script type="math/tex; mode=display">\hat c = \arg \max_{c \in C} P(c|d) = \arg \max_{c \in C} P(d|c) P(c) = \arg \max_{c \in C} P(f_1,f_2,…f_n|c) P(c)</script><ul>
<li>given document d</li>
<li>prior probability of the class P(c)</li>
<li><p>likelihood of the document p(d|c)</p>
</li>
<li><p>f1, f2, … as a set of features</p>
</li>
</ul>
<p>P(f1,f2…|c) is hard to compute directly, estimate the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets.</p>
<p><strong>Naive Bayes classifiers</strong> make two simplifying assumptions:</p>
<ul>
<li>position doesn’t matter</li>
<li>P(fi|c) are independent given the class c: <script type="math/tex">P(f_1,f_2,…,f_n|c) = P(f_1|c)P(f_2|c)…P(f_n|c)</script></li>
</ul>
<script type="math/tex; mode=display">C_{NB} = \arg \max_{c \in C} P(c) \prod_{f \in F} P(f|c)</script><p>To apply the naive Bayes classifier to text, we need to consider word position:</p>
<script type="math/tex; mode=display">C_{NB} = \arg \max_{c \in C} P(c) \prod_{i \in positions} P(w_i|c)</script><p>Calculations are done in log space to avoid underflow and increase speed:</p>
<script type="math/tex; mode=display">C_{NB} = \arg \max_{c \in C} \log P(c)  + \sum_{i \in positions} \log P(w_i|c)</script><h2 id="Training-the-Naive-Bayes-Classifer"><a href="#Training-the-Naive-Bayes-Classifer" class="headerlink" title="Training the Naive Bayes Classifer"></a>Training the Naive Bayes Classifer</h2><script type="math/tex; mode=display">\hat P(c) = \frac {N_c}{N_{doc}}</script><script type="math/tex; mode=display">\hat P(w_i|c) = \frac {count(w_i, c)}{\sum_{w \in V} count(w, c)}</script><p>If a word only occurs in one class, the probability for this feature in the other class will be zero, then the total class is zero, no matter the other evidence. The simplest solution is Add-one smoothing:</p>
<script type="math/tex; mode=display">\hat P(w_i|c) = \frac {count(w_i, c)+1}{\sum_{w \in V} (count(w, c)+1)} =  \frac {count(w_i, c)+1}{(\sum_{w \in V} count(w, c)) + |V|}</script><p>V consists of the union of all the word types in all classes, not just the words in one class c.</p>
<p>The solution for <strong>unknown words</strong> (in test data) is to ignore them, remove them from the test document and not include any probability for them at all.</p>
<p>In most text classification applications, however, using a stop word list doesn’t improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">function Train Naive Bayes(D, C) returns log P(c) <span class="keyword">and</span> log P(w|c)</span><br><span class="line"><span class="keyword">for</span> each <span class="class"><span class="keyword">class</span> <span class="title">c</span> ∈ <span class="title">C</span></span></span><br><span class="line">	Ndoc = number of documents in D</span><br><span class="line">    Nc = number of documents <span class="keyword">from</span> D <span class="keyword">in</span> <span class="class"><span class="keyword">class</span> <span class="title">c</span></span></span><br><span class="line"><span class="class">    <span class="title">logprior</span>[<span class="title">c</span>] ← <span class="title">log</span> <span class="title">Nc</span>/<span class="title">Ndoc</span></span></span><br><span class="line"><span class="class">    <span class="title">V</span> ← <span class="title">vocabulary</span> <span class="title">of</span> <span class="title">D</span></span></span><br><span class="line"><span class="class">    <span class="title">bigdoc</span>[<span class="title">c</span>] ← <span class="title">append</span><span class="params">(d)</span> <span class="title">for</span> <span class="title">d</span> ∈ <span class="title">D</span> <span class="title">with</span> <span class="title">class</span> <span class="title">c</span></span></span><br><span class="line"><span class="class">    <span class="title">for</span> <span class="title">each</span> <span class="title">word</span> <span class="title">w</span> <span class="title">in</span> <span class="title">V</span></span></span><br><span class="line"><span class="class">    	<span class="title">count</span><span class="params">(w, c)</span> ← <span class="title">num</span> <span class="title">of</span> <span class="title">occurrences</span> <span class="title">of</span> <span class="title">w</span> <span class="title">in</span> <span class="title">bigdoc</span>[<span class="title">c</span>]</span></span><br><span class="line">        loglikelihood[w,c] ← log (count(w,c) + 1)/∑(count(w*, c) + 1)</span><br><span class="line">    <span class="keyword">return</span> logprior, loglikelihood</span><br><span class="line"></span><br><span class="line">function Test Naive Bayes(testdoc, logprior, loglikelihood, C, V) returns best c</span><br><span class="line"><span class="keyword">for</span> each <span class="class"><span class="keyword">class</span> <span class="title">c</span> ∈ <span class="title">C</span></span></span><br><span class="line"><span class="class">	<span class="title">sum</span>[<span class="title">c</span>] ← <span class="title">logprior</span>[<span class="title">c</span>]</span></span><br><span class="line"><span class="class">    <span class="title">for</span> <span class="title">each</span> <span class="title">position</span> <span class="title">i</span> <span class="title">in</span> <span class="title">testdoc</span></span></span><br><span class="line"><span class="class">    	<span class="title">word</span> ← <span class="title">testdoc</span>[<span class="title">i</span>]</span></span><br><span class="line"><span class="class">        <span class="title">if</span> <span class="title">word</span> ∈ <span class="title">V</span></span></span><br><span class="line">        	sum[c] ← sum[c] + loglikehood[word, c]</span><br><span class="line"><span class="keyword">return</span> argmax_c sum[c]</span><br></pre></td></tr></table></figure>
<p>The code is here: <a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/Bayes/Naive-Bayes.ipynb" target="_blank" rel="noopener">Naive-Bayes</a></p>
<h2 id="Optimizing-for-Sentiment-Analysis"><a href="#Optimizing-for-Sentiment-Analysis" class="headerlink" title="Optimizing for Sentiment Analysis"></a>Optimizing for Sentiment Analysis</h2><ul>
<li><p>In some tasks like sentiment classification, whether a word occurs or not seems to matter more than its frequency. Thus it often improve performance to clip the word counts in <strong>each document</strong> at 1. This variant is called <strong>binary multinomial naive Bayes</strong> or <strong>binary NB</strong>. DO NOT need to smooth. In Python, just use <code>set(doc_i)</code>.</p>
</li>
<li><p>Deal with <strong>negation</strong>. A simple baseline is to add a prefix <code>NOT_</code> to every word after a token of logical negation (n’t, not, no, never) until the next punctuation mark. It works quite well in practice.</p>
</li>
<li>When training data is insufficient, we can instead derive the positive and negative word features from <strong>sentiment lexicons</strong>. <ul>
<li>A common way to use lexicons is a naive Bayes classifier is to add a feature that is  counted whenever a word from that lexicon occurs. </li>
<li>If training data is sparse or not representative of the test set, using dense lexicon features (whether a word occurs in the lexicon) instead of sparse individual-word features may generalize better.</li>
</ul>
</li>
</ul>
<h2 id="Naive-Bayes-for-other-text-classification-tasks"><a href="#Naive-Bayes-for-other-text-classification-tasks" class="headerlink" title="Naive Bayes for other text classification tasks"></a>Naive Bayes for other text classification tasks</h2><ul>
<li>spam detection: sets of words or phrases as features</li>
<li>language ID: byte n-grams.</li>
</ul>
<h2 id="Naive-Bayes-as-aLanguage-Model"><a href="#Naive-Bayes-as-aLanguage-Model" class="headerlink" title="Naive Bayes as aLanguage Model"></a>Naive Bayes as aLanguage Model</h2><p>If we use all the words in the text, naive Bayes is similar to language model. Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model.</p>
<p>Since the likelihood features from the naive Bayes model assign a probability to each word P(w|c), the model also assigns a probability to each sentence: <script type="math/tex">P(s|c) = \prod_{i \in positions} P(w_i|c)</script></p>
<h2 id="Evaluation-Precision-Recall-F-measure"><a href="#Evaluation-Precision-Recall-F-measure" class="headerlink" title="Evaluation: Precision, Recall, F-measure"></a>Evaluation: Precision, Recall, F-measure</h2><p><img src="http://qnimg.lovevivian.cn/slp-ch4-1.jpeg" alt=""></p>
<p>Accuracy is not a good metric when the goal is to discover something that is rare, or at least not completely balanced in frequency, which is a very common situation in the world.</p>
<p>Precision and recall, emphasize true positives: finding the things that we are supposed to be looking for.</p>
<p><strong>F-measure</strong>: <script type="math/tex">F_\beta = \frac {(\beta^2+1)PR}{\beta^2P + R}</script></p>
<p>The β weights the importance of recall and precision, based on the needs of an application.</p>
<ul>
<li>β &gt; 1 favor recall</li>
<li>β &lt; 1 favor precision</li>
<li>β = 1 equally balanced, called F1</li>
</ul>
<p>F-measure comes from a weighted harmonic mean of precision and recall.</p>
<script type="math/tex; mode=display">HarmonicMean(a_1, a_2,…,a_n) = \frac {n}{\frac{1}{a_1}+\frac{1}{a_2}+…+\frac{1}{a_n}}</script><script type="math/tex; mode=display">F = \frac {1}{\alpha \frac{1}{P} + (1-\alpha) \frac{1}{R}} = F_\beta</script><script type="math/tex; mode=display">\beta^2 = \frac{1-\alpha}{\alpha}</script><h3 id="More-than-two-classes"><a href="#More-than-two-classes" class="headerlink" title="More than two classes"></a>More than two classes</h3><p>Two kinds of multi-class classification tasks:</p>
<ul>
<li>any-of</li>
<li>multi-label</li>
</ul>
<p>We combine the metric values in two ways:</p>
<ul>
<li>macroaveraging: compute the performance for each class, and then average over classes.</li>
<li>microaveraging: collect the decisions for all classes into a single contingency table, and<br>  then compute precision and recall from that table. </li>
</ul>
<p><img src="http://qnimg.lovevivian.cn/slp-ch4-2.jpeg" alt=""></p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch4-3.jpeg" alt=""></p>
<h2 id="Test-sets-and-Cross-validation"><a href="#Test-sets-and-Cross-validation" class="headerlink" title="Test sets and Cross-validation"></a>Test sets and Cross-validation</h2><p>We use the <strong>development test set</strong> (devset) to tune some parameters.</p>
<p>If we could use all our data both for training and test, we do this by <strong>cross-validation</strong>: </p>
<ul>
<li>randomly choose a training and test set division of data, train and compute error rate</li>
<li>repeat with a different randomly selected training and test set</li>
</ul>
<p>Repeat times n, we get an average error rate, it is called <strong>n-fold cross-validation</strong>.</p>
<p>As all the data is used for test, we need the whole corpus to be blind, means we can’t examine any of the data to suggest possible features and see what’s going on. But looking at the corpus is often important.</p>
<p>So it is common to create a fixed training and test set, then do cross-validation inside the training set, but compute error rate the normal way in the test set.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch4-4.jpeg" alt=""></p>
<h2 id="Statistical-Significance-Testing"><a href="#Statistical-Significance-Testing" class="headerlink" title="Statistical Significance Testing"></a>Statistical Significance Testing</h2><p>We have a test set x of n observations x = x1, x2, … xn on which A’s performance is better than B by δ(x). We need to reject the <strong>null hypothesis</strong> that A isn’t really better than B and this difference δ(x) occurred purely by chance.</p>
<p>In language processing we use non-parametric tests like the <strong>bootstrap test</strong>, or a similar test <strong>approximate randomization</strong> because most metrics are not normally distributed. <strong>Bootstrapping</strong> refers to repeatedly drawing large numbers of smaller samples with replacement (bootstrap samples) from an original larger sample.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">function BOOSTRAP(test set x, num of samples b) returns p-value(x)</span><br><span class="line">Calculate δ(x) <span class="comment"># how much better A do than B on x</span></span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to b do</span><br><span class="line">	<span class="keyword">for</span> j = <span class="number">1</span> to n do</span><br><span class="line">    	Select a member of x at random <span class="keyword">and</span> add it to x*(i)</span><br><span class="line">    Calculate δ(x*(i))</span><br><span class="line"><span class="keyword">for</span> each x*(i)</span><br><span class="line">	s ← s + <span class="number">1</span> <span class="keyword">if</span> δ(x*(i)) &gt; <span class="number">2</span>δ(x)</span><br><span class="line">p-value(x) ≈ s/b</span><br><span class="line"><span class="keyword">return</span> p-value(x)</span><br></pre></td></tr></table></figure>
<p>The code is here: <a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/Test/Statistical-Significance-Testing.ipynb" target="_blank" rel="noopener">Statistical-Significance-Testing</a></p>
<h2 id="Advanced-Feature-Selection"><a href="#Advanced-Feature-Selection" class="headerlink" title="Advanced: Feature Selection"></a>Advanced: Feature Selection</h2><p>Features are generally ranked by how informative they are about the classification decision. A very common metric is <strong>information gain</strong> which tells us how many bits of information the presence of the word gives us for guessing the class.</p>
<script type="math/tex; mode=display">G(w) = -\sum_{i=1}^C P(c_i) \log P(c_i) + P(w) \sum_{i=1}^C P(c_i|w) \log P(c_i|w) + P(\overline{w}) \sum_{i=1}^C P(c_i|\overline{w}) \log P(c_i|\overline{w})</script><p> c_i is ith class, w- means that a document does not contain the word w​.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>Many language processing tasks can be viewed as tasks of <strong>classification</strong> learn to model the class given the observation.</li>
<li>Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.</li>
<li>Naive Bayes is a generative model that make the bag of words assumption (position doesn’t matter) and the conditional independence assumption (words are conditionally independent of each other given the class). Naive Bayes with binarized features seems to work better for many text classification tasks.</li>
<li>Feature selection can be used to automatically remove features that aren’t helpful.</li>
<li>Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.</li>
<li>Classifiers are evaluated based on precision and recall.</li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/05/05/SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification/">
    <time datetime="2019-05-05T03:11:00.000Z" class="entry-date">
        2019-05-05
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Evaluation/">Evaluation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/F1/">F1</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Naive-Bayes/">Naive Bayes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Sentiment-Classification/">Sentiment Classification</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Test/">Test</a></li></ul>

    </footer>
</article>





  
    <article id="post-SLP/2019-04-22-RegularExpressions-TextNormalization-EditDistance" class="post-SLP/2019-04-22-RegularExpressions-TextNormalization-EditDistance post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/04/22/SLP/2019-04-22-RegularExpressions-TextNormalization-EditDistance/">Regular Expressions, Text Normalization, and Edit Distance</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/04/22/SLP/2019-04-22-RegularExpressions-TextNormalization-EditDistance/" data-id="cjuti5gru0058i4ccl5fcxl9e" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p><strong>Normalizing text</strong> means converting it to a more convenient, standard form. </p>
<ul>
<li><strong>tokenization</strong>: separating out or tokenizing words from running text</li>
<li><strong>lemmatization</strong>: words have the same root but different surface. <strong>Stemming</strong> refers to a simpler version of lemmatization in which just strip suffixes from the end of the word.</li>
<li><strong>sentence segmentation</strong></li>
</ul>
        
          <p class="article-more-link">
            <a href="/2019/04/22/SLP/2019-04-22-RegularExpressions-TextNormalization-EditDistance/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/04/22/SLP/2019-04-22-RegularExpressions-TextNormalization-EditDistance/">
    <time datetime="2019-04-22T04:52:00.000Z" class="entry-date">
        2019-04-22
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Edit-Distance/">Edit Distance</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reqular-Expressions/">Reqular Expressions</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Text-Normalization/">Text Normalization</a></li></ul>

    </footer>
</article>





  
    <article id="post-2019-04-21-DB-FDW" class="post-2019-04-21-DB-FDW post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/04/21/2019-04-21-DB-FDW/">DataBase Foreign Data Wrapper</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/04/21/2019-04-21-DB-FDW/" data-id="cjuti5gou000li4cc43cg0yta" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>有时候我们需要将多个数据源的 DataBase 放在一个地方，最 Naive 的方法就是把其他 DataBase 的数据备份出来，再全部导入到一个 DataBase，但是这样比较麻烦，而且当数据库很大时也会比较耗时。这时候使用 FDW 就非常方便了。FDW 全称 Foreign Data Wrapper，这里有一些基本的介绍：<a href="https://wiki.postgresql.org/wiki/Foreign_data_wrappers" target="_blank" rel="noopener">Foreign data wrappers - PostgreSQL wiki</a>，FDW 非常简单且效果不错，下面逐步介绍基本操作和注意事项。</p>
        
          <p class="article-more-link">
            <a href="/2019/04/21/2019-04-21-DB-FDW/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/04/21/2019-04-21-DB-FDW/">
    <time datetime="2019-04-21T14:00:00.000Z" class="entry-date">
        2019-04-21
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Coding/">Coding</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DB/">DB</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/FDW/">FDW</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Postgresql/">Postgresql</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-04-13-NLP-Formal-Analysis" class="post-NLPFA/2019-04-13-NLP-Formal-Analysis post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/04/13/NLPFA/2019-04-13-NLP-Formal-Analysis/">自然语言计算机形式分析的理论与方法笔记</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/04/13/NLPFA/2019-04-13-NLP-Formal-Analysis/" data-id="cjuti5gqp003ni4ccjs6f1mq9" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <h2 id="各章主题阅读笔记"><a href="#各章主题阅读笔记" class="headerlink" title="各章主题阅读笔记"></a>各章主题阅读笔记</h2><ul>
<li><a href="https://yam.gift/2018/09/19/NLPFA/2018-09-19-Ch01-Orientation-of-NLP/" target="_blank" rel="noopener">第一章：自然语言处理的学科定位</a></li>
<li><a href="https://yam.gift/2018/10/11/NLPFA/2018-10-11-Ch02-Pioneers-in-Language-Computing/" target="_blank" rel="noopener">第二章：自然语言研究的先驱</a></li>
<li><a href="https://yam.gift/2018/12/22/NLPFA/2018-12-22-Ch03-Formal-Model-Based-on-Phrase-Structure-Grammar/" target="_blank" rel="noopener">第三章：基于短语结构语法的形式模型</a></li>
<li><a href="https://yam.gift/2019/01/09/NLPFA/2019-01-09-Ch04-Formal-Model-Based-on-Unity-Operation/" target="_blank" rel="noopener">第四章：基于合一运算的形式模型</a></li>
<li><a href="https://yam.gift/2019/01/15/NLPFA/2019-01-15-Ch05-Formal-Model-Based-on-Dependence-and-Valence/" target="_blank" rel="noopener">第五章：基于依存和配价的形式模型</a></li>
<li><a href="https://yam.gift/2019/01/18/NLPFA/2019-01-18-Ch06-Formal-Model-Based-on-Grid-Grammar/" target="_blank" rel="noopener">第六章：基于格语法的形式模型</a></li>
<li><a href="https://yam.gift/2019/01/31/NLPFA/2019-01-31-Ch07-Formal-Model-Based-on-Lexicalism/" target="_blank" rel="noopener">第七章：基于词汇主义的形式模型</a></li>
<li><a href="https://yam.gift/2019/02/15/NLPFA/2019-02-15-Ch08-Formal-Model-of-Semantic-Automatic-Processing/" target="_blank" rel="noopener">第八章：语义自动处理的形式模型</a></li>
<li><a href="https://yam.gift/2019/02/21/NLPFA/2019-02-21-Ch09-System-Function-Syntax/" target="_blank" rel="noopener">第九章：系统功能语法</a></li>
<li><a href="https://yam.gift/2019/02/27/NLPFA/2019-02-27-Ch10-Formal-Model-of-Pragmatic-Automatic-Processing/" target="_blank" rel="noopener">第十章：语用自动处理的形式模型</a></li>
<li><a href="https://yam.gift/2019/03/01/NLPFA/2019-03-01-Ch11-Probabilistic-Grammar/" target="_blank" rel="noopener">第十一章：概率语法</a></li>
<li><a href="https://yam.gift/2019/03/11/NLPFA/2019-03-11-Ch12-Bayes-and-Dynamic-Programming/" target="_blank" rel="noopener">第十二章：Bayes 公式与动态规划算法</a></li>
<li><a href="https://yam.gift/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/" target="_blank" rel="noopener">第十三章：N 元语法和数据平滑</a></li>
<li><a href="https://yam.gift/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/" target="_blank" rel="noopener">第十四章：隐 Markov 模型</a></li>
<li><a href="https://yam.gift/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/" target="_blank" rel="noopener">第十五章：语音自动处理的形式模型</a></li>
<li><a href="https://yam.gift/2018/09/05/NLPFA/2019-04-04-Ch16-Formal-Model-in-Statistical-Machine-Translation/" target="_blank" rel="noopener">第十六章：统计机器翻译中的形式模型</a></li>
<li><a href="https://yam.gift/2018/09/05/NLPFA/2019-04-08-Ch17-NLP-System-Evaluation/" target="_blank" rel="noopener">第十七章：自然语言处理系统评测</a></li>
<li><a href="https://yam.gift/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/" target="_blank" rel="noopener">第十八章：自然语言处理中的理性主义与经验主义</a></li>
</ul>
        
          <p class="article-more-link">
            <a href="/2019/04/13/NLPFA/2019-04-13-NLP-Formal-Analysis/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/04/13/NLPFA/2019-04-13-NLP-Formal-Analysis/">
    <time datetime="2019-04-13T06:00:00.000Z" class="entry-date">
        2019-04-13
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Analysis/">Formal Analysis</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
    <article id="post-SLP/2019-04-09-Information-Extraction" class="post-SLP/2019-04-09-Information-Extraction post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/04/09/SLP/2019-04-09-Information-Extraction/">Information Extraction Note</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/04/09/SLP/2019-04-09-Information-Extraction/" data-id="cjuti5gsg0073i4cc1t0ce504" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>Recently, I wanted to build an information extraction system, so I searched for Google. However there were little Chinese articles, the quality was not so good as well. Fortunately, I found several English ones seemed well, and then the summary is here. The whole structure is based on my favorite NLP book <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">Speech and Language Processing</a> (use SLP instead below), also with some other materials in the reference.</p>
<p>Information extraction (IE), turns the unstructured information extraction information embedded in texts into structured data, for example for populating a relational database to enable further processing. Here is a figure of: Simple Pipeline Architecture for an Information Extraction System.</p>
<blockquote>
<p>From: <a href="https://www.nltk.org/book/ch07.html" target="_blank" rel="noopener">https://www.nltk.org/book/ch07.html</a></p>
<p>By the way, this book provides actionable steps, focusing on specific actions.</p>
</blockquote>
<p><img src="https://www.nltk.org/images/ie-architecture.png" alt="architecture"></p>
        
          <p class="article-more-link">
            <a href="/2019/04/09/SLP/2019-04-09-Information-Extraction/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/04/09/SLP/2019-04-09-Information-Extraction/">
    <time datetime="2019-04-09T14:00:00.000Z" class="entry-date">
        2019-04-09
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/IE/">IE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Information-Extraction/">Information Extraction</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP" class="post-NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/">自然语言计算机形式分析的理论与方法笔记(Ch18)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/" data-id="cjuti5grs0052i4cc2mjj1r5e" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <h1 id="第十八章：自然语言处理中的理性主义与经验主义"><a href="#第十八章：自然语言处理中的理性主义与经验主义" class="headerlink" title="第十八章：自然语言处理中的理性主义与经验主义"></a>第十八章：自然语言处理中的理性主义与经验主义</h1><ul>
<li>理性主义：以生成语言学为基础的方法</li>
<li>经验主义：以大规模语料库的分析为基础的方法</li>
</ul>
        
          <p class="article-more-link">
            <a href="/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/">
    <time datetime="2019-04-08T15:46:00.000Z" class="entry-date">
        2019-04-08
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Philosophy/">Philosophy</a></li></ul>

    </footer>
</article>





  
    <article id="post-2019-03-31-Nabokov-Favorite-Word" class="post-2019-03-31-Nabokov-Favorite-Word post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/31/2019-03-31-Nabokov-Favorite-Word/">《纳博科夫最喜欢的词》读书笔记与思考</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/31/2019-03-31-Nabokov-Favorite-Word/" data-id="cjuti5gqf003hi4cclhshicrb" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>作家们确实有各自的风格，而且是可以进行预测的。事实证明，所有书籍的作者都在写作中不断重复自己的遣词造句和行文方式。</p>
        
          <p class="article-more-link">
            <a href="/2019/03/31/2019-03-31-Nabokov-Favorite-Word/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/31/2019-03-31-Nabokov-Favorite-Word/">
    <time datetime="2019-03-31T15:23:00.000Z" class="entry-date">
        2019-03-31
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Thinking/">Thinking</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Style/">Style</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing" class="post-NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/">自然语言计算机形式分析的理论与方法笔记(Ch15)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/" data-id="cjuti5gsz0083i4ccyjt2jvzt" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <h1 id="第十五章：语音自动处理的形式模型"><a href="#第十五章：语音自动处理的形式模型" class="headerlink" title="第十五章：语音自动处理的形式模型"></a>第十五章：语音自动处理的形式模型</h1><p>语音自动处理主要包括：自动语音识别(ASR) 和文本-语音转换(TTS)。</p>
        
          <p class="article-more-link">
            <a href="/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/">
    <time datetime="2019-03-29T03:32:00.000Z" class="entry-date">
        2019-03-29
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-03-22-Ch14-HMM" class="post-NLPFA/2019-03-22-Ch14-HMM post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/">自然语言计算机形式分析的理论与方法笔记(Ch14)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/" data-id="cjuti5grq004yi4cc3vfya00c" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <h1 id="第十四章：隐-Markov-模型"><a href="#第十四章：隐-Markov-模型" class="headerlink" title="第十四章：隐 Markov 模型"></a>第十四章：隐 Markov 模型</h1><h2 id="HMM-概述"><a href="#HMM-概述" class="headerlink" title="HMM 概述"></a>HMM 概述</h2><p>Markov 链也就是加权自动机。HMM 增加了要求：</p>
<ul>
<li>HMM 有一个观察符号的集合 O，这个集合中的符号不是从状态集合 Q 中的字母抽取的</li>
<li>HMM 中观察似然度函数 B 的值不只限于 1 或 0，概率 bi(ot) 可以取 0-1 之间的任何值</li>
</ul>
<p>HMM 要求的参数如下：</p>
<ul>
<li>状态序列：Q = (q1, q2, …, qn)</li>
<li>观察序列：O = (o1, o2, …, on)</li>
<li>转换概率：A = (a01, a02, …, ann)</li>
<li>观察似然度：B = bi(ot)，表示从状态 i 生成观察值 ot 的概率</li>
<li>初始状态概率分布：π，πi 是 HMM 在状态 i 开始时的概率</li>
<li>接收状态：合法的接收状态集合</li>
</ul>
<p>需要求解的是 A B π，因此一般使用 λ = {A, B, π} 来定义一个 HMM 模型，模型对外表现出来的是观察序列，状态序列不能直接观察到，被称为 “隐变量”。</p>
<p>三个基本问题：</p>
<ul>
<li>评估问题：给定观察序列 O 和模型 λ，如何计算由该模型产生该观察序列的概率 P(O|λ)</li>
<li>解码问题：给定观察序列 O 和模型 λ，如何获取在某种意义下最优的状态序列 Q，一般使用 Viterbi 算法</li>
<li>训练问题：如何选择或调整模型参数 λ，使得在该模型下产生观察序列 O 的概率 P(O|λ) 最大</li>
</ul>
        
          <p class="article-more-link">
            <a href="/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/">
    <time datetime="2019-03-22T03:32:00.000Z" class="entry-date">
        2019-03-22
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HMM/">HMM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Markov/">Markov</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing" class="post-NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/">自然语言计算机形式分析的理论与方法笔记(Ch13)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/" data-id="cjuti5gro004vi4cc721zdybe" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <h1 id="第十三章：N-元语法和数据平滑"><a href="#第十三章：N-元语法和数据平滑" class="headerlink" title="第十三章：N 元语法和数据平滑"></a>第十三章：N 元语法和数据平滑</h1><h2 id="N-元语法"><a href="#N-元语法" class="headerlink" title="N 元语法"></a>N 元语法</h2><p>N 元语法模型利用前面 N-1 个单词来预测下一个词。一些特殊情况：标点、大小写、屈折变化等。</p>
<p>一个单词的概率只依赖于它前面一个单词的这种假设叫作 Markov 假设，这样的模型叫 Bi-gram，即二元语法模型，也叫一阶 Markov 模型。</p>
<p>N 元语法模型可以使用训练语料库 “归一化” 得到。</p>
<script type="math/tex; mode=display">p(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n)}{\sum_w C(w_{n-1}w)}​</script><p>以 <script type="math/tex">w_{n-1}</script> 开头的二元语法计数必定等于 <script type="math/tex">w_{n-1}</script> 这个单词的计数，于是：</p>
<script type="math/tex; mode=display">p(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n)}{C(w_{n-1})}</script><p>一般化 N 元语法的参数估计：</p>
<script type="math/tex; mode=display">p(w_n|w_{n-N+1}^{n-1}) = \frac {C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}</script><p>两个重要事实：</p>
<ul>
<li>N 增加时，精确度相应增加，同时生成句子的局限性增加（可选的下个词减少）</li>
<li>严重依赖于语料库</li>
</ul>
        
          <p class="article-more-link">
            <a href="/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/">
    <time datetime="2019-03-15T03:32:00.000Z" class="entry-date">
        2019-03-15
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ngram/">Ngram</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Smoothing/">Smoothing</a></li></ul>

    </footer>
</article>





  
  
    <nav id="pagination">
      <nav id="page-nav">
        <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
      </nav>
    </nav>
  
</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">30</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">9</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2019/05/05/SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification/">Naive Bayes and Sentiment Classification</a>
          </li>
        
          <li>
            <a href="/2019/04/22/SLP/2019-04-22-RegularExpressions-TextNormalization-EditDistance/">Regular Expressions, Text Normalization, and Edit Distance</a>
          </li>
        
          <li>
            <a href="/2019/04/21/2019-04-21-DB-FDW/">DataBase Foreign Data Wrapper</a>
          </li>
        
          <li>
            <a href="/2019/04/13/NLPFA/2019-04-13-NLP-Formal-Analysis/">自然语言计算机形式分析的理论与方法笔记</a>
          </li>
        
          <li>
            <a href="/2019/04/09/SLP/2019-04-09-Information-Extraction/">Information Extraction Note</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a><span class="tag-list-count">33</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backward/">Backward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Business/">Business</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calculus/">Calculus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer-Science/">Computer Science</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DB/">DB</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Clearing/">Data Clearing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Science/">Data Science</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Structure/">Data Structure</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/">DeepLearning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependence/">Dependence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Diary/">Diary</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Economics/">Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Edit-Distance/">Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/">Entropy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Evaluation/">Evaluation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/F1/">F1</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FDW/">FDW</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FSM/">FSM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Analysis/">Formal Analysis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forward/">Forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gan/">Gan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HMM/">HMM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IE/">IE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Industry/">Industry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Extraction/">Information Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LM/">LM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Language-Model/">Language Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalism/">Lexicalism</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Algebra/">Linear Algebra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine/">Machine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markov/">Markov</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">28</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/">Naive Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ngram/">Ngram</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Orientation/">Orientation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Postgresql/">Postgresql</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regex/">Regex</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regular-Expression/">Regular Expression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reqular-Expressions/">Reqular Expressions</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sentiment-Classification/">Sentiment Classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simon/">Simon</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simpson-Paradox/">Simpson Paradox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smoothing/">Smoothing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sort/">Sort</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spell-Check/">Spell Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Style/">Style</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/">System</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test/">Test</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Text-Normalization/">Text Normalization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valence/">Valence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VirtualBox/">VirtualBox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Viterbi/">Viterbi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZhouZhihua/">ZhouZhihua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zipf/">Zipf</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Business/" style="font-size: 10px;">Business</a> <a href="/tags/C/" style="font-size: 12px;">C</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer-Science/" style="font-size: 14px;">Computer Science</a> <a href="/tags/DB/" style="font-size: 10px;">DB</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Science/" style="font-size: 14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 14px;">Data Structure</a> <a href="/tags/DeepLearning/" style="font-size: 14px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 10px;">Diary</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Entropy/" style="font-size: 10px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 12px;">Evaluation</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Formal-Analysis/" style="font-size: 10px;">Formal Analysis</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/LM/" style="font-size: 10px;">LM</a> <a href="/tags/Language-Model/" style="font-size: 10px;">Language Model</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Math/" style="font-size: 12px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/NLP/" style="font-size: 18px;">NLP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Ngram/" style="font-size: 12px;">Ngram</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Philosophy/" style="font-size: 12px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Postgresql/" style="font-size: 10px;">Postgresql</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Python/" style="font-size: 16px;">Python</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Smoothing/" style="font-size: 10px;">Smoothing</a> <a href="/tags/Sort/" style="font-size: 10px;">Sort</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/System/" style="font-size: 12px;">System</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10px;">Viterbi</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2019 Yam
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>