<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>DeBERTa 论文+代码笔记 | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper：[2006.03654] DeBERTa: Decoding-enhanced BERT with Disentangled Attention Code：microsoft/DeBERTa: The implementation of DeBERTa 核心思想：增加位置-内容与内容-位置的自注意力增强位置和内容之间的依赖，用 EMD 缓解 BERT 预训练和精调因为 MASK 造成的">
<meta name="keywords" content="NLP,Transformer,BERT,DeBERTa,Disentangled Attention,EMD">
<meta property="og:type" content="article">
<meta property="og:title" content="DeBERTa 论文+代码笔记">
<meta property="og:url" content="https://www.yam.gift/2020/06/27/Paper/2020-06-27-DeBERTa/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Paper：[2006.03654] DeBERTa: Decoding-enhanced BERT with Disentangled Attention Code：microsoft/DeBERTa: The implementation of DeBERTa 核心思想：增加位置-内容与内容-位置的自注意力增强位置和内容之间的依赖，用 EMD 缓解 BERT 预训练和精调因为 MASK 造成的">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-deberta-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-deberta-2.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-deberta-3.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-deberta-4.jpeg">
<meta property="og:updated_time" content="2021-12-11T13:44:22.643Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DeBERTa 论文+代码笔记">
<meta name="twitter:description" content="Paper：[2006.03654] DeBERTa: Decoding-enhanced BERT with Disentangled Attention Code：microsoft/DeBERTa: The implementation of DeBERTa 核心思想：增加位置-内容与内容-位置的自注意力增强位置和内容之间的依赖，用 EMD 缓解 BERT 预训练和精调因为 MASK 造成的">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/paper-deberta-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Paper/2020-06-27-DeBERTa" class="post-Paper/2020-06-27-DeBERTa post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      DeBERTa 论文+代码笔记
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2020/06/27/Paper/2020-06-27-DeBERTa/" data-id="ckx1vldjd00kwp8bzwn8z1syn" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>Paper：<a href="https://arxiv.org/abs/2006.03654" target="_blank" rel="noopener">[2006.03654] DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a></p>
<p>Code：<a href="https://github.com/microsoft/DeBERTa" target="_blank" rel="noopener">microsoft/DeBERTa: The implementation of DeBERTa</a></p>
<p>核心思想：增加位置-内容与内容-位置的自注意力增强位置和内容之间的依赖，用 EMD 缓解 BERT 预训练和精调因为 MASK 造成的不匹配问题。</p>
<a id="more"></a>
<h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><h3 id="动机和核心问题"><a href="#动机和核心问题" class="headerlink" title="动机和核心问题"></a>动机和核心问题</h3><ul>
<li>一组词的 Attention 不光取决于内容，还和它们的相对位置有关。比如 deep learning 挨在一起时的依赖关系比不在一起时要强（其实中文就更加如此了，所以感觉对中文可能更加有效）。</li>
<li>解决预训练和精调的不匹配问题（精调时没有 MASK）。</li>
</ul>
<p>针对以上问题有针对性地提出解决方案：</p>
<ul>
<li>Disentangled Attention：增加计算 “位置-内容” 和 “内容-位置” 注意力。</li>
<li>Enhanced Mask Decoder：用 EMD 来代替原 BERT 的 SoftMax 层预测遮盖的 Token。因为我们在精调时一般会在 BERT 的输出后接一个特定任务的 Decoder，但是在预训练时却并没有这个 Decoder；所以本文在预训练时用一个两层的 Transformer decoder 和一个 SoftMax 作为 Decoder。</li>
</ul>
<h3 id="模型和算法"><a href="#模型和算法" class="headerlink" title="模型和算法"></a>模型和算法</h3><p><strong>Disentangled Attention</strong></p>
<p>模型架构和 BERT 类似，主要区别就是 Attention 分数的计算额外增加了位置信息。</p>
<p>假设 k 是最大相对距离，δ(i,j) 是 token i 到 j 的相对位置，定义如下：</p>
<script type="math/tex; mode=display">
\delta(i, j)=\left\{\begin{array}{rcc}
0 & \text { for } & i-j \leqslant-k \\
2 k-1 & \text { for } & i-j \geqslant k \\
i-j+k & \text { others }
\end{array}\right.</script><p>默认是 512，也就是说相对距离的范围从 -512 到 512。</p>
<script type="math/tex; mode=display">
Q_{c}=H W_{q, c}, K_{c}=H W_{k, c}, V_{c}=H W_{v, c}, Q_{r}=P W_{q, r}, K_{r}=P W_{k, r}</script><p>除了正常的 QKV 外，另外还定义了 Qr 和 Kr，P 表示所有层之间共享的相对位置 Embedding。</p>
<script type="math/tex; mode=display">
\tilde{A}_{i, j}=
\underbrace{Q_{i}^{c} K_{j}^{c \top}}_{(\mathrm{a}) \text { content-to-content }} + 
\underbrace{Q_{i}^{c} K_{\delta(i, j)}^{r}{\top}}_{(\mathrm{b}) \text { content-to-position }} +
\underbrace{K_{j}^{c} Q_{\delta(j, i)}^{r}{\top}}_{(\mathrm{c}) \text { position-to-content }}</script><p>Aij 表示 Token i 到 j 的 Attention Score 。最后的 Attention 就是上面的三项，进一步计算 H：</p>
<script type="math/tex; mode=display">
\boldsymbol{H}_{o}=\operatorname{softmax}\left(\frac{\tilde{\boldsymbol{A}}}{\sqrt{3 d}}\right) \boldsymbol{V}_{c}</script><p>在具体实现时做了一些优化，也就是重复使用相对位置 Embedding，因为每次使用的其实都是整体的一个子集。</p>
<p>代码和详细的注释如下（做了一些简化）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From https://github.com/microsoft/DeBERTa</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DisentangledSelfAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_attention_heads = config.num_attention_heads</span><br><span class="line">        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)</span><br><span class="line">        self.all_head_size = self.num_attention_heads * self.attention_head_size</span><br><span class="line">        self.in_proj = torch.nn.Linear(config.hidden_size, self.all_head_size*<span class="number">3</span>, bias=<span class="keyword">False</span>) </span><br><span class="line">        self.q_bias = torch.nn.Parameter(</span><br><span class="line">            torch.zeros((self.all_head_size), dtype=torch.float))</span><br><span class="line">        self.v_bias = torch.nn.Parameter(</span><br><span class="line">            torch.zeros((self.all_head_size), dtype=torch.float))</span><br><span class="line">        self.pos_att_type = [<span class="string">"p2c"</span>, <span class="string">"c2p"</span>]</span><br><span class="line">        self.max_relative_positions = config.max_relative_positions</span><br><span class="line">        self.pos_dropout = StableDropout(config.hidden_dropout_prob)</span><br><span class="line">        self.pos_proj = torch.nn.Linear(config.hidden_size, self.all_head_size, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.pos_q_proj = torch.nn.Linear(config.hidden_size, self.all_head_size)</span><br><span class="line">        self.dropout = StableDropout(config.attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        new_x_shape = x.size()[:<span class="number">-1</span>] + (self.num_attention_heads, <span class="number">-1</span>)</span><br><span class="line">        x = x.view(*new_x_shape)</span><br><span class="line">        <span class="keyword">return</span> x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, hidden_states, attention_mask, </span></span></span><br><span class="line"><span class="function"><span class="params">                return_att=False, query_states=None, </span></span></span><br><span class="line"><span class="function"><span class="params">                relative_pos=None, rel_embeddings=None)</span>:</span></span><br><span class="line">        <span class="string">"""  Call the module</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            hidden_states (:obj:`torch.FloatTensor`):</span></span><br><span class="line"><span class="string">                Input states to the module usally the output from previous layer, </span></span><br><span class="line"><span class="string">                it will be the Q,K and V in `Attention(Q,K,V)`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            attention_mask (:obj:`torch.ByteTensor`):</span></span><br><span class="line"><span class="string">                An attention mask matrix of shape [`B`, `N`, `N`] </span></span><br><span class="line"><span class="string">                where `B` is the batch size, </span></span><br><span class="line"><span class="string">                `N` is the maxium sequence length in which element [i,j] = `1` means </span></span><br><span class="line"><span class="string">                the `i` th token in the input can attend to the `j` th token.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            return_att (:obj:`bool`, optional):</span></span><br><span class="line"><span class="string">                Whether return the attention maxitrix.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            query_states (:obj:`torch.FloatTensor`, optional):</span></span><br><span class="line"><span class="string">                The `Q` state in `Attention(Q,K,V)`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            relative_pos (:obj:`torch.LongTensor`):</span></span><br><span class="line"><span class="string">                The relative position encoding between the tokens in the sequence. </span></span><br><span class="line"><span class="string">                It's of shape [`B`, `N`, `N`] with values ranging in </span></span><br><span class="line"><span class="string">                [`-max_relative_positions`, `max_relative_positions`].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">            rel_embeddings (:obj:`torch.FloatTensor`):</span></span><br><span class="line"><span class="string">                The embedding of relative distances. </span></span><br><span class="line"><span class="string">                It's a tensor of shape </span></span><br><span class="line"><span class="string">                [:math:`2 \\times \\text&#123;max_relative_positions&#125;`, `hidden_size`].</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># (batch_size, seq_len, hidden_size * 3)</span></span><br><span class="line">        qp = self.in_proj(hidden_states)</span><br><span class="line">        <span class="comment"># (batch_size, num_attention_heads, seq_len, 3 * attention_head_size).chunk(3, dim=-1) =&gt;</span></span><br><span class="line">        <span class="comment"># (batch_size, num_attention_heads, seq_len, attention_head_size)</span></span><br><span class="line">        query_layer,key_layer, value_layer = self.transpose_for_scores(qp).chunk(<span class="number">3</span>, dim=<span class="number">-1</span>)</span><br><span class="line">        </span><br><span class="line">        query_layer += self.transpose_for_scores(self.q_bias.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>))</span><br><span class="line">        value_layer += self.transpose_for_scores(self.v_bias.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        rel_att = <span class="keyword">None</span></span><br><span class="line">        <span class="comment"># Take the dot product between "query" and "key" to get the raw attention scores.</span></span><br><span class="line">        scale_factor = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'c2p'</span> <span class="keyword">in</span> self.pos_att_type:</span><br><span class="line">            scale_factor += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'p2c'</span> <span class="keyword">in</span> self.pos_att_type:</span><br><span class="line">            scale_factor += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'p2p'</span> <span class="keyword">in</span> self.pos_att_type:</span><br><span class="line">            scale_factor += <span class="number">1</span></span><br><span class="line">        scale = math.sqrt(query_layer.size(<span class="number">-1</span>)*scale_factor)</span><br><span class="line">        query_layer = query_layer/scale</span><br><span class="line">        <span class="comment"># (batch_size, num_attention_heads, query_size, key_size)</span></span><br><span class="line">        attention_scores = torch.matmul(query_layer, key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 本文定义的额外计算 Attention 分数</span></span><br><span class="line">        rel_embeddings = self.pos_dropout(rel_embeddings)</span><br><span class="line">        <span class="comment"># (batch_size, num_attention_heads, query_size, key_size)</span></span><br><span class="line">        rel_att = self.disentangled_att_bias(query_layer, key_layer, </span><br><span class="line">                                             relative_pos, rel_embeddings, </span><br><span class="line">                                             scale_factor)</span><br><span class="line"></span><br><span class="line">        attention_scores = attention_scores + rel_att</span><br><span class="line">        </span><br><span class="line">        attention_probs = XSoftmax.apply(attention_scores, attention_mask, <span class="number">-1</span>)</span><br><span class="line">        attention_probs = self.dropout(attention_probs)</span><br><span class="line"></span><br><span class="line">        context_layer = torch.matmul(attention_probs, value_layer)</span><br><span class="line">        context_layer = context_layer.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        new_context_layer_shape = context_layer.size()[:<span class="number">-2</span>] + (<span class="number">-1</span>,)</span><br><span class="line">        context_layer = context_layer.view(*new_context_layer_shape)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> (context_layer, attention_probs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">disentangled_att_bias</span><span class="params">(self, </span></span></span><br><span class="line"><span class="function"><span class="params">                              query_layer, </span></span></span><br><span class="line"><span class="function"><span class="params">                              key_layer, </span></span></span><br><span class="line"><span class="function"><span class="params">                              relative_pos, </span></span></span><br><span class="line"><span class="function"><span class="params">                              rel_embeddings, </span></span></span><br><span class="line"><span class="function"><span class="params">                              scale_factor)</span>:</span></span><br><span class="line">        <span class="comment"># query_layer: (batch_size, num_attention_heads, query_seq_len, attention_head_size)</span></span><br><span class="line">        <span class="comment"># key_layer: like query_layer</span></span><br><span class="line">        <span class="comment"># relative_pos: (1, query_size, key_size)</span></span><br><span class="line">        <span class="comment"># rel_embeddings: (max_relative_positions*2, hidden_size)</span></span><br><span class="line">        <span class="comment"># scale_factor: 3</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (1, query_size, key_size) =&gt; (1, 1, query_size, key_size)</span></span><br><span class="line">        relative_pos = relative_pos.unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># an int number</span></span><br><span class="line">        att_span = min(max(query_layer.size(<span class="number">-2</span>), key_layer.size(<span class="number">-2</span>)), </span><br><span class="line">                       self.max_relative_positions)</span><br><span class="line">        relative_pos = relative_pos.long().to(query_layer.device)</span><br><span class="line">        <span class="comment"># (1, att_span*2, hidden_size)</span></span><br><span class="line">        <span class="comment"># 层间共享的 P</span></span><br><span class="line">        rel_embeddings = rel_embeddings[</span><br><span class="line">            self.max_relative_positions - att_span:</span><br><span class="line">            self.max_relative_positions + att_span, :].unsqueeze(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 位置 Kr</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'c2p'</span> <span class="keyword">in</span> self.pos_att_type:</span><br><span class="line">            <span class="comment"># without bias</span></span><br><span class="line">            <span class="comment"># (1, att_span*2, hidden_size)</span></span><br><span class="line">            pos_key_layer = self.pos_proj(rel_embeddings)</span><br><span class="line">            <span class="comment"># (1, num_attention_heads, att_span*2, attention_head_size)</span></span><br><span class="line">            pos_key_layer = self.transpose_for_scores(pos_key_layer)</span><br><span class="line">        <span class="comment"># 位置 Qr</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'p2c'</span> <span class="keyword">in</span> self.pos_att_type:</span><br><span class="line">            <span class="comment"># with bias</span></span><br><span class="line">            <span class="comment"># (1, att_span*2, hidden_size)</span></span><br><span class="line">            pos_query_layer = self.pos_q_proj(rel_embeddings)</span><br><span class="line">            <span class="comment"># (1, num_attention_heads, att_span*2, attention_head_size)</span></span><br><span class="line">            pos_query_layer = self.transpose_for_scores(pos_query_layer)</span><br><span class="line"></span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        <span class="comment"># content-&gt;position</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'c2p'</span> <span class="keyword">in</span> self.pos_att_type:</span><br><span class="line">            <span class="comment"># (batch_size, num_attention_heads, query_size, att_span * 2)</span></span><br><span class="line">            c2p_att = torch.matmul(query_layer, pos_key_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">            <span class="comment"># (1, 1, query_size, key_size)  # i-j+k, [0, 2*k)</span></span><br><span class="line">            c2p_pos = torch.clamp(relative_pos + att_span, <span class="number">0</span>, att_span*<span class="number">2</span><span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch_size, num_attention_heads, query_size, key_size)</span></span><br><span class="line">            <span class="comment"># expand(batch_size, num_attention_heads, query_size, key_size)</span></span><br><span class="line">            c2p_att = torch.gather(c2p_att, dim=<span class="number">-1</span>, index=c2p_pos.expand(</span><br><span class="line">                [query_layer.size(<span class="number">0</span>), </span><br><span class="line">                 query_layer.size(<span class="number">1</span>), </span><br><span class="line">                 query_layer.size(<span class="number">2</span>), </span><br><span class="line">                 relative_pos.size(<span class="number">-1</span>)]))</span><br><span class="line">            score += c2p_att</span><br><span class="line"></span><br><span class="line">        <span class="comment"># position-&gt;content</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'p2c'</span> <span class="keyword">in</span> self.pos_att_type:</span><br><span class="line">            pos_query_layer /= math.sqrt(pos_query_layer.size(<span class="number">-1</span>)*scale_factor)</span><br><span class="line">            <span class="comment"># j-i+k, [0, 2*k), δ(j,i)</span></span><br><span class="line">            p2c_pos = torch.clamp(-relative_pos + att_span, <span class="number">0</span>, att_span*<span class="number">2</span><span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># (batch_size, num_attention_heads, key_size, att_span * 2)</span></span><br><span class="line">            p2c_att = torch.matmul(key_layer, pos_query_layer.transpose(<span class="number">-1</span>, <span class="number">-2</span>))</span><br><span class="line">            <span class="comment"># expand(batch_size, num_attention_heads, key_size, query_size)</span></span><br><span class="line">            <span class="comment"># transpose to =&gt; (batch_size, num_attention_heads, query_size, key_size)</span></span><br><span class="line">            p2c_att = torch.gather(p2c_att, dim=<span class="number">-1</span>, index=p2c_pos.expand(</span><br><span class="line">                [key_layer.size(<span class="number">0</span>), </span><br><span class="line">                 key_layer.size(<span class="number">1</span>), </span><br><span class="line">                 key_layer.size(<span class="number">2</span>), </span><br><span class="line">                 relative_pos.size(<span class="number">-2</span>)])).transpose(<span class="number">-1</span>, <span class="number">-2</span>)</span><br><span class="line">            <span class="comment"># expand 里面稍微改了一下，以前是这样的：</span></span><br><span class="line">            <span class="comment"># [query_layer.size(0), query_layer.size(1), key_layer.size(-2), key_layer.size(-2)]</span></span><br><span class="line">            score += p2c_att</span><br><span class="line">        <span class="keyword">return</span> score</span><br></pre></td></tr></table></figure>
<p>发现里面并没有对 c2p 做 scale，不知道是不是忘记了还是其他原因，有知道的小伙伴还望指教一下。</p>
<p>这里最重要的就是这个 <code>disentangled_att_bias</code> 函数，其他的都和 BERT 基本一致。有几个特别需要注意的点（论文中也提到了）：</p>
<ul>
<li>p2c 的时候，是 δ(j,i) 而不是  δ(i,j)，所以需要稍微做个变换，原因论文也解释了，因为 i 是 position，j 是 content，所以要反过来。也就是说 p2c 实际计算的是 j 位置的 key content 与 query position i 的 Attention。</li>
<li>相对位置的 Embedding 是层间共享的，每次取了其中一个子集。</li>
</ul>
<p>其实由于是 SelfAttention，query_size, key_size, seq_len 都是相等的，因为 qkv 其实都是它自己。所以代码其实可以更加精炼。另外值得注意的是本文实现的几个辅助工具，比如 StableDropout, MaskedLayerNorm, XSoftmax 等，主要是对自带的模块做了一些优化。</p>
<p><strong>Enhanced Mask Decoder</strong></p>
<p>前面提过这个主要针对的是预训练和精调阶段的不匹配，所以把 BERT 的 SoftMax 替换为 EMD，其实也就是包含一个或多个 Transformer 层（完全成标配的感觉）再接 SoftMax。</p>
<p>另外本文还做了一个改动，就是在将 Encoder 的输出喂进 Decoder 时，将 MASK Token 中 10% 不改变的 Token 编码换成了他们的绝对位置 Embedding，然后再用 MLM 预测。因为这些 Token 虽然不会造成预训练和精调阶段的不匹配，但是却导致了信息泄露——Token 以本身为条件进行预测。</p>
<h3 id="特点和创新"><a href="#特点和创新" class="headerlink" title="特点和创新"></a>特点和创新</h3><ul>
<li>关注了位置和内容之间的注意力。</li>
<li>通过在预训练中加入 Decoder 避免了预训练和精调 MASK 的不匹配问题。</li>
</ul>
<h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><h3 id="如何训练使用"><a href="#如何训练使用" class="headerlink" title="如何训练使用"></a>如何训练使用</h3><p>数据方面和 BERT 一致，训练代码官方并未提供，不过使用倒是提供了开箱即用的方法，可以用 Docker，甚至 pip 安装。具体可以参考官方 <a href="https://github.com/microsoft/DeBERTa" target="_blank" rel="noopener">GitHub</a>。不过并没有提供中文的模型，要想训练只能自己辛苦一下了。由于使用了 EMD，还是有点好奇下游任务中的使用细节，就简单看了下分类任务，发现和正常的 BERT 并无二样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From https://github.com/microsoft/DeBERTa</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SequenceClassificationModel</span><span class="params">(NNModule)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config, num_labels=<span class="number">2</span>, drop_out=None, pre_trained=None)</span>:</span></span><br><span class="line">    super().__init__(config)</span><br><span class="line">    self.num_labels = num_labels</span><br><span class="line">    self.bert = DeBERTa(config, pre_trained=pre_trained)</span><br><span class="line">    self.config = config</span><br><span class="line">    pool_config = PoolConfig(self.config)</span><br><span class="line">    output_dim = self.bert.config.hidden_size</span><br><span class="line">    self.pooler = ContextPooler(pool_config)</span><br><span class="line">    output_dim = self.pooler.output_dim()</span><br><span class="line">    self.classifier = nn.Linear(output_dim, num_labels)</span><br><span class="line">    drop_out = self.config.hidden_dropout_prob <span class="keyword">if</span> drop_out <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> drop_out</span><br><span class="line">    self.dropout = StableDropout(drop_out)</span><br><span class="line">    self.apply(self.init_weights)</span><br><span class="line">    self.bert.apply_state()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, </span></span></span><br><span class="line"><span class="function"><span class="params">              input_ids, type_ids=None, input_mask=None, </span></span></span><br><span class="line"><span class="function"><span class="params">              labels=None, position_ids=None, **kwargs)</span>:</span></span><br><span class="line">    encoder_layers = self.bert(</span><br><span class="line">        input_ids, type_ids, input_mask, </span><br><span class="line">        position_ids=position_ids, output_all_encoded_layers=<span class="keyword">True</span>)</span><br><span class="line">    pooled_output = self.pooler(encoder_layers[<span class="number">-1</span>])</span><br><span class="line">    pooled_output = self.dropout(pooled_output)</span><br><span class="line">    logits = self.classifier(pooled_output)</span><br></pre></td></tr></table></figure>
<p>因为 <code>self.bert</code> 出来的其实就是 Encoder 的所有输出和 Attention（设置为 True 时），和 EMD 没有关系，这里池化用的是 Encoder Layer 最后一层的输出，size 为 <code>(batch_size, seq_len, hidden_size)</code>。</p>
<h3 id="数据和实验"><a href="#数据和实验" class="headerlink" title="数据和实验"></a>数据和实验</h3><p><strong>Large</strong>：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-deberta-1.jpeg" alt=""></p>
<p><strong>Base</strong>：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-deberta-2.jpeg" alt=""></p>
<p><strong>Ablation</strong>：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-deberta-3.jpeg" alt=""></p>
<p>结果表明每个点（EMD，C2P 和 P2C）都挺重要的。</p>
<p><strong>Attention</strong>：</p>
<p><img src="http://qnimg.lovevivian.cn/paper-deberta-4.jpeg" alt=""></p>
<ul>
<li>DeBERTa 并没有观察到明显的对角线效应（Token 注意到自己），这归功于 EMD。</li>
<li>RoBERTa 出现的竖直条纹主要由高频虚词引起，DeBERTa 的主要出现在第一列，表示 <code>[CLS]</code>。因此对于一个好的预训练模型，强调 <code>[CLS]</code> 是可取的，因为它的向量通常用作下游任务中整个输入序列的上下文表示。</li>
</ul>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>从 2018 年开始的基于 Transformer 架构的大型预训练模型：GPT, BERT, RoBERTa, XLNet, UniLM, ELECTRA, T5, ALUM, StructBERT, ERINE。</p>
<p>两篇已有的根据相对位置计算注意力权重的文献：</p>
<ul>
<li>Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer: Generating music with long-term structure. 2018.</li>
<li>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In <em>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</em>, pages 464–468, 2018.</li>
</ul>
<p>本文两个主要创新点虽说都是对现有的一点补充，但感觉还是挺有意义的，尤其是对类似中文、日文这种没分词使用字符作为 Token 的语言，唯一的遗憾是都增加了额外的复杂度（个人对这点很是看重）。另外，Self-Attention 本来就有 c2c，再同时增加 p2c 和 c2p 究竟会不会太过，本文（当然）没有探讨。所以，可能又是一次锦上添花罢了。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2020/06/27/Paper/2020-06-27-DeBERTa/">
    <time datetime="2020-06-27T15:00:00.000Z" class="entry-date">
        2020-06-27
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/BERT/">BERT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/DeBERTa/">DeBERTa</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Disentangled-Attention/">Disentangled Attention</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/EMD/">EMD</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/">Transformer</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2020/07/05/AIQA/2020-07-05-Introduction/" rel="prev"><span class="meta-nav">←</span> QA 小课堂：Introduction</a></span>
    
    
        <span class="nav-next"><a href="/2020/06/25/Paper/2020-06-25-RoBERTa/" rel="next">RoBERTa 论文+代码笔记 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">89</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">19</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2022/02/19/Paper/2022-02-19-ExT5/">ExT5：Towards Extreme Multi-Task Scaling for Transfer Learning</a>
          </li>
        
          <li>
            <a href="/2021/12/26/Diary/2021-12-25-Lion-Dance/">《舞狮少年》观后——信念、文化与希望</a>
          </li>
        
          <li>
            <a href="/2021/12/25/Paper/2021-12-25-MLT-Promote/">Multitask Prompted Training Enables Zero-shot Task Generalization</a>
          </li>
        
          <li>
            <a href="/2021/12/19/Net/2021-12-19-VirtualNetwork/">虚拟网络指南</a>
          </li>
        
          <li>
            <a href="/2021/12/04/Paper/2021-12-04-Prompt/">Pretrain, Prompt and Predict, A Systematic Survey of Prompting Methods in NLP</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.09px;">AI</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Algorithm/" style="font-size: 13.64px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12.73px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 17.27px;">BERT</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/Binary-Search/" style="font-size: 11.82px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 11.82px;">Business</a> <a href="/tags/C/" style="font-size: 10.91px;">C</a> <a href="/tags/CCG/" style="font-size: 10.91px;">CCG</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Classification/" style="font-size: 10.91px;">Classification</a> <a href="/tags/Cognition/" style="font-size: 10.91px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12.73px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.91px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DB/" style="font-size: 10.91px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 14.55px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 16.36px;">Data Structure</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12.73px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 11.82px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dropout/" style="font-size: 10.91px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.91px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.82px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.91px;">Embeddings</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 10.91px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.91px;">Evaluation</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 10px;">Few-Shot</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.82px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GPT-2/" style="font-size: 10px;">GPT-2</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.91px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.91px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 10.91px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.91px;">Knowledge Graph</a> <a href="/tags/LM/" style="font-size: 10.91px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Language-Model/" style="font-size: 10.91px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.91px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MTL/" style="font-size: 10px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 14.55px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.82px;">Managemnt</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.91px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 10.91px;">NER</a> <a href="/tags/NLG/" style="font-size: 10px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.91px;">Ngram</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.91px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.91px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Postgres/" style="font-size: 10.91px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 10.91px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.91px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 10px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.91px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/R-Drop/" style="font-size: 10.91px;">R-Drop</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RaspberryPi/" style="font-size: 10.91px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 13.64px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.91px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.91px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 15.45px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.91px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.91px;">SVM</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.91px;">Search</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Self-Attention/" style="font-size: 11.82px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.91px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.91px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.91px;">Sort</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.91px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.91px;">System</a> <a href="/tags/T5/" style="font-size: 10px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.91px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.27px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.91px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2022 Yam
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>