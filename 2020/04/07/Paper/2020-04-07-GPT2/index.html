<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>GPT-2 论文+代码笔记 | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Paper: Language Models are Unsupervised Multitask Learners Code:   openai/gpt-2: Code for the paper “Language Models are Unsupervised Multitask Learners” minimaxir/gpt-2-simple: Python package to easi">
<meta name="keywords" content="NLP,Transformer,GPT-2,Language Model">
<meta property="og:type" content="article">
<meta property="og:title" content="GPT-2 论文+代码笔记">
<meta property="og:url" content="https://www.yam.gift/2020/04/07/Paper/2020-04-07-GPT2/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Paper: Language Models are Unsupervised Multitask Learners Code:   openai/gpt-2: Code for the paper “Language Models are Unsupervised Multitask Learners” minimaxir/gpt-2-simple: Python package to easi">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/paper-gpt2-1.jpeg">
<meta property="og:updated_time" content="2020-05-03T03:29:30.011Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="GPT-2 论文+代码笔记">
<meta name="twitter:description" content="Paper: Language Models are Unsupervised Multitask Learners Code:   openai/gpt-2: Code for the paper “Language Models are Unsupervised Multitask Learners” minimaxir/gpt-2-simple: Python package to easi">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/paper-gpt2-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Paper/2020-04-07-GPT2" class="post-Paper/2020-04-07-GPT2 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      GPT-2 论文+代码笔记
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2020/04/07/Paper/2020-04-07-GPT2/" data-id="ck9cxxmpg00ck8mccicmczb7z" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>Paper: <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a></p>
<p>Code: </p>
<ul>
<li><a href="https://github.com/openai/gpt-2" target="_blank" rel="noopener">openai/gpt-2: Code for the paper “Language Models are Unsupervised Multitask Learners”</a></li>
<li><a href="https://github.com/minimaxir/gpt-2-simple" target="_blank" rel="noopener">minimaxir/gpt-2-simple: Python package to easily retrain OpenAI’s GPT-2 text-generating model on new texts</a></li>
</ul>
<p>核心思想：基于 Transformer 的更加 General 的语言模型。</p>
<a id="more"></a>
<h2 id="What"><a href="#What" class="headerlink" title="What"></a>What</h2><h3 id="动机和核心问题"><a href="#动机和核心问题" class="headerlink" title="动机和核心问题"></a>动机和核心问题</h3><p>作者认为目前这种对单域数据集进行单任务训练的普遍性，是造成当前系统缺乏普遍性的主要原因。另外，目前最好的方法是预训练模型 + 下游任务的监督训练。本文是将二者结合起来，提供更加 general 的迁移方法，它可以使下游任务能够在 zero-shot 下实施，不需要参数或架构调整。证明了语言模型有在 zero-shot 下执行一系列任务的潜力。所以本文核心点有两个：</p>
<ul>
<li>更加 general 的预训练模型</li>
<li>zero-shot 实施的多下游任务</li>
</ul>
<p>前者是后者的基础，后者是前者的应用。</p>
<h3 id="模型和算法"><a href="#模型和算法" class="headerlink" title="模型和算法"></a>模型和算法</h3><p>其核心是：多领域文本建模，以实现在<strong>不同任务</strong>上的迁移。所以 model 是这样的：p(output | input, task). </p>
<p>具体方法是：基于 Transformer，对 GPT-1 的改进：</p>
<ul>
<li>Layer normalization 移动到每个 sub-block 的 input</li>
<li>最后一个 self-attention block 后面加 layer normalization</li>
<li>在初始化时按 1 /√N 的比例缩放残差层的权重，其中 N 是残差层的数量</li>
<li>Vocabulary 扩展到 50,257</li>
<li>上下文 token 数从 512 调整到 1024</li>
<li>更大的 batch size（512）</li>
</ul>
<p>看这些貌似没啥感觉（当你对底层不理解时，顶层的东西看似懂了的其实都没懂，又称 “司机的知识”），还是代码来的实在。这部分的代码主要来自官方代码。正式开始前，先把参数放出来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HParams</span>:</span></span><br><span class="line">    n_vocab:int=<span class="number">50257</span></span><br><span class="line">    n_ctx:int=<span class="number">1024</span></span><br><span class="line">    n_embd:int=<span class="number">768</span></span><br><span class="line">    n_head:int=<span class="number">12</span></span><br><span class="line">    n_layer:int=<span class="number">12</span></span><br></pre></td></tr></table></figure>
<p><strong>输入</strong></p>
<p>首先是输入的 X：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context = tf.fill([batch_size, <span class="number">1</span>], start_token)</span><br></pre></td></tr></table></figure>
<p>这里的 context 就是 start 不为 None 时的 token，我们假设 batch_size 为 1，<code>tf.fill</code> 就是把 start_token 填充为 <code>[batch_size, 1]</code> 的 shape，后面的 1 是指序列长度，我们是按一个一个 token 依次生成的。</p>
<p>然后是 position 和 token 的 embedding：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># weight of position embedding</span></span><br><span class="line">wpe = tf.compat.v1.get_variable(<span class="string">'wpe'</span>, [hparams.n_ctx, hparams.n_embd],</span><br><span class="line">                                initializer=tf.random_normal_initializer(stddev=<span class="number">0.01</span>))</span><br><span class="line"><span class="comment"># weight of token embedding</span></span><br><span class="line">wte = tf.compat.v1.get_variable(<span class="string">'wte'</span>, [hparams.n_vocab, hparams.n_embd],</span><br><span class="line">                                initializer=tf.random_normal_initializer(stddev=<span class="number">0.02</span>))</span><br></pre></td></tr></table></figure>
<p><code>n_ctx</code> 是上下文的 token 数，<code>n_vocab</code> 是词表大小。合并两个 embedding：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))</span><br></pre></td></tr></table></figure>
<p><code>position_for</code> 的目的是给 position 编码，它的结果是和 x 类似的一个 Tensor，可以看出这里采用的是绝对位置编码。<code>tf.gather</code> 就是根据索引从参数轴收集切片，即 X 这里的 one hot token 编码使用 wte 索引 X 的向量，和 lookup 类似。这样就得到了最终的输入 h，它的 shape 为：<code>[batch_size, 1, embedding_shape]</code>，在本例中即为：<code>[1, 1, 768]</code>。</p>
<p><strong>Transformer</strong></p>
<p>接下来就是核心的模型部分了，也就是我们熟知的 Transformer。先看一下整体的流程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pasts = tf.unstack(past, axis=<span class="number">1</span>) <span class="keyword">if</span> past <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> [<span class="keyword">None</span>] * hparams.n_layer</span><br><span class="line"><span class="keyword">for</span> layer, past <span class="keyword">in</span> enumerate(pasts):</span><br><span class="line">    h, present = block(h, <span class="string">'h%d'</span> % layer, past=past, hparams=hparams)</span><br><span class="line">    presents.append(present)</span><br><span class="line">    </span><br><span class="line">results[<span class="string">'present'</span>] = tf.stack(presents, axis=<span class="number">1</span>)</span><br><span class="line">h = norm(h, <span class="string">'ln_f'</span>)</span><br></pre></td></tr></table></figure>
<p><code>tf.unstack</code> 是从 axis 维对张量从 R 级调整为 R-1 级，我们从 None 开始，暂时不需要考虑。可以看到就是 n_layer 个 block 叠加，最后做了个归一化处理。<code>tf.stack</code> 和 <code>tf.unstack</code> 的作用恰好相反。</p>
<p>在 block 前，可以先简单看一下 norm 函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">norm</span><span class="params">(x, *, axis=<span class="number">-1</span>, epsilon=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Normalize to mean = 0, std = 1, then do a diagonal affine transform."""</span></span><br><span class="line">    n_state = x.shape[<span class="number">-1</span>]</span><br><span class="line">    g = tf.compat.v1.get_variable(<span class="string">'g'</span>, [n_state], initializer=tf.constant_initializer(<span class="number">1</span>))</span><br><span class="line">    b = tf.compat.v1.get_variable(<span class="string">'b'</span>, [n_state], initializer=tf.constant_initializer(<span class="number">0</span>))</span><br><span class="line">    u = tf.reduce_mean(x, axis=axis, keepdims=<span class="keyword">True</span>) <span class="comment"># 均值</span></span><br><span class="line">    s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=<span class="keyword">True</span>) <span class="comment"># 方差</span></span><br><span class="line">    x = (x - u) * tf.compat.v1.rsqrt(s + epsilon)</span><br><span class="line">    x = x*g + b</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p><code>tf.compat.v1.rsqrt</code> 是计算平方根的倒数，上面这其实就是 z 分数标准化过程。</p>
<p><strong>block</strong></p>
<p>先看 block 是什么：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">block</span><span class="params">(x, scope, *, past, hparams)</span>:</span></span><br><span class="line">	nx = x.shape[<span class="number">-1</span>]</span><br><span class="line">	a, present = attn(norm(x, <span class="string">'ln_1'</span>), <span class="string">'attn'</span>, nx, past=past, hparams=hparams)</span><br><span class="line">	x = x + a</span><br><span class="line">	m = mlp(norm(x, <span class="string">'ln_2'</span>), <span class="string">'mlp'</span>, nx*<span class="number">4</span>, hparams=hparams)</span><br><span class="line">	x = x + m</span><br><span class="line">    <span class="keyword">return</span> x, present</span><br></pre></td></tr></table></figure>
<p>它的结构如下：</p>
<ul>
<li>attention layer：注意力层</li>
<li>skip connection：残差连接</li>
<li>mlp：就按字面理解为感知机吧</li>
<li>skip connection</li>
</ul>
<p><strong>attention</strong></p>
<p>无疑这个地方是最核心的了：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># x: [1, 1, 768], nx: 768, past: None</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attn</span><span class="params">(x, scope, n_state, *, past, hparams)</span>:</span></span><br><span class="line">    c = conv1d(x, <span class="string">'c_attn'</span>, n_state*<span class="number">3</span>)</span><br><span class="line">    q, k, v = map(split_heads, tf.split(c, <span class="number">3</span>, axis=<span class="number">2</span>))</span><br><span class="line">    present = tf.stack([k, v], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> past <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        pk, pv = tf.unstack(past, axis=<span class="number">1</span>)</span><br><span class="line">        k = tf.concat([pk, k], axis=<span class="number">-2</span>)</span><br><span class="line">        v = tf.concat([pv, v], axis=<span class="number">-2</span>)</span><br><span class="line">    a = multihead_attn(q, k, v)</span><br><span class="line">    a = merge_heads(a)</span><br><span class="line">    a = conv1d(a, <span class="string">'c_proj'</span>, n_state)</span><br><span class="line">    <span class="keyword">return</span> a, present</span><br></pre></td></tr></table></figure>
<p>这里出现了好几个操作：<code>conv1d</code>, <code>split_heads</code>, <code>multihead_attn</code>, <code>merge_heads</code>，我们分别来看一下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conv1d</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv1d</span><span class="params">(x, scope, nf, *, w_init_stdev=<span class="number">0.02</span>)</span>:</span></span><br><span class="line">    <span class="comment"># x [1, 1, 768], start is [1,1], nx=768, nf=768*3</span></span><br><span class="line">    *start, nx = shape_list(x)</span><br><span class="line">    w = tf.compat.v1.get_variable(</span><br><span class="line">        <span class="string">'w'</span>, [<span class="number">1</span>, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))</span><br><span class="line">    b = tf.compat.v1.get_variable(</span><br><span class="line">        <span class="string">'b'</span>, [nf], initializer=tf.constant_initializer(<span class="number">0</span>))</span><br><span class="line">    c = tf.reshape(</span><br><span class="line">        	tf.matmul(tf.reshape(x, [<span class="number">-1</span>, nx]), tf.reshape(w, [<span class="number">-1</span>, nf])) + b, </span><br><span class="line">        	start+[nf])</span><br><span class="line">    <span class="comment"># 1*768 × 768*2304 =&gt; 1*2304 reshape to [1, 1, 2304]</span></span><br><span class="line">    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure>
<p>我不知道这里为什么会有这么个操作，不过在 <a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener"> Transformers</a> 源代码中看到这么一句注释：Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2). Basically works like a Linear layer but the weights are transposed. 姑且就这样吧。如果只看输入输出的话，它无非就是把输入的维度做了调整，在这里就是把维度放大 3 倍（真不知道怎么想出来的）。这里的目的是为了后面的 q k v。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_heads</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># From [batch, sequence, features] to [batch, heads, sequence, features]</span></span><br><span class="line">    <span class="comment"># 即：[1, 1, 768] ==&gt; [1, 12, 1, 64]</span></span><br><span class="line">    <span class="keyword">return</span> tf.transpose(split_states(x, hparams.n_head), [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_states</span><span class="params">(x, n)</span>:</span></span><br><span class="line">    <span class="string">"""Reshape the last dimension of x into [n, x.shape[-1]/n]."""</span></span><br><span class="line">    *start, m = shape_list(x) <span class="comment"># [1, 1, 768]</span></span><br><span class="line">    <span class="keyword">return</span> tf.reshape(x, start + [n, m//n]) <span class="comment"># [1, 1, 12, 768//12=64]</span></span><br></pre></td></tr></table></figure>
<p>最后的 shape：<code>[batch, heads, sequence, features]</code> 也就是 q k v 的 shape，也就是把输入 X 的维度分配到 12 个 head 上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attn</span><span class="params">(q, k, v)</span>:</span></span><br><span class="line">    <span class="comment"># q, k, v have shape [batch, heads, sequence, features], [1, 12, 1, 64]</span></span><br><span class="line">    w = tf.matmul(q, k, transpose_b=<span class="keyword">True</span>) <span class="comment"># [1, 12, 1, 1]</span></span><br><span class="line">    w = w * tf.compat.v1.rsqrt(tf.cast(v.shape[<span class="number">-1</span>], w.dtype)) <span class="comment"># q*k^T/√d_k</span></span><br><span class="line">    w = mask_attn_weights(w)</span><br><span class="line">    w = softmax(w)</span><br><span class="line">    a = tf.matmul(w, v)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask_attn_weights</span><span class="params">(w)</span>:</span></span><br><span class="line">    <span class="comment"># w has shape [batch, heads, dst_sequence, src_sequence]</span></span><br><span class="line">    <span class="comment"># where information flows from src to dst.</span></span><br><span class="line">    _, _, nd, ns = shape_list(w)</span><br><span class="line">    b = attention_mask(nd, ns, dtype=w.dtype)</span><br><span class="line">    b = tf.reshape(b, [<span class="number">1</span>, <span class="number">1</span>, nd, ns])</span><br><span class="line">    w = w*b - tf.cast(<span class="number">1e10</span>, w.dtype)*(<span class="number">1</span>-b)</span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_mask</span><span class="params">(nd, ns, *, dtype)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    1's in the lower triangle, counting from the lower right corner.</span></span><br><span class="line"><span class="string">    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), </span></span><br><span class="line"><span class="string">    but doesn't produce garbage on TPUs.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    i = tf.range(nd)[:,<span class="keyword">None</span>]</span><br><span class="line">    j = tf.range(ns)</span><br><span class="line">    m = i &gt;= j - ns + nd <span class="comment"># 只有 nd &gt; ns 时，m 才可能有 False，mask 才生效</span></span><br><span class="line">    <span class="keyword">return</span> tf.cast(m, dtype)</span><br></pre></td></tr></table></figure>
<p><code>tf.cast</code> 是改变类型，w 其实就是 Transformer 中的标准 w 计算方式，后面的这个 <code>mask_attn_weights</code> 是对权重 mask，即 masked self-attention，这里其实没有发生作用，我感觉它的主要作用是  mask 未来的字符，这里的 LM 模型利用的是已生成文本，所以并不需要 mask。<code>softmax</code> 没啥好说的，也是 self-attention 的标准配置。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_heads</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="comment"># Reverse of split_heads</span></span><br><span class="line">    <span class="keyword">return</span> merge_states(tf.transpose(x, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>]))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_states</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""Smash the last two dimensions of x into a single dimension."""</span></span><br><span class="line">    *start, a, b = shape_list(x)</span><br><span class="line">    <span class="keyword">return</span> tf.reshape(x, start + [a*b])</span><br></pre></td></tr></table></figure>
<p>这步是 合并 head 操作，最终的 shape 为 <code>[batch_size, sequence, n_state]</code>，即 <code>[1, 1, 768]</code>。然后后面再接一个 <code>conv1d</code> 就是最终的输出。</p>
<p><strong>mlp</strong></p>
<p>接下来是一个 mlp 层：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mlp</span><span class="params">(x, scope, n_state, *, hparams)</span>:</span></span><br><span class="line">    nx = x.shape[<span class="number">-1</span>]</span><br><span class="line">    h = gelu(conv1d(x, <span class="string">'c_fc'</span>, n_state))</span><br><span class="line">    h2 = conv1d(h, <span class="string">'c_proj'</span>, nx)</span><br><span class="line">    <span class="keyword">return</span> h2</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gelu</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span>*x*(<span class="number">1</span>+tf.tanh(np.sqrt(<span class="number">2</span>/np.pi)*(x+<span class="number">0.044715</span>*tf.pow(x, <span class="number">3</span>))))</span><br></pre></td></tr></table></figure>
<p><code>gelu</code> 是个激活函数，其实就是两层 <code>conv1d</code>，最终 h2 的 shape 和输入的 x 其实一样，只不过中间的隐层这里为 4 倍的 <code>n_state</code>。</p>
<p>所有的 block 跑完后，最后接一个归一化层，就这样 Transformer 部分就执行完了。简单总结一下大致的结构：</p>
<ul>
<li>若干个（n_layer）block + 归一化。</li>
<li>每个 block 包括一个 self_attention 层和 mlp 层，以及分别对应了残差连接。</li>
<li>self_attention 和 mlp 的输入都做了归一化。</li>
<li>self_attention 分为：<code>conv1d =&gt; split_heads =&gt; multihead_attention =&gt; merge_heads =&gt; conv1d</code>。</li>
<li>mlp 分为就是一个两层的 <code>conv1d</code>，其实就是一个感知机。</li>
</ul>
<p><strong>Language model loss</strong></p>
<p>Transformer 之后接的是语言模型的损失函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])</span><br><span class="line">logits = tf.matmul(h_flat, wte, transpose_b=<span class="keyword">True</span>)</span><br><span class="line">logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])</span><br></pre></td></tr></table></figure>
<h3 id="特点和创新"><a href="#特点和创新" class="headerlink" title="特点和创新"></a>特点和创新</h3><ul>
<li>更加 general（更加庞大的数据集和参数）</li>
<li>下游任务 zero-shot</li>
</ul>
<h2 id="How"><a href="#How" class="headerlink" title="How"></a>How</h2><p>官方代码也没提供训练代码，下面这部分的代码部分来自 gpt-2-simple，注意这个并不适用于 Tensorflow 2.0 及以上版本（无法读取预训练模型），其他的倒是没有啥问题。</p>
<h3 id="如何构造数据"><a href="#如何构造数据" class="headerlink" title="如何构造数据"></a>如何构造数据</h3><p>输入的数据可以是最原始的纯文本，只不过进来后需要 Token 化，将文本变成 One-Hot 编码。参考官方的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, text)</span>:</span></span><br><span class="line">    bpe_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> re.findall(pat, text):</span><br><span class="line">        token = <span class="string">''</span>.join(byte_encoder[b] <span class="keyword">for</span> b <span class="keyword">in</span> token.encode(<span class="string">'utf-8'</span>))</span><br><span class="line">        bpe_tokens.extend(encoder[bpe_token] <span class="keyword">for</span> bpe_token <span class="keyword">in</span> bpe(token).split(<span class="string">' '</span>))</span><br><span class="line">    <span class="keyword">return</span> bpe_tokens</span><br><span class="line">text = <span class="string">"I'm loving U."</span></span><br><span class="line"><span class="comment"># 原始 tokens: ['I', "'m", ' loving', ' U', '.']</span></span><br><span class="line"><span class="comment"># unicode tokens: ['I', "'m", 'Ġloving', 'ĠU', '.']</span></span><br><span class="line"><span class="comment"># bpe tokens: [40, 1101, 14442, 471, 13]</span></span><br></pre></td></tr></table></figure>
<p>这个就是把原始的输入 text 转为 one-hot 编码的代码，Tokenize 的模型是 bpe，encoder 类似 word2id，作者这里利用了 unicode 编码，先将 token 换成 unicode 的统一编码，然后再利用 bpe 模型去 token 化。中文的处理方式类似。</p>
<h3 id="如何开始训练"><a href="#如何开始训练" class="headerlink" title="如何开始训练"></a>如何开始训练</h3><p>这里主要是定义好 loss function：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">context = tf.compat.v1.placeholder(tf.int32, [batch_size, <span class="keyword">None</span>])</span><br><span class="line">output = model(hparams=hparams, X=context)</span><br><span class="line">loss = tf.reduce_mean(</span><br><span class="line">    input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">        labels=context[:, <span class="number">1</span>:],</span><br><span class="line">        logits=output[<span class="string">'logits'</span>][:, :<span class="number">-1</span>]))</span><br></pre></td></tr></table></figure>
<p>label 其实就是输入 one-hot 的下一个 token，logits 可以看出没有取最后一个。这里 context 可以批量训练，序列的长度在 GPT-2 中是 1024。</p>
<h3 id="如何使用结果"><a href="#如何使用结果" class="headerlink" title="如何使用结果"></a>如何使用结果</h3><p>使用 gpt-2-simple 结合官方代码，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gpt_2_simple <span class="keyword">as</span> gpt2 </span><br><span class="line"></span><br><span class="line">models_dir = <span class="string">"/path/to/gpt-2/models/"</span></span><br><span class="line">model_name = <span class="string">"124M"</span></span><br><span class="line"></span><br><span class="line">sess = gpt2.start_tf_sess()</span><br><span class="line">gpt2.load_gpt2(sess, model_name=model_name, model_dir=models_dir)</span><br><span class="line"></span><br><span class="line">output = sample_sequence(</span><br><span class="line">    hparams=hparams, length=<span class="number">50</span>, start_token=enc.encoder[<span class="string">'&lt;|endoftext|&gt;'</span>], </span><br><span class="line">    batch_size=<span class="number">1</span>, temperature=<span class="number">1</span>, top_k=<span class="number">40</span>, top_p=<span class="number">1</span>)[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">context_tokens = sampler.sample(<span class="number">1</span>) <span class="comment"># 取一个 token，比如：array([198])</span></span><br><span class="line">out = sess.run(output, feed_dict=&#123;context: batch_size * [context_tokens]&#125;)</span><br><span class="line">text = enc.decode(out[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>如果需要进一步理解，可以仔细阅读下 gpt-2-simple 的代码，可以算是一个最佳实践了。在此基础上调整为中文的也非常方便。</p>
<h3 id="数据和实验"><a href="#数据和实验" class="headerlink" title="数据和实验"></a>数据和实验</h3><p>文章一共做了八个项目的对比实验，分别是：</p>
<ul>
<li>Language Modeling：</li>
<li>Children’s Book Test：考察 LM 在命名实体，名词，动词和介词等不同类别单词上的性能。</li>
<li>LAMBADA：测试系统对文本中的远程依存关系建模的能力。</li>
<li>Winograd Schema Challenge：测量系统解决文本歧义的能力来衡量系统执行常识推理的能力。</li>
<li>Reading Comprehension：测试阅读理解能力及模型根据历史对话记录回答问题的能力。</li>
<li>Summarization</li>
<li>Translation</li>
<li>Question Answering</li>
</ul>
<p>内容着实有点多，就不一一列出来了，重点看一下 LM 的吧。</p>
<p><img src="http://qnimg.lovevivian.cn/paper-gpt2-1.jpeg" alt=""></p>
<h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><ul>
<li>RNN + 1 Billion Word: Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.</li>
<li>更大的数据集：Bajgar, O., Kadlec, R., and Kleindienst, J. Embracing data abundance: Booktest dataset for reading comprehension. arXiv preprint arXiv:1610.00956, 2016.</li>
<li>深入分析了模型的性能如何随模型容量和数据集大小的变化：Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M., Ali, M., Yang, Y., and Zhou, Y. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017.</li>
<li>生成模型如何学习：Karpathy, A., Johnson, J., and Fei-Fei, L. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078, 2015.</li>
<li>生成 Wikipedia 文章的模型也学会了在语言之间翻译名称：Liu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.</li>
<li>过滤和构建网页的大型文本语料库的替代方法：Davies, M. The 14 billion <a href="https://corpus.byu.edu/iWeb/" target="_blank" rel="noopener">https://corpus.byu.edu/iWeb/</a>, 2018.</li>
<li>GloVe 引入全局词频统计：Pennington, J., Socher, R., and Manning, C. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1532–1543, 2014.</li>
<li>Skip-thought 向量：Kiros, R., Zhu, Y., Salakhutdinov, R. R., Zemel, R., Urtasun, R., Torralba, A., and Fidler, S. Skip-thought vectors. In Advances in neural information processing systems, pp. 3294–3302, 2015.</li>
<li>机器翻译模型中的向量表示：McCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pp. 6294–6305, 2017.</li>
<li>基于 RNN 精调：Howard, J. and Ruder, S. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 328–339, 2018.</li>
<li>自然语言推理模型的迁移表示：Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A. Supervised learning of universal sentence representations from natural language inference data. arXiv preprint arXiv:1705.02364, 2017a.</li>
<li>大型多任务训练：Subramanian, S., Trischler, A., Bengio, Y., and Pal, C. J. Learning general purpose distributed sentence representations via large scale multi-task learning. arXiv preprint arXiv:1804.00079, 2018.</li>
<li>预训练模型对 Seq2Seq 模型的 Encoder 和 Decoder 有益：Ramachandran, P., Liu, P. J., and Le, Q. V. Unsupervised pretraining for sequence to sequence learning. arXiv preprint arXiv:1611.02683, 2016.</li>
<li>预训练模型对下游生成任务有效：<ul>
<li>Wolf, T., Sanh, V., Chaumond, J., and Delangue, C. Transfertransfo: A transfer learning approach for neural network based conversational agents. arXiv preprint arXiv:1901.08149, 2019.</li>
<li>Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J. Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:1811.01241, 2018.</li>
</ul>
</li>
</ul>
<h3 id="特殊情况"><a href="#特殊情况" class="headerlink" title="特殊情况"></a>特殊情况</h3><p>训练集和测试集数据重叠问题。好的去重技术：可伸缩的模糊匹配（Scalable fuzzy matching），目前建议在切分训练集和测试集时，将基于 n-gram 重叠的重复数据删除方法作为重要的验证步骤和健全性检查。另一个确定模型性能是否由重复数据导致的方法是，根据数据本身的测试集（而不是另外的数据集）检查性能（其实这也可能有重复）。</p>
<h3 id="打开脑洞"><a href="#打开脑洞" class="headerlink" title="打开脑洞"></a>打开脑洞</h3><p>我：这 GPT-2 怎么看起来好像就是一个 Transformer 的 Language Model 啊？</p>
<p>小 A：可不是么，怎么有点大力出奇迹的感觉呢：更大的词表、更大的 batch size、更长的 context、更多的参数……然后结果就——更加 general……</p>
<p>小 B：也不能这么说，至少它证明了两点，第一，Transformer 的表征能力；第二，大模型的表征能力。而且也让我们的武器库里又多了一件不错的工具不是么？</p>
<p>小 C：确实如此，NLP 领域 LM 是最基本的模型，纵览发展历史，从 N-gram 到 RNN 再到 LSTM 再到 GPT-2，每个模型背后都隐藏着一次大的创新。现在是 Attention 的高光时刻。大家觉得下一个创新点可能在哪里？</p>
<p>小 A：我觉得肯定是某个新东西 + 大力出奇迹！</p>
<p>小 B：那你说这个新东西可能的方向是什么？</p>
<p>小 C：我觉得这样干想不现实，可能还是从人的认知方面去借鉴一些思想。我们再次概览历史，N-gram 可以看作局部建模，RNN 建模范围更广，LSTM 让建模范围更广的同时更加符合人的机制（遗忘、更新等），Transformer 则是关注更广范围内的重点信息，就像人会关注核心词和关键词一样。如果再从人的角度往下推，可能就是背景知识这些外部信息了，因为同样的文本在不同的背景下，我们关注的点可能不一样。也许可以可以借鉴 GloVe 的方式，也许是使用 Knowledge Graph。</p>
<p>小 B：我之前看过一篇关于 AI 建模体系的<a href="https://yam.gift/2018/07/22/2018-07-22-NLP-and-AI/" target="_blank" rel="noopener">文章</a>，作者对不同种类的 AI 方法进行了整合，最终的结论就是多种方法融合。不过那个有点大，而且也是一家之言。</p>
<p>小 A：你们想的可真多，我还就喜欢大力出奇迹，简单粗暴，就是干！</p>
<p>众人：……</p>
<h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><p>补充两篇我觉得不错的博客文章和本文可以相互补充：</p>
<ul>
<li><a href="http://jalammar.github.io/illustrated-gpt2/" target="_blank" rel="noopener">The Illustrated GPT-2 (Visualizing Transformer Language Models) – Jay Alammar – Visualizing machine learning one concept at a time</a></li>
<li><a href="https://amaarora.github.io/2020/02/18/annotatedGPT2.html" target="_blank" rel="noopener">The Annotated GPT-2 | Making commits each day, towards a better future</a></li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2020/04/07/Paper/2020-04-07-GPT2/">
    <time datetime="2020-04-07T04:00:00.000Z" class="entry-date">
        2020-04-07
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GPT-2/">GPT-2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Language-Model/">Language Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Transformer/">Transformer</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2020/04/14/Paper/2020-04-14-Luong-Attention/" rel="prev"><span class="meta-nav">←</span> Luong Attention 论文+代码笔记</a></span>
    
    
        <span class="nav-next"><a href="/2020/03/30/Paper/2020-03-30-Node2Vec/" rel="next">Node2Vec 论文+代码笔记 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">44</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">59</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">13</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19px;">AI</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Algorithm/" style="font-size: 14px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Attention/" style="font-size: 13px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Bert/" style="font-size: 14px;">Bert</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Binary-Search/" style="font-size: 12px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Business/" style="font-size: 12px;">Business</a> <a href="/tags/C/" style="font-size: 11px;">C</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Cognition/" style="font-size: 11px;">Cognition</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 13px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/DB/" style="font-size: 11px;">DB</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 15px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 16px;">Data Structure</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 13px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 11px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 11px;">Elixir</a> <a href="/tags/Embedding/" style="font-size: 10px;">Embedding</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 11px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10px;">Evaluation</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 10px;">Few-Shot</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GPT-2/" style="font-size: 10px;">GPT-2</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Graph/" style="font-size: 11px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 11px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 11px;">Knowledge Graph</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/Language-Model/" style="font-size: 10px;">Language Model</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 11px;">LinkedList</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 12px;">Managemnt</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 11px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NER/" style="font-size: 10px;">NER</a> <a href="/tags/NLG/" style="font-size: 10px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Ngram/" style="font-size: 10px;">Ngram</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Philosophy/" style="font-size: 11px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Postgres/" style="font-size: 11px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-training/" style="font-size: 11px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretraining/" style="font-size: 11px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Psychology/" style="font-size: 11px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18px;">Python</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recursion/" style="font-size: 11px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 11px;">SQL</a> <a href="/tags/SVM/" style="font-size: 11px;">SVM</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 11px;">Search</a> <a href="/tags/Self-Attention/" style="font-size: 12px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 11px;">Sort</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10px;">Summarization</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 11px;">System</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10px;">Viterbi</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2020/10/13/Paper/2020-10-13-FunnelTransformer/">Funnel Transformer 论文笔记</a>
          </li>
        
          <li>
            <a href="/2020/09/26/ML/2020-09-26-ModelFusing/">模型融合</a>
          </li>
        
          <li>
            <a href="/2020/09/24/ML/2020-09-24-ModelParameters/">建模调参</a>
          </li>
        
          <li>
            <a href="/2020/09/21/ML/2020-09-21-FeatureEngineering/">特征工程</a>
          </li>
        
          <li>
            <a href="/2020/09/18/ML/2020-09-18-EDA/">EDA</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AE/">AE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a><span class="tag-list-count">33</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ALBERT/">ALBERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AR/">AR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AUC/">AUC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Accuracy/">Accuracy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Activation/">Activation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Array/">Array</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/">Attention</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backtracking/">Backtracking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backward/">Backward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bahdanau-Attention/">Bahdanau Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bart/">Bart</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bert/">Bert</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bi-LSTM/">Bi-LSTM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Binary-Search/">Binary Search</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blending/">Blending</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Business/">Business</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CRF/">CRF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calculus/">Calculus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Catalan/">Catalan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChatBot/">ChatBot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chi2/">Chi2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cognition/">Cognition</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer/">Computer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer-Science/">Computer Science</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Confusing-Labels/">Confusing Labels</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Coordinate-Ascent/">Coordinate Ascent</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ctrl/">Ctrl</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DB/">DB</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DP/">DP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Clearing/">Data Clearing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Preprocess/">Data Preprocess</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Science/">Data Science</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Structure/">Data Structure</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Database/">Database</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeBERTa/">DeBERTa</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decoder/">Decoder</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepGraph/">DeepGraph</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/">DeepLearning</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependence/">Dependence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Diary/">Diary</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Disentangled-Attention/">Disentangled Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DistilBERT/">DistilBERT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Django/">Django</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dynamic-Mask/">Dynamic-Mask</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EDA/">EDA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EMD/">EMD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ERNIE/">ERNIE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Economics/">Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Elasticsearch/">Elasticsearch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Electra/">Electra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Elixir/">Elixir</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Embedding/">Embedding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Encoder/">Encoder</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/">Entropy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Evaluation/">Evaluation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FDW/">FDW</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FSM/">FSM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feature-Engineering/">Feature Engineering</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feature-based/">Feature-based</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Few-Shot/">Few-Shot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fine-tuning/">Fine-tuning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forward/">Forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Full-Text-Search/">Full-Text-Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Funnel-Transformer/">Funnel Transformer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GELU/">GELU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPT-2/">GPT-2</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPU/">GPU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GSG/">GSG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gan/">Gan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Graph/">Graph</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GraphQL/">GraphQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HMM/">HMM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hard-SVM/">Hard-SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hinge-Loss/">Hinge Loss</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IQR/">IQR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Imbalance-Data/">Imbalance Data</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Industry/">Industry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Theory/">Information Theory</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Isolation-Forest/">Isolation Forest</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Job/">Job</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KKT/">KKT</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KS/">KS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kernel/">Kernel</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kernel-Function/">Kernel Function</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kernel-Method/">Kernel Method</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Keyword/">Keyword</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Knowledge-Graph/">Knowledge Graph</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LOF/">LOF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Language-Model/">Language Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalism/">Lexicalism</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Algebra/">Linear Algebra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Sturcture/">Linear Sturcture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linked-List/">Linked List</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LinkedList/">LinkedList</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lucene/">Lucene</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Luong-Attention/">Luong Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine/">Machine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Manacher/">Manacher</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Managemnt/">Managemnt</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markov/">Markov</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Materialized-Views/">Materialized Views</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Median/">Median</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Model-Evaluation/">Model Evaluation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Module/">Module</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Multi-Head-Attention/">Multi-Head Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Multiway-Tree/">Multiway Tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NER/">NER</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLG/">NLG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLM/">NLM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">48</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLU/">NLU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neo4j/">Neo4j</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ngram/">Ngram</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Occupation/">Occupation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Orientation/">Orientation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/P-R/">P-R</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PEGASUS/">PEGASUS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PageRank/">PageRank</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Palindromic/">Palindromic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pooling/">Pooling</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Position-Encoding/">Position-Encoding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Postgres/">Postgres</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pre-training/">Pre-training</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Precision/">Precision</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pretraining/">Pretraining</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Model/">Probabilistic Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Psychology/">Psychology</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyPI/">PyPI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">19</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Quant/">Quant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Query/">Query</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Queue/">Queue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RELU/">RELU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RFE/">RFE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RMSE/">RMSE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ROC/">ROC</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recall/">Recall</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Recursion/">Recursion</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reformer/">Reformer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regex/">Regex</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regular-Expression/">Regular Expression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relationship-Extraction/">Relationship Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RoBERTa/">RoBERTa</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Rotated-Sorted-Array/">Rotated Sorted Array</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SMO/">SMO</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/">SQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Search/">Search</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Self-Attention/">Self-Attention</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sigmoid/">Sigmoid</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simon/">Simon</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simpson-Paradox/">Simpson Paradox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Slide/">Slide</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smoothing/">Smoothing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Soft-SVM/">Soft-SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Softmax/">Softmax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sort/">Sort</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spell-Check/">Spell Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stack/">Stack</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stacking/">Stacking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stirling/">Stirling</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/StratifiedKFold/">StratifiedKFold</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/String/">String</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Style/">Style</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Substring/">Substring</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Summarization/">Summarization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Swap/">Swap</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/">System</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TanH/">TanH</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Text-Generation/">Text Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TextRank/">TextRank</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Thought/">Thought</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a><span class="tag-list-count">11</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tree/">Tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tuning/">Tuning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valence/">Valence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VirtualBox/">VirtualBox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Viterbi/">Viterbi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Voting/">Voting</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/WOE/">WOE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Work/">Work</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XLNet/">XLNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Z-Score/">Z-Score</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZhouZhihua/">ZhouZhihua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zipf/">Zipf</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/binning/">binning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/knowledge-Graph/">knowledge Graph</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/node2vec/">node2vec</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ssh/">ssh</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2020 Yam
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>