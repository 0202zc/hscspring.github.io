<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>核方法 和 SMO | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="上一部分介绍了硬间隔和软间隔支持向量机，本部分介绍非线性支持向量机（核方法）和序列最小最优化算法。">
<meta name="keywords" content="Machine Learning,SVM,Kernel,Kernel Method,Kernel Function,SMO,Coordinate Ascent">
<meta property="og:type" content="article">
<meta property="og:title" content="核方法 和 SMO">
<meta property="og:url" content="https://www.yam.gift/2020/09/09/ML/2020-09-09-Kernel-SMO/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="上一部分介绍了硬间隔和软间隔支持向量机，本部分介绍非线性支持向量机（核方法）和序列最小最优化算法。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/ml-smo-1.jpeg">
<meta property="og:updated_time" content="2020-09-11T00:33:51.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="核方法 和 SMO">
<meta name="twitter:description" content="上一部分介绍了硬间隔和软间隔支持向量机，本部分介绍非线性支持向量机（核方法）和序列最小最优化算法。">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/ml-smo-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-ML/2020-09-09-Kernel-SMO" class="post-ML/2020-09-09-Kernel-SMO post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      核方法 和 SMO
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2020/09/09/ML/2020-09-09-Kernel-SMO/" data-id="ckx1vldeq00aup8bz3rgoadl2" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p><a href="https://yam.gift/2020/08/13/ML/2020-08-13-SVM-Hard-Soft-KKT/" target="_blank" rel="noopener">上一部分</a>介绍了硬间隔和软间隔支持向量机，本部分介绍非线性支持向量机（核方法）和序列最小最优化算法。</p>
<a id="more"></a>
<div class="toc"><ul class="toc-item"><li><span><a href="#核方法" data-toc-modified-id="核方法-1">核方法</a></span><ul class="toc-item"><li><span><a href="#核函数与核技巧" data-toc-modified-id="核函数与核技巧-1.1">核函数与核技巧</a></span></li><li><span><a href="#正定核" data-toc-modified-id="正定核-1.2">正定核</a></span></li><li><span><a href="#常用核函数" data-toc-modified-id="常用核函数-1.3">常用核函数</a></span></li><li><span><a href="#算法" data-toc-modified-id="算法-1.4">算法</a></span></li></ul></li><li><span><a href="#SMO" data-toc-modified-id="SMO-2">SMO</a></span><ul class="toc-item"><li><span><a href="#两个变量二次规划求解" data-toc-modified-id="两个变量二次规划求解-2.1">两个变量二次规划求解</a></span></li><li><span><a href="#变量选择" data-toc-modified-id="变量选择-2.2">变量选择</a></span></li><li><span><a href="#算法" data-toc-modified-id="算法-2.3">算法</a></span></li><li><span><a href="#实现" data-toc-modified-id="实现-2.4">实现</a></span></li></ul></li><li><span><a href="#参考资料" data-toc-modified-id="参考资料-3">参考资料</a></span></li></ul></div>

<h2 id="核方法"><a href="#核方法" class="headerlink" title="核方法"></a>核方法</h2><p>核方法主要针对非线性可分问题，给定数据集 <code>T = {(x1, y1), ..., (xn, yn)}</code> 其中 xi ∈ R^n，对应的标记为 <code>yi= {-1, +1}</code>，如果能用 R^n 中一个超曲面将正负例正确分开，则称该问题为非线性可分问题。</p>
<p>用通俗的话来说，非线性可分问题就是有非常多的错误（而不是软间隔的一点点错误），完全无法线性可分的问题。</p>
<p>对于这类问题有两种思路：</p>
<ul>
<li>感知机 PLA：大于一层隐层 + 非线性变换可以拟合任意曲线。</li>
<li>非线性可分，经过一个非线性转换将原空间的数据映射到新空间，变为线性可分（高维比低维更易线性可分）。核技巧属于这样的方法。</li>
</ul>
<h3 id="核函数与核技巧"><a href="#核函数与核技巧" class="headerlink" title="核函数与核技巧"></a>核函数与核技巧</h3><p>频率派的优化问题最后都可以使用拉格朗日转为对偶问题进行求解，而对偶问题中有一项是 x 的内积：</p>
<script type="math/tex; mode=display">
\min_\alpha \quad \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_j y_iy_j (x_i · x_j) - \sum_{i=1}^N\alpha_i \quad \\
s.t.\quad \sum_{i=1}^N \alpha_iy_i = 0 ,\quad \alpha_i \ge 0, \quad i = 1, 2, ..., N</script><p>那么对于非线性问题，上式的内积就变成了非线性转换的内积：</p>
<script type="math/tex; mode=display">
\min_\alpha \quad \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_j y_iy_j (\phi(x_i) · \phi(x_j)) - \sum_{i=1}^N\alpha_i</script><p>实际问题中，φ(x) 有时候可能维度很高，导致很难求解（更不用说还要求解内积）。是否有种方法能够不求解 φ 而直接获得内积呢（因为我们实际上并不关心 φ 是什么，而只关心它的内积）？核函数就是这样的方法：</p>
<script type="math/tex; mode=display">
\forall x,x' \in \chi, \exists \phi: \chi \rightarrow \Z \\
s.t. \quad K(x, x') = \phi(x) \cdot \phi(x')</script><p>则称 K 是一个核函数。</p>
<p>此时，假设我们已经找到一个 K，那么可以直接使用 K 得到 φ 的内积，这种取巧的方法称为核技巧。</p>
<p>使用了核方法的 SVM 则称为核 SVM，其基本模型不变（硬间隔或软间隔），只是将其中的内积变为核函数即可求解。</p>
<h3 id="正定核"><a href="#正定核" class="headerlink" title="正定核"></a>正定核</h3><p>使用核技巧的一个很重要的问题就是：<strong>能否直接判断一个给定的函数 K 是不是核函数，或者说 K 满足什么条件才能成为核函数</strong>。通常所说的核函数就是正定核函数。</p>
<p>正定核函数的充要条件（另一个定义）为：</p>
<script type="math/tex; mode=display">
k: \chi \times \chi \rightarrow \R \\
\forall x,z \in \chi \quad {has} \quad K(x,z)</script><p>k 为对称函数。如果 K 满足以下性质：</p>
<ul>
<li>对称性：K(x,z) = K(z,x)</li>
<li>正定性：任取 N 个元素，<code>x, x2, ..., xn ∈ X</code>，对应的 Gram 矩阵是半正定的</li>
</ul>
<p>那么称 K(x,z) 为正定核函数。</p>
<p>回到本节开始的问题，其实就是要证明：</p>
<script type="math/tex; mode=display">
K(x,z) = <\phi(x), \phi(z)> \Leftrightarrow {Gram Matrix\ is\ positive\ semidefinited}</script><p><strong>必要性证明</strong>：</p>
<p>已知 <code>K(x,z) = &lt;φ(x), φ(z)&gt;</code>，证 Gram 矩阵半正定，且 K 对称。</p>
<p>根据内积的对称性可知 K 满足对称性。</p>
<p>要证明正定性，只需要证：存在 α ∈ R^n，α^T · K · α ≥ 0。<br>左边展开结果为一个实数：</p>
<script type="math/tex; mode=display">
\alpha^T \cdot K \cdot \alpha = \left( \alpha_1,\alpha_2, ...\alpha_n \right) 
\begin{pmatrix}
K_{11}  & K_{12} & ... & K_{1n}\\
K_{21}  & K_{21} & ... & K_{21}\\
...     &  ...   & ... & ... &\\
K_{n1}  & K_{n2} & ... & K_{nn}
\end{pmatrix}
\begin{pmatrix}
 \alpha_1\\
 \alpha_2\\
 ...\\
 \alpha_n
\end{pmatrix} \\
= \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j K_{ij} \\
= \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j <\phi(x_i), \phi(x_j)> \\
= \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j \phi(x_i)^T \phi(x_j) \\
= \left[\sum_{i=1}^N \alpha_i \phi(x_i) \right]^T \cdot \sum_{j=1}^N \alpha_j \phi(j) \\
= <\sum_{i=1}^N \alpha_i \phi(x_i), \sum_{j-1}^N \alpha_j \phi(x_j)> \\
= ||\sum_{i=1}^N \alpha_i \phi(x_i)||^2 \\
\ge 0</script><p>所以 K 是半正定的。</p>
<p><strong>充分性证明</strong>：</p>
<p>已知 K 对称且 Gram 矩阵半正定，证 <code>K(x,z) = &lt;φ(x), φ(z)&gt;</code>。以下内容来自李航老师《统计机器学习》。</p>
<p>定义映射：</p>
<script type="math/tex; mode=display">
\phi: x \rightarrow K(\cdot, x)</script><p>定义线性组合：</p>
<script type="math/tex; mode=display">
f(\cdot) = \sum_{i=1}^m \alpha_i K(\cdot, x_i) \\
x_i \in \chi, \alpha_i \in \mathrm {R} , i=1,2,...,m</script><p>由 f 组成的集合 S，由于 S 对加法和数乘封闭，所以 S 构成一个向量空间。</p>
<p>在集合 S 上定义点乘运算：</p>
<script type="math/tex; mode=display">
f \cdot g = \sum_{i=1}^m \sum_{j=1}^l \alpha_i \beta_j K(x_i, z_j)</script><p>证明过程可以参考李航老师《统计机器学习方法》，不再赘述。由此可得：</p>
<script type="math/tex; mode=display">
K(\cdot, x) \cdot f = \sum_{i=1}^m \alpha_i K(x, x_i) = f(x) \\
K(\cdot, x) \cdot K(\cdot, z) = K(x, z)</script><p>此时 S 为再生核希尔伯特空间，因为核 K 具有再生性（满足上面两个条件）。</p>
<p>因此，给定对称函数 K，K 关于 x 的 Gram 矩阵是半正定的，可对 K 构造从 X 到希尔伯特空间 H 的映射：</p>
<script type="math/tex; mode=display">
\phi: x \rightarrow K(\cdot, x)</script><p>再根据第二个条件可得：</p>
<script type="math/tex; mode=display">
K(x,z) = \phi(x) \cdot \phi(z)</script><p>这个证明感觉不是特别理解，然后又看到有用矩阵分解证明的方法：</p>
<p>因为 K 对称，所以</p>
<script type="math/tex; mode=display">
K = V \Lambda V^T \\
let\ \phi(x_i) = [\sqrt{\lambda_1} V_1^{(i)}, ..., \sqrt{\lambda_m} V_m^{(i)}] \\
<\phi(x_i), \phi(x_j)> = \sum_{t=1}^m \lambda_t v_t^{(i)}v_t^{(j)} \\
= (V \Lambda V^t)_{ij} = K_{ij} = k(x_i, x_j)</script><p>其中 V 是由特征向量组成的正交矩阵，Λ 是特征值组成的对角矩阵。</p>
<p>不过这块整体有点迷，没有弄得特别清晰，后来看到一个 MIT 的 <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes" target="_blank" rel="noopener">Lecture Notes</a> 觉得比较清楚，推荐阅读。</p>
<blockquote>
<p>【希尔伯特空间】：完备的、可能是无限维的、被赋予内积的线性空间。</p>
<ul>
<li>线性空间：向量空间（满足加法和数乘）</li>
<li>完备：可理解为对极限操作封闭（存在极限且极限也属于 Hilbert 空间）</li>
<li>内积，满足三个特征：<ul>
<li>对称性：<code>&lt;f, g&gt; = &lt;g, f&gt;, f,g ∈ H</code></li>
<li>正定性：<code>&lt;f, f&gt; ≥ 0, &quot;=&quot; &lt;=&gt; f=0</code></li>
<li>线性：<code>&lt;r1f1 + r2f2, g&gt; = r1&lt;f, g&gt; + r2&lt;f2, g&gt;</code></li>
</ul>
</li>
</ul>
</blockquote>
<h3 id="常用核函数"><a href="#常用核函数" class="headerlink" title="常用核函数"></a>常用核函数</h3><p><strong>多项式核函数</strong>：</p>
<script type="math/tex; mode=display">
K(x, z) = (x \cdot z + 1)^p</script><p>对应的支持向量机是一个 p 次多项式分类器。</p>
<p><strong>高斯核函数</strong>：</p>
<script type="math/tex; mode=display">
K(x,z) = \exp \left( -\frac{||x - z||^2}{2 \sigma^2 } \right)</script><p>对应的支持向量机是高斯径向基函数分类器。</p>
<p><strong>字符串核函数</strong>：</p>
<p>定义在离散集合上的核函数。字符串核是定义在字符串集合上的核函数。</p>
<p>考虑有限字符集 Σ，字符串 s 是从 Σ 中取出的有限个字符序列，s 的长度为 |s|，元素记为 s(1)s(2)…s(|s|)。所有长度为 n 的字符串集合记为 Σ^n，所有字符串的集合记为：</p>
<script type="math/tex; mode=display">
\Sigma^* = \bigcup_{n=0}^{\infty}  \Sigma^n</script><p>假设 S 是长度大于或等于 n 的字符串集合，s ∈ S，字符串集合 S 到特征空间 <script type="math/tex">H_n = R^{\Sigma^n}</script> 的映射为 φn(s)。<script type="math/tex">R^{\Sigma^n}</script> 表示定义在 Σ^n 上的实数空间，其每一维对应一个字符串 u ∈ Σ^n，映射 φn(s) 将字符串 s 对应于空间 R 的一个向量，在 u 维上的取值为：</p>
<script type="math/tex; mode=display">
[\phi_n(s)]_u = \sum_{i: s(i) = u} \lambda^{l(i)}</script><p>其中，0 ≤ λ ≤ 1 是一个衰减参数，l(i) 表示字符串 i 的长度，求和在 s 中<strong>所有与 u 相同的子串上</strong>进行。</p>
<p>两个字符串 s 和 t 上的字符串核函数是基于映射 φn 的特种空间中的内积：</p>
<script type="math/tex; mode=display">
k_n(s,t) = \sum_{u \in \Sigma^n} [\phi_n(s)]_u  [\phi_n(t)]_u  \\
= \sum_{u \in \Sigma^n} \sum_{(i,j): s(i) = t(j) = u} \lambda^{l(i)} \lambda^{l(j)}</script><p>上式给出了字符串 s 和 t 中长度等于 n 的所有子串组成的特征向量的余弦相似度。相同的子串越多，s 和 t 越相似。</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><ul>
<li>输入：数据集 <code>T = {(x1,y1), (x2, y2), ..., (xn, yn)}</code>，其中 <code>xi ∈ R^n, yi ∈ {+1, -1} , i=1,2,...,N</code></li>
<li>输出：分类决策函数</li>
</ul>
<p>利用对偶问题求解算法如下：</p>
<ul>
<li><p>选择适当的核函数 K 和 惩罚参数 C，构造并求解最优化问题：</p>
<script type="math/tex; mode=display">
  \min_\alpha \quad \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i\alpha_j y_iy_j K(x_i, x_j) - \sum_{i=1}^N\alpha_i \quad (1) \\
  s.t.\quad \sum_{i=1}^N \alpha_iy_i = 0 \\
  \quad 0 \le\alpha_i \le C, \quad i = 1, 2, ..., N</script><p>  得到最优解 <code>α* = (α1*, α2*,  ..., αN*)^T</code></p>
</li>
<li><p>根据 KKT 计算 <code>w*</code>，并选择 <code>α*</code> 的一个分量 <code>0 &lt; αj* &lt; C</code> 计算 <code>b*</code>:</p>
<script type="math/tex; mode=display">
  b^* = y_j - \sum_{i=1}^{N} \alpha^* y_i K(x_i, x_j)</script></li>
<li><p>构造决策函数：</p>
<script type="math/tex; mode=display">
  f(x) = {sign} \left( \sum_{i=1}^{N} \alpha_i^* y_i K(x_i, x) + b^* \right)</script></li>
</ul>
<h2 id="SMO"><a href="#SMO" class="headerlink" title="SMO"></a>SMO</h2><p>SMO（Sequential minimal optimization）是一种启发式算法，主要用于解决训练样本很大时，高效地实现支持向量机的学习问题，具体看就是要解决式（1）的凸二次规划对偶问题。</p>
<p>在正式开始之前需要介绍一种优化方法： Coordinate Ascent，假设我们要解决如下优化问题：</p>
<script type="math/tex; mode=display">
\min_{\alpha} W(\alpha_1, \alpha_2, ..., \alpha_n)</script><p>Coordinate ascent 的解决思路如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Loop until convergence: &#123;</span><br><span class="line">	For i = <span class="number">1</span>,...,n &#123;</span><br><span class="line">        α_i := arg min_α W(α<span class="number">1</span>, α<span class="number">2</span>, ..., αn)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在内循环中，固定住除了 ai 的所有元素，通过调整参数 ai 重新优化 W，优化完每个 a 后就能得到最终的最优解。</p>
<p>SMO 的基本思路如下：</p>
<ul>
<li>如果所有变量的解都满足 KKT 条件，那么这个解就是问题的最优解（KKT 是该最优化问题的充要条件）。</li>
<li>否则选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。</li>
</ul>
<p>这里第二步用到的就是  Coordinate Ascent，不过需要注意的是，因为有（1）式的约束条件，所以如果只留一个自由变量而固定住其他变量其实等于固定了所有变量。这里选择两个变量其实也只有一个是自由变量。比如假设 α1 和 α2 为两个变量，α3, α4, …, αn 固定，根据约束条件可得：</p>
<script type="math/tex; mode=display">
\alpha_1 = -y_1 \sum_{i=2}^{N} \alpha_i y_i</script><p>如果 α2 确定，α1 自然也就确定了。</p>
<p>这样就将原始的优化问题变成一个个子问题，因为二次规划问题也是求极小，所以子问题的解应该能更接近原始问题的解。而且子问题可以通过解析法直接求得，这就提高了速度。</p>
<p>SMO 包括两个部分：求解两个变量二次规划的解析方法和选择变量的启发式方法。</p>
<h3 id="两个变量二次规划求解"><a href="#两个变量二次规划求解" class="headerlink" title="两个变量二次规划求解"></a>两个变量二次规划求解</h3><p>假设选择的变量为：α1 和 α2，其他变量固定，原始的优化问题（1）可以写为：</p>
<script type="math/tex; mode=display">
\min_{\alpha_1, \alpha_2} W(\alpha_1, \alpha_2) = \frac{1}{2}K_{11}\alpha_{1}^{2} + \frac{1}{2}K_{22}\alpha_{2}^{2} + y_1y_2K_{12}\alpha_1\alpha_2 - (\alpha_1 + \alpha_2) + y_1\alpha_1\sum_{i=3}^{N} y_i\alpha_iK_{i1}+ y_2\alpha_2\sum_{i=3}^{N} y_i\alpha_iK_{i2} - \sum_{i=3}^{N} \alpha_i \quad (2)\\
s.t. \quad \alpha_1y_1 + \alpha_2y_2 = - \sum_{i=3}^{N} y_i\alpha_i = \zeta \\
\quad 0 \le\alpha_i \le C, \quad i = 1, 2, ..., N</script><p>该问题即为有约束的二次规划问题，可以求得极值。</p>
<p>根据约束条件可知， α1 和 α2 一定在下面这个正方形区域中：</p>
<p><img src="http://qnimg.lovevivian.cn/ml-smo-1.jpeg" alt=""></p>
<p>图片来自 Stanford CS229（第二个参考文献）</p>
<p>同时，α1 也可以表示为：</p>
<script type="math/tex; mode=display">
\alpha_{1}=\left(\zeta-\alpha_{2} y_{2}\right) y_{1}</script><p>于是，目标函数可以写为：</p>
<script type="math/tex; mode=display">
\min_{\alpha_2} W(\alpha_2) = \frac{1}{2}K_{11}(\zeta-\alpha_{2} y_{2})^{2} + \frac{1}{2}K_{22}\alpha_{2}^{2} + y_2K_{12}\alpha_2(\zeta-\alpha_{2} y_{2}) - ((\zeta-\alpha_{2} y_{2})y_1 + \alpha_2) + \\
(\zeta-\alpha_{2} y_{2})\sum_{i=3}^{N} y_i\alpha_iK_{i1}+ y_2\alpha_2\sum_{i=3}^{N} y_i\alpha_iK_{i2} - \sum_{i=3}^{N} \alpha_i \quad (3)\\
s.t. \quad L \le\alpha_2 \le H</script><p>假设式（2）的初始可行解为 <script type="math/tex">\alpha_1^{old}, \alpha_2^{old}</script>，最优解为 <script type="math/tex">\alpha_1^{new}, \alpha_2^{new}</script>，沿着约束方向未剪辑时 α2 的最优解为 <script type="math/tex">\alpha_2^{new, unclipped}</script>，那么：</p>
<script type="math/tex; mode=display">
L = \max(0, \alpha_2^{old} - \alpha_1^{old}) \\
H = \min(C, C +  \alpha_2^{old} - \alpha_1^{old}) \\
s.t. \quad \ y_1 \neq y_2
\\

L = \max(0, \alpha_2^{old} + \alpha_1^{old} - C) \\
H = \min(C, \alpha_2^{old} + \alpha_1^{old}) \\
s.t. \quad \ y_1 = y_2</script><p>为了记录方便，记：</p>
<script type="math/tex; mode=display">
g(x) = \sum_{i=1}^{N} \alpha_i y_i K(x_i, x) + b \\
E_i = g(x_i) - y_i  \\
v_i = \sum_{j=3}^{N} \alpha_j y_j K(x_j, x_i) = g(x_i) - \sum_{j=1}^{2} \alpha_j y_j K(x_j, x_i) - b</script><p>于是目标函数可以表示为：</p>
<script type="math/tex; mode=display">
\min_{\alpha_2} W(\alpha_2) = \frac{1}{2}K_{11}(\zeta-\alpha_{2} y_{2})^{2} + \frac{1}{2}K_{22}\alpha_{2}^{2} + y_2K_{12}\alpha_2(\zeta-\alpha_{2} y_{2}) - ((\zeta-\alpha_{2} y_{2})y_1 + \alpha_2) + \\
(\zeta-\alpha_{2} y_{2})v_1+ y_2 \alpha_2 v_2- \sum_{i=3}^{N} \alpha_i \quad (4)\\
s.t. \quad L \le\alpha_2 \le H</script><p>对 α2 求导并令其等于 0：</p>
<script type="math/tex; mode=display">
（K_{11} + K_{22} - 2K_{12})\alpha_2 = y_2(y_2 - y_1 + \zeta K_{11} - \zeta K_{12} + v_1 - v_2)</script><p>将 <script type="math/tex">\zeta = \alpha_1^{old} y_1 + \alpha_2^{old} y_2</script> 以及 v 代入得：</p>
<script type="math/tex; mode=display">
（K_{11} + K_{22} - 2K_{12}) \alpha_2^{new, unclipped} \\
= y_2((K_{11} + K_{22} - 2K_{12}) \alpha_2^{old} y_2 + y_2 - y_1 + g(x_1) - g(x_2)) \\
= (K_{11} + K_{22} - 2K_{12}) \alpha_2^{old} + y_2(E_1 - E_2)</script><p>于是得：</p>
<script type="math/tex; mode=display">
\alpha_2^{new, unclipped} = \alpha_2^{old} + \frac{y_2(E_1 - E_2)}{K_{11} + K_{22} - 2K_{12}} \quad(5)</script><p>其中：</p>
<script type="math/tex; mode=display">
K_{11} + K_{22} - 2K_{12} = || \phi(x_1) - \phi(x_2)||^2</script><p>φ 是输入空间到特征空间的映射。</p>
<p>进而可得：</p>
<script type="math/tex; mode=display">
\alpha_{2}^{n e w}=\left\{\begin{array}{ll}
H & \text { if } \alpha_{2}^{n e w, \text {unclipped}}>H \\
\alpha_{2}^{\text {new,unclipped}} & \text {if } L \leq \alpha_{2}^{\text {new}, \text {unclipped}} \leq H \\
L & \text { if } \alpha_{2}^{\text {new}, \text {unclipped}}<L
\end{array}\right. \\
\alpha_1^{new}  = \alpha_1^{old}  + y_1 y_2 (\alpha_2^{old} - \alpha_2^{new})</script><h3 id="变量选择"><a href="#变量选择" class="headerlink" title="变量选择"></a>变量选择</h3><p>每个子问题要选择两个变量，其中至少有一个不满足 KKT 条件（如果两个变量都满足 KKT 条件的话，最优解已经得到了）。</p>
<p>第一个变量的选择为外循环，选择违反 KKT 最严重的点。具体地，根据对偶互补条件：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\alpha_{i}=0 & \Rightarrow y_{i}\left(w \cdot x_{i}+b\right) \geq 1 \\
\alpha_{i}=C & \Rightarrow y_{i}\left(w \cdot x_{i}+b\right) \leq 1 \\
0<\alpha_{i}<C & \Rightarrow y_{i}\left(w \cdot x_{i}+b\right)=1
\end{aligned}</script><p>可得：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\alpha_{i}=0 & \Leftrightarrow y_{i}\left(\sum_{j=1}^{N} \alpha_j y_j K(x_j, x_i) + b\right) \geq 1 \\
\alpha_{i}=C & \Leftrightarrow y_{i}\left(\sum_{j=1}^{N} \alpha_j y_j K(x_j, x_i)+b\right) \leq 1 \\
0<\alpha_{i}<C & \Leftrightarrow y_{i}\left(\sum_{j=1}^{N} \alpha_j y_j K(x_j, x_i)+b\right)=1
\end{aligned}</script><p>然后检验样本点（xi, yi）是否满足上面的条件。</p>
<p>在检验过程中，外循环在首先遍历所有满足条件 0&lt;αi&lt;C 的样本点（间隔边界上的支持向量点），检验它们是否满足上述（KKT）条件。如果这些点都满足，则遍历整个数据集，检验它们是否满足。</p>
<p>第二个变量的选择为内循环，假设外循环已经找到了第一个变量 α1，现在要找第二个变量，选择的标准是希望 α2 有足够大的变化（有助于快速收敛）。因为 α1 已定，E1 也已定，根据式（5），要使 α2 最大，如果 E1&gt;0，选择最小的 Ei 作为 E2，如果 E1&lt;0，选择最大的 Ei 作为 E2。</p>
<p>如果按照上面方法找到的 α2 不能使目标函数有足够的下降，则采用启发式方法继续选择 α2：</p>
<ul>
<li>遍历在间隔边界上的点，依次将其对应的变量作为 α2，直到目标函数有足够的下降</li>
<li>如果找不到合适的 α2，就遍历数据集</li>
<li>如果还是找不到就放弃第一个 α1，重新选择一个</li>
</ul>
<p>每次完成两个变量的优化后都要重新计算 b，当 <script type="math/tex">0 < \alpha_1^{new} < C</script> 时，由上面的条件可知：</p>
<script type="math/tex; mode=display">
\sum_{j=1}^{N} \alpha_j y_j K(x_j, x_1) + b = y_1</script><p>于是：</p>
<script type="math/tex; mode=display">
b_1^{new} = y_1 - \sum_{j=3}^{N} \alpha_j y_j K_{j1} - \alpha_1^{new}y_1K_{11} - \alpha_2^{new}y_2K_{21}</script><p>再根据 E 的定义：</p>
<script type="math/tex; mode=display">
E_1 = \sum_{j=3}^{N} \alpha_j y_j K_{j1} + \alpha_1^{old} y_1 K_{11} + \alpha_2^{old} y_1 K_{21} + b^{old} - y_1</script><p>进而可得：</p>
<script type="math/tex; mode=display">
b_1^{new} = -E_1 + \alpha_1^{old} y_1 K_{11} + \alpha_2^{old} y_1 K_{21} + b^{old} - \alpha_1^{new}y_1K_{11} - \alpha_2^{new}y_2K_{21} \\
= -E_1 - y_1K_{11} (\alpha_1^{new} - \alpha_1^{old}) - y_2K_{21} (\alpha_2^{new} - \alpha_2^{old})+ b^{old}</script><p>同理，如果 <script type="math/tex">0 < \alpha_2^{new} < C</script> 可得：</p>
<script type="math/tex; mode=display">
b_2^{new} = -E_2 - y_1K_{12} (\alpha_1^{new} - \alpha_1^{old}) - y_2K_{22} (\alpha_2^{new} - \alpha_2^{old})+ b^{old}</script><p>如果新的 α1 和 α2 都满足条件 0&lt;α&lt;C，那么新的 b1=b2；如果新的 α1 和 α2 是 0 或 C，新的 b1 和 b2 以及它们之间的数都是符合 KKT 条件的阈值，此时选择中点作为新 b。</p>
<p>完成两个变量的优化后，还需更新对应的 Ei 值：</p>
<script type="math/tex; mode=display">
E_i^{new} = \sum_{S} y_j \alpha_jK(x_i, x_j) + b^{new} - y_i</script><p>其中 S 是所有支持向量 xj 的集合。</p>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><ul>
<li>输入：数据集 <code>T = {(x1,y1), (x2, y2), ..., (xn, yn}</code>，其中 <code>xi ∈ R^n, yi ∈ {+1, -1} , i=1,2,...,N</code>，精度 ε</li>
<li>输出：近似解 α</li>
</ul>
<p>步骤如下：</p>
<ul>
<li><p>取初始值 α0 = 0，令 k = 0</p>
</li>
<li><p>选取优化变量 <script type="math/tex">\alpha_1^k, \alpha_2^k</script>，解析求解最优化问题（2），得到最优解 <script type="math/tex">\alpha_1^{k+1}, \alpha_2^{k+1}</script>，更新 α 为 <script type="math/tex">\alpha^{k+1}</script></p>
</li>
<li><p>如果在精度 ε 内满足停机条件：</p>
<script type="math/tex; mode=display">
  \sum_{i=1}^N \alpha_i y_i = 0, \quad 0 \le \alpha_i \le C, \quad i = 1,2, ..., N \\
  y_i \cdot g(x_i) \left\{\begin{matrix}
   \ge 1 & \{x_i | \alpha_i = 0 \} \\
   =1 & \{x_i | 0 \le \alpha_i \le C \} \\
   \le 1 & \{x_i | \alpha_i = C \}
  \end{matrix}\right. \\
  s.t. \quad g(x_i) = \sum_{j=1}^{N} \alpha_j y_j K(x_j, x_i) + b</script><p>  则转下一步，否则令 k=k+1，回到第二步</p>
</li>
<li><p>取近似解 <script type="math/tex">\hat{\alpha} = \alpha^{k+1}</script> </p>
</li>
</ul>
<h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><p>具体实现可以参考<a href="https://github.com/hscspring/AI-Methods/blob/master/ML-DeepUnderstand/SVM.ipynb" target="_blank" rel="noopener">这里</a>，本文主要梳理一下主要的流程。</p>
<ul>
<li>首先是初始化参数，主要包括 α，b 和 E（也就是 error）</li>
<li>接下来就是初始化 α1，根据 SMO 算法，首先选择满足条件 0&lt;αi&lt;C 的样本点，即间隔边界上的支持向量点，检验它们是否满足 KKT 条件。如果这些点都满足，则遍历整个数据集，检验它们是否满足。</li>
<li>找到 α1 后，可以根据 E 的差距最大选择 α2，进而可以求得新的 α，b，E 和支持向量。如果 α2 不能使目标函数有足够的下降，则根据启发式规则重新选择 α2，即：<ul>
<li>遍历在间隔边界上的点，依次作为 α2 直到目标函数有足够的下降</li>
<li>再不行就遍历数据集</li>
<li>还是不行就放弃第一个 α1，重新选择一个</li>
</ul>
</li>
<li>直到计算完所有 α 或达到最大迭代次数为止。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/" target="_blank" rel="noopener">Lecture Notes | Prediction: Machine Learning and Statistics | Sloan School of Management | MIT OpenCourseWare</a></li>
<li><a href="http://cs229.stanford.edu/summer2020/cs229-notes3.pdf" target="_blank" rel="noopener">cs229-notes3.pdf</a></li>
<li><a href="https://book.douban.com/subject/33437381/" target="_blank" rel="noopener">统计学习方法（第2版） (豆瓣)</a></li>
<li><a href="https://www.bilibili.com/video/BV1aE411o7qd?p=28" target="_blank" rel="noopener">【机器学习】【白板推导系列】【合集 1～23】_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2020/09/09/ML/2020-09-09-Kernel-SMO/">
    <time datetime="2020-09-09T04:00:00.000Z" class="entry-date">
        2020-09-09
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Coordinate-Ascent/">Coordinate Ascent</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kernel/">Kernel</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kernel-Function/">Kernel Function</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kernel-Method/">Kernel Method</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SMO/">SMO</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SVM/">SVM</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2020/09/13/Paper/2020-09-13-PEGASUS/" rel="prev"><span class="meta-nav">←</span> PEGASUS 论文笔记</a></span>
    
    
        <span class="nav-next"><a href="/2020/08/31/DSA/LeetCode/2020-08-31-Find-First-and-Last-Position-of-Element-in-Sorted-Array/" rel="next">Find First and Last Position of Element in Sorted Array (LeetCode 34) <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">69</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">88</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">19</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2021/12/26/Diary/2021-12-25-Lion-Dance/">《舞狮少年》观后——信念、文化与希望</a>
          </li>
        
          <li>
            <a href="/2021/12/25/Paper/2021-12-25-MLT-Promote/">Multitask Prompted Training Enables Zero-shot Task Generalization</a>
          </li>
        
          <li>
            <a href="/2021/12/19/Net/2021-12-19-VirtualNetwork/">虚拟网络指南</a>
          </li>
        
          <li>
            <a href="/2021/12/04/Paper/2021-12-04-Prompt/">Pretrain, Prompt and Predict, A Systematic Survey of Prompting Methods in NLP</a>
          </li>
        
          <li>
            <a href="/2021/11/28/Paper/2021-11-28-DataAugmentation/">Data Augmentation Approaches in Natural Language Processing：A Survey</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.09px;">AI</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Algorithm/" style="font-size: 13.64px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12.73px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 17.27px;">BERT</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/Binary-Search/" style="font-size: 11.82px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 11.82px;">Business</a> <a href="/tags/C/" style="font-size: 10.91px;">C</a> <a href="/tags/CCG/" style="font-size: 10.91px;">CCG</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Classification/" style="font-size: 10.91px;">Classification</a> <a href="/tags/Cognition/" style="font-size: 10.91px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12.73px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.91px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DB/" style="font-size: 10.91px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 14.55px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 16.36px;">Data Structure</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12.73px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 11.82px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dropout/" style="font-size: 10.91px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.91px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.82px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.91px;">Embeddings</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 10.91px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.91px;">Evaluation</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 10px;">Few-Shot</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.82px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GPT-2/" style="font-size: 10px;">GPT-2</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.91px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.91px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 10.91px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.91px;">Knowledge Graph</a> <a href="/tags/LM/" style="font-size: 10.91px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Language-Model/" style="font-size: 10.91px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.91px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 14.55px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.82px;">Managemnt</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.91px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 10.91px;">NER</a> <a href="/tags/NLG/" style="font-size: 10px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.91px;">Ngram</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.91px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.91px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Postgres/" style="font-size: 10.91px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 10.91px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.91px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 10px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.91px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18.18px;">Python</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/R-Drop/" style="font-size: 10.91px;">R-Drop</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RaspberryPi/" style="font-size: 10.91px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 13.64px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.91px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.91px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 15.45px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.91px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.91px;">SVM</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.91px;">Search</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Self-Attention/" style="font-size: 11.82px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.91px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.91px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.91px;">Sort</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.91px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.91px;">System</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.91px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.27px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.91px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2022 Yam
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>