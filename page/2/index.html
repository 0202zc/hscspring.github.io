<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
<meta property="og:type" content="website">
<meta property="og:title" content="Yam">
<meta property="og:url" content="http://www.yam.gift/page/2/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Yam">
<meta name="twitter:description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="http://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main">
  
    <article id="post-NLPFA/2019-01-31-Ch07-Formal-Model-Based-on-Lexicalism" class="post-NLPFA/2019-01-31-Ch07-Formal-Model-Based-on-Lexicalism post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/01/31/NLPFA/2019-01-31-Ch07-Formal-Model-Based-on-Lexicalism/">自然语言计算机形式分析的理论与方法笔记(Ch07)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/01/31/NLPFA/2019-01-31-Ch07-Formal-Model-Based-on-Lexicalism/" data-id="cju9u9zbs002h5occotqtcpdq" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第七章：基于词汇主义的形式模型"><a href="#第七章：基于词汇主义的形式模型" class="headerlink" title="第七章：基于词汇主义的形式模型"></a>第七章：基于词汇主义的形式模型</h1><h2 id="Gross-的词汇语法"><a href="#Gross-的词汇语法" class="headerlink" title="Gross 的词汇语法"></a>Gross 的词汇语法</h2><p>1975 年，Gross 首次提出 “词汇语法” 的理论，1979 年进一步完善了理论，是一种基于词汇主义的形式化语言理论。词汇语法的理论基础是结构主义语言学。</p>
<ul>
<li><p>坚持索绪尔的纯语言学立场，主张 “语言学唯一的、真正的对象是就语言并为语言而研究的语言”，把语言定义为一种特殊的、带有自然现象许多特点的社会现象，主张从语言的内在结构去研究，即把语言作为音义结合的符号系统来研究，把一切非语言的因素严格限制在一个能把握得了的范围之内。</p>
</li>
<li><p>坚持结构主义方法论原则。即语言是一个结构系统，应当注重各个成分之间关系的探索，重视共时的研究，强调形式的分析和描写。</p>
</li>
<li><p>坚持实证主义的变换方法。</p>
</li>
<li><p>中心思想是词汇及语法，对二者的互动关系探求应当系统地进行。</p>
</li>
<li><p>在理论和实践的关系上，坚持方法的选择必须以应用价值为先导，反对以假设为前提、忽视应用的做法。</p>
</li>
<li><p>主张句法独立，坚持在句法描写中摈弃语义上的先验模式，使语义的描写处于最低量态度，做到 “语义低量”。Gross 坚信，句法可以形式化到相当的应用程度，而语义不可能独立于句法而达到形式化，他不排斥语义，但是主张语义描写的最低量。</p>
<blockquote>
<p>如果从形式化角度来说，语义的确不如句法；但句法的出现和发展也是为了表达语义的。</p>
<p>我现在越来越怀疑语言学的这一系列研究方法，肯定有其价值，但到底是不是正确的方向，或者说究竟有没有正确的方向？  </p>
<p>从某种意义上来说，词汇语法有一定道理，尤其是其基于词汇主义的思想，词很有可能比现有研究所表现出来的更加重要。</p>
</blockquote>
</li>
<li><p>相信一切语言理论和方法都受到语言事实的检验。</p>
</li>
</ul>
<p>操作理据、背景、意图和方法如下：</p>
<ul>
<li>操作理据：基于经验证实原则和客观主义的实证主义。</li>
<li>操作背景：后海里斯主义（Post-Harrisism），Harris 一贯重视语言的形式特征，其核心概念是 “分布”，即某个单位或特征在话语里出现的不同位置的总和，也就是出现其中一切环境的总和。主张采取形式方法，不回避口语材料和反例，以直觉为本，从结构入手进行研究，而把句子的内容（语义）放在次要位置。</li>
<li>操作意图：构建相互有机联系的描写、验证、分类、语料这四种机制<ul>
<li>描写：<ul>
<li>对特定语言进行系统描写的共时机制</li>
<li>集中研究句子得以形成和出现的实证条件</li>
<li>以词汇驱动，尽量进行穷尽性的描写，在具有规模真实语料范围内，对其中相关语言现象全部彻底地逐一描写</li>
<li>建立以句子为基点的语法描写机制，把词汇放到句子中来验证句子的组配规则</li>
<li>描写核心句的成分条件的句法模式，以符合足句条件的核心句为基本观念</li>
</ul>
</li>
<li>验证：把过去的语言学家所提出的语法规则放在一个形式化的、词汇与语法互动的系统中，核实和操作性</li>
<li>分类：建立一套有关处理语料的理论，以便更加系统地收集事实，寻找观念与自省的互补关系，寻找语料及其种种相关形态的层次关系</li>
<li>语料：语言学唯一的研究对象，要建立词库和语法库</li>
</ul>
</li>
</ul>
<p>操作方法分词汇的处理、语法的处理、矩阵的设计和专家的干预。</p>
<ul>
<li>词汇的处理：包括分出核心词汇、确定相关词汇义项的分立以及制定句法表现的合法度</li>
<li>语法的处理：主张句法的研究需要一个明确的、清楚界定了的框架。<ul>
<li><strong>只有从一个个具体的词项出发来引导和控制句法分析，才能从根本上把握词汇和语法的互动关系，这一点正是词汇语法方法论上的本质特征所在。</strong></li>
<li>词汇语法另一个重点是转换。分布和转换是密切联系的两个概念，句法的转换关系联系着相同的词类构成的相关句子集合，而且与单词的特性密切相关。</li>
</ul>
</li>
<li>矩阵的设计：母版是词汇语法形式化的关键，包括纵向标示和横向标示，纵向标示包括句型属性、分布属性、语义属性和相关变换属性；横向标示标示预设的词汇语法的词项。</li>
<li>专家的干预：句法可接受度高于语义，这种可接受度通过专家干预决定：通过各种渠道系统地积集语料，以专家干预为主导处理语料。</li>
</ul>
<p>词汇语法特点：</p>
<ul>
<li>基于表层形态以矩阵作为表达语言信息的媒介，通过具体词汇的各种特性的描写来描述语言的语法规则，对于母版的格式做定量描述，对配价做恒量描述。</li>
<li>不回避口语材料和反例，以直觉为本，强调句法的可接受度，适当注意语义因素。</li>
<li>以词汇驱动，尽量进行穷尽性研究，充分重视词汇的个体特性，要求尽可能高的词汇覆盖面。</li>
<li>以符合足句条件的核心句为基本观念，把词放到句子中来验证句子的组配规则，在结构平面上把对句子里各部分之间形式上的关系放在主导地位上，排除会话情景等语用因素以及篇章分析等修辞因素。</li>
</ul>
<h2 id="链语法"><a href="#链语法" class="headerlink" title="链语法"></a>链语法</h2><p>D.Sleator 和 Temperley 于 1991 年提出，是一种立足于单词链接特性的语法。</p>
<p>单词的链接通过链描述，链有两种：链头和链座。句子通过连接子描述，连接子左边为链座，表示需要向左找对应的链头；连接子右边为链头，表示要向右找对应的链座。链座或链头有 “逻辑或” 关系的，组成该逻辑的对象称为 “选言肢”。链语法由一组词以及这些词相应的连接子组成，连接子由一列逻辑选言肢（逻辑“或”）组成。</p>
<p>句子的链系统应满足的四个条件：</p>
<ul>
<li>平面性：在一个句子上画出的词与词之间的链不交叉</li>
<li>连接性：画出的链可以无遗漏地把这个词序列中的所有词都链接起来</li>
<li>顺序性：在一个选言肢的左链（链座）和右链（链头）中从左到右排列的成分必须同它们分别要链接在其左边的词从近到远的顺序一致</li>
<li>排他性：一对词之间最多只能有一条链相链接</li>
</ul>
<h2 id="词汇语义学"><a href="#词汇语义学" class="headerlink" title="词汇语义学"></a>词汇语义学</h2><p>语言中的词汇具有高度系统化的结构，这种结构决定了单词的意义和用法。这种结构包括单词和它的意义之间的关系以及个别单词的内部结构。对这种系统化的、与意义相关的结构的词汇研究叫 “词汇语义学”。</p>
<p>词位表示词典中一个单独的条目，是一个特定的正字法形式和音素形式与一些符号的意义表示形式的组合。词典是有限个词位的列表。</p>
<p>词位和它的含义（意义）之间的关系包括：</p>
<ul>
<li>同形关系：形式相同意义上没有联系<ul>
<li>同形异义词</li>
<li>同音异义词</li>
</ul>
</li>
<li>多义关系：一个单独的词位具有若干个彼此关联的含义，两种判断方法：<ul>
<li>词源判断法：词源有联系的是多义词，反之为同形词</li>
<li>共轭搭配法：把待判断的两个含义用连接词组合到一个句子中，句子成立是多义词，否则是同形词</li>
</ul>
</li>
<li>同义关系：两个词位具有相同的意义，可以根据 “可替换性” 来定义，可替换性与以下因素有联系：<ul>
<li>多义关系中的某些含义的有无</li>
<li>微妙的意义色彩的差别</li>
<li>搭配约束的不同</li>
<li>使用域的不同</li>
</ul>
</li>
<li>上下位关系：一个词位是另一个词位的次类，则存在上下位关系</li>
</ul>
<h2 id="知识本体"><a href="#知识本体" class="headerlink" title="知识本体"></a>知识本体</h2><ul>
<li><p>关于本地的经典定义（对于存在的研究或科学）来自哲学研究。大师们的观点</p>
<ul>
<li>Parmenides（巴门尼德）：事物的本质独立于感官</li>
<li>Aristotle（亚里士多德）：把存在分为不同的模式，建立了包含 10 个范畴的范畴系统：实体、质量、数量、关系、行动、感情、空间、时间、主动、被动</li>
<li>Kant（康德）：事物的本质不仅仅由事物本身决定，也受到人们对于事物的感知或理解的影响。建立了包含四个大范畴的范畴框架：<ul>
<li>数量：单量、多量、总量</li>
<li>质量：实在质、否定质、限度质</li>
<li>关系：继承、因果、交互</li>
<li>模态：可能性、现实性、必要性</li>
</ul>
</li>
</ul>
</li>
<li><p>计算机处理</p>
<ul>
<li>1991 年，美国计算机专家 R. Niches 提出构建智能系统的两个部分：知识本体+问题求解方法</li>
<li>1990 年，作者提出 “双态原则”：基于语法信息和知识本体的静态标记标注的机器词典 + 基于产生式规则的动态标记求解规则 = 机器翻译系统<ul>
<li>静态标记：存储在机器词典中的词类特征和单词固有的语义特征，与上下文语境无关</li>
<li>动态标记：使用静态标记经过计算得出的句法功能标记、语义关系标记、逻辑关系标记等</li>
</ul>
</li>
</ul>
</li>
<li><p>在人工智能中的定义：知识本体是对概念体系（所描述的客观世界的现象中有关概念的抽象模型）的明确的、形式化的、可共享的规范。（1998 年，Studer）</p>
</li>
<li><p>作者的知识本体系统：ONTOL-MT 初始概念有：事物、时间、空间、数量、行为状态和属性，初始概念下还有不同层次的下位概念。</p>
<ul>
<li>事物：在空间（包括思维空间）上和时间上延展的事物本体。包括物：具体物 + 抽象物和事</li>
<li>时间：运动和变化持续性的表现，物质存在的客观形式。包括时点、时段、时间属性</li>
<li>空间：事物及其运动存在的另一种客观形式。包括场所、距离、途径、方向</li>
<li>数量：包括数值、计量、金额、历时、频次</li>
<li>行为状态：包括物理行为、心理行为、状态、关系、进化、关涉、改动、转移</li>
<li>属性：包括外形、表象、颜色、味道、性质、德才、境况</li>
</ul>
</li>
<li><p>作者把《同义词词林》融入 ONTOL-MT，形成 ONTOL-MT2，共有 14 大类：人、自然物、人造物 工具、抽象物、事、时间、空间、数量、物理行为 动作、心理行为、社会活动、现象和状态、属性、其他</p>
</li>
</ul>
<h2 id="词网"><a href="#词网" class="headerlink" title="词网"></a>词网</h2><p>词网是 1985 年美国普林斯顿大学的 G. A. Miller, R. C. Beckwick, C. Fellbaum 等研制的英语的词汇关系数据库。</p>
<p>词网的三个基本假设：</p>
<ul>
<li>分离性假设：语言中的词汇成分可以从语言中分离出来单独研究</li>
<li>模式化假设：人们倾向于特别关注词语所表达的含义之间的系统模式和关系</li>
<li>完全性假设：系统需要尽可能地把人们的词语知识存储在词网中</li>
</ul>
<p>词网的基本单位是单词，包括动词、名词、形容词-副词数据库。</p>
<ul>
<li>名词：词网的基本语义关系是同义关系。同义词的集合构成了同义词集，叫做 SYNSET。为了表达上的方便，一般只用 SYNSET 中的代表性单词表示 SYNSET。名词数据库使用了 25 个初始概念，经过归纳整理形成了 11 个初始概念：实体、抽象、心理特征、自然现象、活动、事件、集体、位置、所属、外形和状态。<ul>
<li>上下位关系：单词的特定含义之间的关系，代表的是词汇化的概念之间的关系<ul>
<li>由概括性较弱的含义指向较强的含义，叫作普遍化</li>
<li>由概括性较强的含义指向较弱的含义，叫作具体化</li>
</ul>
</li>
<li>部分-整体关系：部分词（Sm）和整体词（Sh），六种类型：<ul>
<li>组成成分-客体（词网使用），如：树枝-树</li>
<li>成员-集体（词网使用），如：树-森林</li>
<li>局部-物质，如：一片蛋糕-蛋糕</li>
<li>材料-客体（词网使用），如：铝-飞机</li>
<li>特征-活动，如：支付-购物</li>
<li>地点-地域，如：普林斯顿-新泽西州</li>
</ul>
</li>
<li>反义关系：具有反义关系的名词的上位词往往是相同的</li>
</ul>
</li>
<li>形容词：修饰名词的都可看做形容词：形容词、名词、现在分词、过去分词、介词短语、小句等<ul>
<li>描写形容词：可以给被它修饰的名词赋上一个属性值，两个特征：<ul>
<li>属性的两极性：表示的属性彼此对立。把直接反义词和间接反义词组织到 ”两极聚类“。</li>
<li>属性的分级性：按不同的属性分级，可以看出形容词含义近似的程度，而形容词表示的属性也会因此显示出方向性（维度）。</li>
</ul>
</li>
<li>关系形容词：由名词派生，与描写形容词的区别：<ul>
<li>关系形容词不涉及所修饰名词的性质，因此与属性无关</li>
<li>关系形容词不能分级</li>
<li>大多数关系形容词没有直接反义词</li>
</ul>
</li>
</ul>
</li>
<li>副词</li>
<li>动词，14 个语义领域：运动、感知、接触、交际、竞争、变化、认知、消耗、创造、情绪、占有、身体保健和功能、社会行为、交互。有些语义领域需要使用若干个独立的树形结构表示。使用 ”承袭“ 描述两个动词间的关系，承袭关系具有如下性质：<ul>
<li>单向关系</li>
<li>彼此承袭必定是同义词</li>
<li>否定可以改变承袭关系</li>
<li>否定承袭的一方会造成矛盾</li>
<li>具有承袭关系的动词在时间上存在着联系</li>
<li>在时间上的包含关系</li>
</ul>
</li>
</ul>
<h2 id="知网"><a href="#知网" class="headerlink" title="知网"></a>知网</h2><p>董振东和董强研制，是一个词典知识描述系统。在知网中，每一个词语的概念及其描述构成一个记录，每一记录有八项内容：</p>
<ul>
<li>W_C=汉语词语</li>
<li>G_C=汉语词语的词性</li>
<li>E_C=汉语词语的例子</li>
<li>W_E=英语词语</li>
<li>G_E=英语词语的词性</li>
<li>E_E=英语词语的例子</li>
<li>Def=概念类别和属性：类别，属性1，属性2……</li>
</ul>
<p>知网的设计者认为，世界上的一切事物都在一定的时间和空间内不停地运动和变化，它们通常是从一种状态变化到另一种状态，并通常由其属性值的改变来体现。因此知网把概念的范畴分为 N 范畴、V 范畴和 A 范畴三类。</p>
<ul>
<li>N 范畴：包含实体、属性和单位，其中实体包含万物、时间、空间和部分，万物又进一步分为物质、精神和事情，通常是运动和变化的主体</li>
<li>V 范畴：包含各种事件，事件又可分为静态事件和行为动作</li>
<li>A 范畴：包含各种属性值，与 N 范畴的属性严格对应</li>
</ul>
<p>知网描述的是词语的语义。Def 中的概念类别代表了概念的主要属性，它们被组织在体现上下位关系的层级结构中，主要属性体现了概念的本质属性。概念的次要属性不存在上下位关系。对于主要属性和次要属性的处理遵循如下原则：</p>
<ul>
<li>上位的概念的属性可以由下位概念的属性继承；下位概念至少有一个属性是它的上位概念所不具备的</li>
<li>词典中每一个概念都必须有一个主要属性，就是这个概念的类别</li>
<li>在确定类范畴及其上位下位关系时，分类标准必须保持一致</li>
<li>当主要属性用作次要属性时，可以保留它的全部或部分属性，但要失去它在层级系统中上下位关系的地位，不能再推导它的上位或下位关系</li>
</ul>
<h2 id="Pustejovesky-的生成词库理论"><a href="#Pustejovesky-的生成词库理论" class="headerlink" title="Pustejovesky 的生成词库理论"></a>Pustejovesky 的生成词库理论</h2><p>美国布兰代斯大学教授 Pustejovesky 于 1991 年提出，1995 年出版专著《生成词库》，理论框架基本成形。</p>
<p>生成词库理论（Generative Lexicon Theory，GLT）首次把广义的生成方法引入到词义和其他领域的研究中，它是在研究了词的创造性用法的基础上建立的词义表示方法，关注词义的形式化和计算，试图从生成的角度解释词的不同用法以及词在上下文中的创新性用法。</p>
<p>GLT 的核心思想是，一个词项的意义在词库中是相对稳定的，到了句子层面，在上下文中，通过一些生成机制可以获得词项的延伸意义。其主要目标是研究各语言中的多义、意义模糊和意义变化等现象。</p>
<p>GLT 包括两大部分：词项在词库中的词汇语义表达（词库问题）；句法层面的语义生成机制（生成问题）。</p>
<h3 id="词库问题"><a href="#词库问题" class="headerlink" title="词库问题"></a>词库问题</h3><p>词库中一个词项的词汇语义表达包括四个层面：</p>
<ul>
<li>论元结构：论元的具体数目、类型，并说明它们在句法层面的实现方法</li>
<li>事件结构：事件类型包括状态、过程和转变；可能有子事件；要说明哪个事件是核心事件并说明事件组合规则</li>
<li>物性结构：词项所指对象，说明词项由什么构成、指向什么、怎样产生的，并说明词项用途和功能，包括<ul>
<li>构成特征（构成角色）：描写物体与其组成部分之间的关系<ul>
<li>包括材料、质量、部分</li>
<li>也描写物体在更大范围内构成或组成哪些物体</li>
</ul>
</li>
<li>形式特征（形式角色）：描写物体在更大的认知域内区别于其他物体的属性<ul>
<li>包括方位、大小、形状、颜色和维度</li>
</ul>
</li>
<li>功用特征（功用角色）：描写物体的用途和功能<ul>
<li>功用有两种：直接或间接功用</li>
<li>还描写人的社会功用</li>
</ul>
</li>
<li>施成特征（施成角色）：描写物体是怎样形成或产生的，如创造、因果关系等<ul>
<li>涉及物体的来源和产生的因素</li>
</ul>
</li>
</ul>
</li>
<li>词汇类结构：说明一个词项在一个类型系统中的位置，这决定了与其他词项的关联方式，即词汇继承关系</li>
</ul>
<p>Pustejovsky 等学者在物性结构中的功用角色的基础上，把词汇的类型分为自然类、人造类和合成类，并据此建构了整个语义类型体系。GLT 假设人类的认知能力反映在语言中，尤其反映在心理词典中，这个心理词典是复杂、动态而又连贯的知识系统，是结构化的语言学操作和生成意义的组合规则之间的接口。心理词典中的词汇按其所代表的意义内容分为自然类、人造类和合成类。</p>
<ul>
<li>自然类：<ul>
<li>是物性结构中的<strong>形式角色和构成角色</strong>相关的原子概念</li>
<li>从上位类中继承形式角色，是其他类的基础</li>
</ul>
</li>
<li>人造类：<ul>
<li>结合了物性结构中<strong>施成角色和功用角色</strong>信息的基础类型</li>
<li>增加了功能概念，从上位类中继承功用角色</li>
<li>与自然类的区别是有 “意图”</li>
</ul>
</li>
<li>合成类：<ul>
<li>又叫 “点对象”，由自然类和人造类组成，从两三个自然类或人造类继承角色</li>
<li>在描写中用词汇概念范例（lexical conceptual paradigms，LCP）标记：把一个词的不同词义合并到一个元词项，这个元词项就叫 LCP</li>
</ul>
</li>
</ul>
<p>三大语义类的区分以名词为出发点，动词和形容词根据其与名词语义类的对应关系也相应地分为三大类。GLT 关于词项的语义描述，最大的特色在于增加了物性结构，把名词词义与经验知识相结合，把名词与动词相联系，尤其是功用角色的引入，直接影响了其语义类型体系。特点和贡献如下：</p>
<ul>
<li><p>通过物性结构，把日常经验知识和词汇语义连接在一起。</p>
<blockquote>
<p>关于语言知识与非语言知识的问题：</p>
<ul>
<li>传统的语义学：两者有明显界限，后者不是语言研究对象</li>
<li>认知语言学：没有明显界限</li>
<li>框架语义学：语言的理解要引入非语言知识的背景</li>
</ul>
</blockquote>
</li>
<li><p>区分了自然类与人造类。GLT 的贡献在于把自然类和人造类的区分与动词联系起来，并加以形式化。</p>
</li>
<li><p>引入多重继承。</p>
</li>
</ul>
<h3 id="生成问题"><a href="#生成问题" class="headerlink" title="生成问题"></a>生成问题</h3><p>语义生成机制分为三类：类型强迫、选择约束和共同组合。近年来的变化是把类型强迫纳入了语法上的论元选择机制。根据论元选择的情况，有三种论元选择生成机制可以解释词项在组合中句法和语用的表现。语义生成机制也就是论元选择生成机制。</p>
<ul>
<li>纯粹类型选择（Sel）：函项要求的类型能被论元直接满足</li>
<li>类型调节（Acc）：函项要求的类型能从论元继承</li>
<li>类型强迫：函项要求的类型被强加到论元上。通过 “利用” 和 “引入” 两种方式实现<ul>
<li>利用（Exploit）：利用论元类型结构的一部分满足函项要求</li>
<li>引入（Intro）：引入函项要求的类型来包装论元</li>
</ul>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>论元类型\</th>
<th>要求的类型</th>
<th>Natual</th>
<th>Artifactual</th>
<th>Complex</th>
</tr>
</thead>
<tbody>
<tr>
<td>Natual</td>
<td>Sel/Acc</td>
<td>Intro</td>
<td>Intro</td>
</tr>
<tr>
<td>Artifactual</td>
<td>Exploit</td>
<td>Sel/Acc</td>
<td>Intro</td>
</tr>
<tr>
<td>Complex</td>
<td>Exploit</td>
<td>Exploit</td>
<td>Sel/Acc</td>
</tr>
</tbody>
</table>
</div>
<p>GLT 在语义生成机制方面的改进主要表现在，从类型选择的角度区分了纯粹类型选择和类型强迫，分别来处理类型匹配和不匹配的情况，尤其强调类型强迫这一机制的作用，从而可以解决某些多义或语义模糊问题。</p>
<p>类型强迫可以把论元转换成符合函项要求的类型，否则就会出现类型匹配错误。出现类型强迫时，词项的语义可能发生变化，主要有两种：</p>
<ul>
<li>保持域不变</li>
<li>域发生变化<ul>
<li>实体变成事件</li>
<li>事件变成时间间隔</li>
<li>实体变成命题</li>
</ul>
</li>
</ul>
<p>基于语料库的一个语义体系：<a href="http://www.cs.brandeis.edu/~arum/publications/lrec-bso.pdf" target="_blank" rel="noopener">lrec-bso.pdf</a></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章主要介绍了基于词汇主义的形式模型。</p>
<ul>
<li>Gross 的词汇语法<ul>
<li>1975 年，Gross 首次提出 “词汇语法” 的理论，1979 年进一步完善了理论，是一种基于词汇主义的形式化语言理论。词汇语法的理论基础是结构主义语言学。</li>
<li>操作理据：基于经验证实原则和客观主义的实证主义。</li>
<li>操作背景：后海里斯主义（Post-Harrisism），Harris 一贯重视语言的形式特征，其核心概念是 “分布”，即某个单位或特征在话语里出现的不同位置的总和，也就是出现其中一切环境的总和。主张采取形式方法，不回避口语材料和反例，以直觉为本，从结构入手进行研究，而把句子的内容（语义）放在次要位置。</li>
<li>操作意图：构建相互有机联系的描写、验证、分类、语料这四种机制</li>
<li>操作方法分词汇的处理、语法的处理、矩阵的设计和专家的干预。</li>
</ul>
</li>
<li><p>链语法</p>
<ul>
<li>D.Sleator 和 Temperley 于 1991 年提出，是一种立足于单词链接特性的语法。</li>
<li>单词的链接通过链描述，链有两种：链头和链座。<ul>
<li>句子通过连接子描述，连接子左边为链座，表示需要向左找对应的链头；</li>
<li>连接子右边为链头，表示要向右找对应的链座。</li>
</ul>
</li>
<li>链座或链头有 “逻辑或” 关系的，组成该逻辑的对象称为 “选言肢”。链语法由一组词以及这些词相应的连接子组成，连接子由一列逻辑选言肢（逻辑“或”）组成。</li>
</ul>
</li>
<li><p>词汇语义学</p>
<ul>
<li>语言中的词汇具有高度系统化的结构，这种结构决定了单词的意义和用法。这种结构包括单词和它的意义之间的关系以及个别单词的内部结构。对这种系统化的、与意义相关的结构的词汇研究叫 “词汇语义学”。</li>
<li>词位表示词典中一个单独的条目，是一个特定的正字法形式和音素形式与一些符号的意义表示形式的组合。词典是有限个词位的列表。词位和它的含义（意义）之间的关系包括：同形、多义、同义、上下位。</li>
</ul>
</li>
<li><p>知识本体</p>
<ul>
<li>1991 年，美国计算机专家 R. Niches 提出构建智能系统的两个部分：知识本体+问题求解方法</li>
<li>1990 年，作者提出 “双态原则”：基于语法信息和知识本体的静态标记标注的机器词典 + 基于产生式规则的动态标记求解规则 = 机器翻译系统</li>
<li>作者的知识本体系统：ONTOL-MT 初始概念有：事物、时间、空间、数量、行为状态和属性，初始概念下还有不同层次的下位概念。</li>
</ul>
</li>
<li><p>词网</p>
<ul>
<li>词网是 1985 年美国普林斯顿大学的 G. A. Miller, R. C. Beckwick, C. Fellbaum 等研制的英语的词汇关系数据库。</li>
<li>词网的三个基本假设：分离性假设、模式化假设、完全性假设</li>
<li>词网的基本单位是单词，包括动词、名词、形容词-副词数据库。<ul>
<li>名词数据库使用了 25 个初始概念，经过归纳整理形成了 11 个初始概念：实体、抽象、心理特征、自然现象、活动、事件、集体、位置、所属、外形和状态</li>
<li>形容词：修饰名词的都可看做形容词：形容词、名词、现在分词、过去分词、介词短语、小句等</li>
<li>副词</li>
<li>动词，14 个语义领域：运动、感知、接触、交际、竞争、变化、认知、消耗、创造、情绪、占有、身体保健和功能、社会行为、交互。</li>
</ul>
</li>
</ul>
</li>
<li><p>知网</p>
<ul>
<li>董振东和董强研制，是一个词典知识描述系统。在知网中，每一个词语的概念及其描述构成一个记录。</li>
<li>知网的设计者认为，世界上的一切事物都在一定的时间和空间内不停地运动和变化，它们通常是从一种状态变化到另一种状态，并通常由其属性值的改变来体现。因此知网把概念的范畴分为 N 范畴、V 范畴和 A 范畴三类。</li>
<li>知网描述的是词语的语义。</li>
</ul>
</li>
<li><p>Pustejovesky 的生成词库理论</p>
<ul>
<li>美国布兰代斯大学教授 Pustejovesky 于 1991 年提出，1995 年出版专著《生成词库》，理论框架基本成形。</li>
<li>GLT 的核心思想是，一个词项的意义在词库中是相对稳定的，到了句子层面，在上下文中，通过一些生成机制可以获得词项的延伸意义。其主要目标是研究各语言中的多义、意义模糊和意义变化等现象。</li>
<li>GLT 包括两大部分：词项在词库中的词汇语义表达（词库问题）；句法层面的语义生成机制（生成问题）。<ul>
<li>词库中一个词项的词汇语义表达包括四个层面：论元结构、事件结构、物性结构（构成角色、形式角色、功用角色、施成角色）、词汇类结构。GLT 假设人类的认知能力反映在语言中，尤其反映在心理词典中，这个心理词典是复杂、动态而又连贯的知识系统，是结构化的语言学操作和生成意义的组合规则之间的接口。心理词典中的词汇按其所代表的意义内容分为自然类、人造类和合成类。</li>
<li>根据论元选择的情况，有三种论元选择生成机制可以解释词项在组合中句法和语用的表现。语义生成机制也就是论元选择生成机制。包括：纯粹类型选择（Sel）、类型调节（Acc）、类型强迫（通过 “利用” 和 “引入” 两种方式实现）</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>本章都是以词汇为中心的形式模型，尤其是知识本体、词网（知网）和生成词库理论关于 “知识” 概念和语言哲学的探讨，引人深思。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/01/31/NLPFA/2019-01-31-Ch07-Formal-Model-Based-on-Lexicalism/">
    <time datetime="2019-01-31T03:32:00.000Z" class="entry-date">
        2019-01-31
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Model/">Formal Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Lexicalism/">Lexicalism</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-01-18-Ch06-Formal-Model-Based-on-Grid-Grammar" class="post-NLPFA/2019-01-18-Ch06-Formal-Model-Based-on-Grid-Grammar post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/01/18/NLPFA/2019-01-18-Ch06-Formal-Model-Based-on-Grid-Grammar/">自然语言计算机形式分析的理论与方法笔记(Ch06)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/01/18/NLPFA/2019-01-18-Ch06-Formal-Model-Based-on-Grid-Grammar/" data-id="cju9u9zaq000q5occ8ib9qenv" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第六章：基于格语法的形式模型"><a href="#第六章：基于格语法的形式模型" class="headerlink" title="第六章：基于格语法的形式模型"></a>第六章：基于格语法的形式模型</h1><h2 id="Fillmore-的格语法"><a href="#Fillmore-的格语法" class="headerlink" title="Fillmore 的格语法"></a>Fillmore 的格语法</h2><p>美国语言学家 C.Fillmore 提出的语法理论。发展的两个阶段：</p>
<ul>
<li>20 世纪 60 年代末到 70 年代初：只用格分析平面做工具，把句子的底层语义表达跟句子描述的情景的特点联系起来，不考虑深层语法关系平面。</li>
<li>20 世纪 70 年代中期以后：增加了深层语法关系平面来解释语义和句法现象。</li>
</ul>
<h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><p>Fillmore 认为，自然语言的句子中存在着一个体现其主题的深层结构，这个深层结构由一个作为中心成分的动词和若干个名词短语组成，每个名词短语都以一种特定的关系与中心动词发生联系，这些联系就是 ”格关系“。</p>
<p>这里的格不是传统语法中的格，而是深层次的格。传统的格与名词的形态变化联系在一起，格语法用格来表示深层结构中的句法语义关系。主语、宾语等都是表层结构的概念，在深层结构需要的是施事、受事、工具、处所等格的关系。</p>
<p>在格语法中，一个句子包括情态（M）和命题（P）两部分：S → M + P</p>
<ul>
<li>P 可以扩展为一个动词和一个或多个格的范畴：P → V + C1 + C2 + … + Cn，每一个格的范畴又可以表示为一个格标（K）加上一个名词短语（NP）：C → K + NP。</li>
<li>M 与传统意义的情态（主要表示可能、必然等）不同，主要是指动词的时、体、态以及肯定、否定、祈使、疑问、感叹、陈述等。</li>
</ul>
<p>格的概念包括一整套带有普遍性的、可以假定是内在的概念，相当于人类对在周围发生的事所能做出的某些类型的判断。</p>
<ul>
<li>施事格（A）：表示由动词确定的动作能察觉到的典型的动作发生者</li>
<li>工具格（I）：表示对于动词所确定的动作或状态而言，作为某种因素而牵涉到的、无生命的力量或客体</li>
<li>承受格（D）：表示由动词确定的动作或状态所影响的有生物</li>
<li>使成格（F）：表示由动词确定的动作或状态所形成的客体或有生物，或者是理解为动词意义的一部分的客体或有生物</li>
<li>方位格（L）：表示由动词确定的动作或状态的处所或空间方向</li>
<li>客体格（O）：表示由动词确定的事物或状态所影响的事物，它是由名词所表示的事物，其作用由动词本身的词义确定</li>
<li>受益格（B）：表示由动词所确定的动作为之服务的有生命的现象</li>
<li>源点格（S）：表示由动词所确定的动作所作用到的事物的来源或发生位置变化过程中的起始位置</li>
<li>终点格（G）：表示由动词所确定的动作所作用到的事物的终点或发生位置变化过程中的终端位置</li>
<li>伴随格（C）：表示由动词确定的、与施事共同完成动作的伴随者</li>
</ul>
<p>格语法着重研究了名词和动词的特征。open 的格框架可以简写为：<code>+[—O (I) (A)]</code>，表示这个动词必须用 O（客体格），I 和 A 则是时有时无的。</p>
<p>由深层结构中的深层格转化为表层结构中的主语的过程叫做 ”主语化“，在主语化时，如有 A，则 A 为主语；无 A 有 I，则 I 为主语；无 A I，则 O 为主语。</p>
<h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3><p>主要做了如下修改：把第一阶段表示格角色的结构叫做底层结构，在转换为表层结构前还必须经过深层主语和深层宾语等语法关系的分配。每个句子就有格角色和语法关系两个平面，两个平面把句子和句子所描述的事件联系起来，解释句子的语义和句法现象。</p>
<p>句子描述的是场景，场景中各参与者承担格角色，构成句子的底层结构。底层结构经过 ”透视域“ 的选择，使得一部分参与者进入透视域，成为句子的核心成分。每一个核心成分根据突出的等级体系确定其语法关系，其他参与者不一定能进入句子，即使它们出现在句子中，也只能成为外围成分。核心成分突出等级确定原则（从上到下递减）：</p>
<ul>
<li>主动成分级别高于非主动成分</li>
<li>原因成分级别高于非原因成分</li>
<li>作为人的（或有生命的）感受者的级别高于其他成分</li>
<li>蒙受改变的成分的级别高于未蒙受改变的成分</li>
<li>完全的或个性化的成分的级别高于一个成分的某一部分或无个性化的部分</li>
<li>实际形体的级别高于背景成分</li>
<li>有定成分的级别高于不定成分</li>
</ul>
<p>场景是语言之外的真实世界，如物体、事件、状态、行为、变化，以及人们对于真实世界的记忆、感觉、知觉等语言中的每一个词、短语、句子都是对场景的描述。当人们说出一个词、短语、句子或一段话语，都是在确定一个场景，并且突出或强调那个场景中的某一部分。</p>
<blockquote>
<p>书中对 ”写“ 的例子比较有趣。</p>
</blockquote>
<p>语义联系着场景，但场景并不等于语义，场景必须通过语言使用者的透视才能进入语言，才能与语义发生联系。我们说出每一个句子或一段话语都有一个特定的透视域，在一段话语的任何一个地方，我们都是从一个特殊的透视域去考虑一个场景。当整个场景都在考虑之中的时候，我们一般只注意场景的某一部分。</p>
<h2 id="Fillmore-的框架网络"><a href="#Fillmore-的框架网络" class="headerlink" title="Fillmore 的框架网络"></a>Fillmore 的框架网络</h2><p>动词的语义角色必须在动词的词典条目中列出，从潜在的概念结构是不能预测的。因此，需要列出每个动词的句法和语义组合的可能性，通过 ”框架“ 来描述。20 世纪末，Fillmore 提出了 ”框架语义学“，框架网络成了在格语法的基础上进一步发展起来的另一个自然语言处理的形式模型。框架网络根据框架语义学的理论，依靠语料库的支持，对每一个词位的每一个含义都要详尽地描述它的语义和句法的各种结合可能性，也就是它的配价。</p>
<p>框架语义学的中心思想是词的意义的描述必须与语义框架相联系。框架是信仰、实践、制度、想象等概念结构和模式的图解表征。框架网络的任务：</p>
<ul>
<li>描述给定词元所隶属的概念结构或者框架</li>
<li>从语料库中抽取包含某个词的句子，并从中挑选能够描述要分析的具有某种给定意义的词元的例子</li>
<li>通过把与框架相关的标记（框架元素：情景实例中的事件和参与者）指派到包含词元的句子中的短语上，使挑选的句子得到标注</li>
<li>准备最终的标注总结报告，简明显示每个词元在组合上的可能性，这些被称作 ”配价描述“</li>
</ul>
<p>语义框架中各个成分由词汇单元的意义联系起来。</p>
<p>框架网络区分了中心框架元素（core FEs）和非中心框架元素（non-core FEs），主要是语义的，关注某个概念对于框架的意义理解是否必要。在框架网络中，对于相关动词的基本的配价的描述只包括那些中心框架元素。</p>
<p>每一个框架是框架元素的集合，包括框架的参与者和道具，它们是题元角色。词汇单元的框架语义要描述在所给定的含义下，框架元素的结合方式和框架元素在框架中的分布情况。</p>
<p>每一个含义都要描述它的配价，配价不仅要表示出框架元素组合方式的集合信息，而且还要表示出在有关语料库中检验过的语法功能信息和词组类型信息。</p>
<p>框架网络数据库既可以作为词典，也可以作为叙词表：</p>
<ul>
<li>作为词典，单词条目信息包括：<ul>
<li>单词的定义</li>
<li>标注好的例句</li>
<li>框架元素表：说明框架元素在标注报告中的出现情况及它们表示的句法关系</li>
<li>配价模式：说明该单词可以具有的配价模式，并说明每一个配价模式中的框架元素相应的词组类型和句法功能</li>
<li>索引</li>
</ul>
</li>
<li>作为叙词表，每一个单词都与它们所参与的语义框架相链接，而框架反过来又与词表和其他相关的框架相链接</li>
</ul>
<p>框架网络中的每一个条目要列出该条目的所有论元，包括题元角色及它们的词组类型（如 NP，PP）和语法功能（如 Subj，Obj）。</p>
<p>框架网络包括若干个领域，每一个领域又包括若干个框架，每一个框架由若干题元角色来定义。</p>
<p>值得注意的是，一个短语的句法核心并不总是最重要的框架唤起者，依存短语的句法核心也不总是这些短语的意义的最重要的指示者。这些现象包括：</p>
<ul>
<li>支撑动词：一个动词的句法核心在语义方面作用很小，其主要框架引介者是与支撑动词有关的名词，如 have，do，make，take，give 等轻动词，使用频率高，与大量事件名词搭配，但对于名词唤起的场景几乎没有语义贡献</li>
<li>零形式框架元素：有时，核心框架元素既不是谓词的依存成分，也不能通过槽填充得以发现。因此，明显体会得出的概念成分在句子中却没有相应的形式，这种情况叫做零形式框架元素，有三种：<ul>
<li>结构零形式框架元素（CNI）：如祈使句中省略的主语、被动句中省略的 by 短语中的施事</li>
<li>有定零形式框架元素（DNI）：缺失的元素一定在篇章或者上下文中已经理解了的</li>
<li>无定零形式框架元素（INI）：缺省的元素的自然类型或语义类型都能够被理解，没有必要找回或建立一个特定的篇章所指</li>
</ul>
</li>
<li>透明名词：一个名词短语的句法核心成分代表了数量成分、类型或者容器，它的补足语则包含了这个名词短语的语义核心</li>
<li>框架元素融合：与两个框架元素相关的信息由一个成分来表达</li>
</ul>
<p>软件从标注语料库中自动生成两个包括：</p>
<ul>
<li>词元标注报告<ul>
<li>框架元素表</li>
<li>标注例句</li>
<li>配价模式</li>
</ul>
</li>
<li>词条报告<ul>
<li>框架元素句法实现表</li>
<li>词元的配价模式表</li>
</ul>
</li>
</ul>
<p>资源：<a href="https://framenet.icsi.berkeley.edu/fndrupal/" target="_blank" rel="noopener">Welcome to FrameNet! | fndrupal</a></p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章主要介绍了基于场景意义理解的格语法形式模型。</p>
<ul>
<li><p>格语法，美国语言学家 C.Fillmore 提出的语法理论。发展的两个阶段：</p>
<ul>
<li>20 世纪 60 年代末到 70 年代初：只用格分析平面做工具，把句子的底层语义表达跟句子描述的情景的特点联系起来，不考虑深层语法关系平面。<ul>
<li>Fillmore 认为，自然语言的句子中存在着一个体现其主题的深层结构，这个深层结构由一个作为中心成分的动词和若干个名词短语组成，每个名词短语都以一种特定的关系与中心动词发生联系，这些联系就是 ”格关系“</li>
<li>在格语法中，一个句子包括情态（M）和命题（P）两部分：S → M + P</li>
<li>格的概念包括一整套带有普遍性的、可以假定是内在的概念，相当于人类对在周围发生的事所能做出的某些类型的判断。</li>
</ul>
</li>
<li>20 世纪 70 年代中期以后：增加了深层语法关系平面来解释语义和句法现象。<ul>
<li>把第一阶段表示格角色的结构叫做底层结构，在转换为表层结构前还必须经过深层主语和深层宾语等语法关系的分配。每个句子就有格角色和语法关系两个平面，两个平面把句子和句子所描述的事件联系起来，解释句子的语义和句法现象。</li>
<li>句子描述的是场景，场景中各参与者承担格角色，构成句子的底层结构。底层结构经过 ”透视域“ 的选择，使得一部分参与者进入透视域，成为句子的核心成分。</li>
</ul>
</li>
</ul>
</li>
<li><p>框架网络</p>
<ul>
<li>动词的语义角色必须在动词的词典条目中列出，从潜在的概念结构是不能预测的。因此，需要列出每个动词的句法和语义组合的可能性，通过 ”框架“ 来描述。20 世纪末，Fillmore 提出了 ”框架语义学“，框架网络成了在格语法的基础上进一步发展起来的另一个自然语言处理的形式模型。框架网络根据框架语义学的理论，依靠语料库的支持，对每一个词位的每一个含义都要详尽地描述它的语义和句法的各种结合可能性，也就是它的配价。</li>
<li>框架语义学的中心思想是词的意义的描述必须与语义框架相联系。框架是信仰、实践、制度、想象等概念结构和模式的图解表征。</li>
<li>框架网络中的每一个条目要列出该条目的所有论元，包括题元角色及它们的词组类型（如 NP，PP）和语法功能（如 Subj，Obj）。</li>
<li>框架网络包括若干个领域，每一个领域又包括若干个框架，每一个框架由若干题元角色来定义。</li>
<li>一个短语的句法核心并不总是最重要的框架唤起者，依存短语的句法核心也不总是这些短语的意义的最重要的指示者。这些现象包括：<ul>
<li>支撑动词</li>
<li>零形式框架元素</li>
<li>透明名词</li>
<li>框架元素融合</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>这一章看完能对 NLP 处理中的语义角色分析有一些了解，可以发现彼时语料库已经逐渐开始在自然语言处理中崭露头角，相关的研究成果也有资源能够使用。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/01/18/NLPFA/2019-01-18-Ch06-Formal-Model-Based-on-Grid-Grammar/">
    <time datetime="2019-01-18T03:32:00.000Z" class="entry-date">
        2019-01-18
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Model/">Formal Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-01-15-Ch05-Formal-Model-Based-on-Dependence-and-Valence" class="post-NLPFA/2019-01-15-Ch05-Formal-Model-Based-on-Dependence-and-Valence post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/01/15/NLPFA/2019-01-15-Ch05-Formal-Model-Based-on-Dependence-and-Valence/">自然语言计算机形式分析的理论与方法笔记(Ch05)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/01/15/NLPFA/2019-01-15-Ch05-Formal-Model-Based-on-Dependence-and-Valence/" data-id="cju9u9zbk00225occg8kozs90" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第五章：基于依存和配价的形式模型"><a href="#第五章：基于依存和配价的形式模型" class="headerlink" title="第五章：基于依存和配价的形式模型"></a>第五章：基于依存和配价的形式模型</h1><h2 id="配价概念的起源"><a href="#配价概念的起源" class="headerlink" title="配价概念的起源"></a>配价概念的起源</h2><ul>
<li>12 世纪，语言学家 Petrus Helias 提出 “动词中心说”，指出了动词对于句子成分的要求，隐含了 “配价” 的概念。</li>
<li>1781 年，德国语言学家 Johann Werner Meiner 将谓语（动词）分为一价动词、二价动词和三价动词，很接近 “配价” 了。</li>
<li>1934 年，奥地利语言学家 Karl Bühler 提到 “空位”（某一词类的词在自己周围辟开一个或一个空位，这些空位必须由其他类型的词来填补）概念，揭示了 “配价” 的本质，也被认为是配价理论研究的先驱。</li>
<li>1948 年，苏联语言学家 Kacnel’son 首次提出 “配价” 这个术语：词在句中以一定方式出现以及与其他词组合的这种特性称为 “句法配价”。</li>
<li>40 年后，Kacnel’son 对配价的理解有了变化：配价可以被定义为一种包含在词的词汇意义中的句法潜力，这意味着这种可与其他词产生关系的能力是由实词决定的。用配价来揭示那些隐藏在词汇意义里面，需要用一定类型的词在句子中完善词义的东西。所以不是所有实词都有配价，只有那些本身让人感到表达不完整并且需要使其完整的词，才具有 “配价”。他特别强调配价的 “潜在性”。</li>
<li>1949 年，荷兰语言学家 A. W. de Groot 系统描述了建立在配价概念基础上的句法体系。他认为词类具有不同的句法配价，配价是被其他词所限定或限定其他词的可能性或不可能性，所有词类都有配价。这是一种 “泛配价” 的观点。</li>
</ul>
<h2 id="Tesniere-的依存语法"><a href="#Tesniere-的依存语法" class="headerlink" title="Tesnière 的依存语法"></a>Tesnière 的依存语法</h2><p>依存语法又称 “从属关系语法”，由法国语言学家 L. Tesnière 提出，他的著作让 “配价” 这个术语广为人知，被称为 “配价理论之父”。依存语法最基本的概念是 “关联” 和 “转位”。</p>
<h3 id="关联"><a href="#关联" class="headerlink" title="关联"></a>关联</h3><p>两个单独的形式通过句法联系起来表达，这种联系就叫 “关联”，类似化合物与组成的元素。</p>
<p>关联要服从于层次原则，即要建立起句子中词与词之间的依存关系，这种关系用 “图式” 表示。层次原则的一个必然推论是：所有的依存成分都依存于其支配者。</p>
<p>结构顺序和线性顺序不同，前者是二维的，后者是一维的，句法理论中一个重要问题是确定那些把一维线性顺序改变为二维结构顺序或相反的规则。</p>
<p>在表示句子结构顺序的图式中，直接处于动词结点之下的是名词词组和副词词组，前者形成 “行动元”，后者形成 “状态元”。</p>
<ul>
<li>状态元是含义不言自明的。状态元的数目可以是无限的。</li>
<li>行动元的含义则必须加以界说。行动元是某种名称或某种方式的事或物，可以通过简单的名称或消极的方式来参与过程。行动元的数目不得超过三：主语、宾语 1、宾语 2。</li>
</ul>
<p>“在大部分欧洲语言中占中心地位的动词结点代表了一出完整的小戏，有剧情，大多也有人物和场景”，剧情、人物、场景分别对应了动词、行动元和状态元。</p>
<p>行动元的数目决定了动词的配价的数目：</p>
<ul>
<li>没有行动元：零价动词</li>
<li>一个行动元：一价动词</li>
<li>两个行动元：二价动词</li>
<li>三个行动元：三价动词</li>
</ul>
<p>不必总是要求动词依照配价带全所有的行动元，或者说让动词达到饱和，有些价可以不用或空缺。</p>
<p>还有一种潜在关联，是语义上的关联而不是结构上的，在图式中用虚线表示。</p>
<h3 id="转位"><a href="#转位" class="headerlink" title="转位"></a>转位</h3><p>Tesnière 提出了四个基本词类：动词（I）、名词（O）、形容词（A）、副词（E），第一级是动词，第二级是名词和副词，第三级是形容词和副词，第四级是副词。</p>
<p>转位就是词类的转换，分为一度转位和二度转位。如果转位的<strong>被转位者</strong>是名词、形容词和副词，这种转位就是一度转位；如果<strong>被转位者</strong>是动词就是二度转位（如动词被转位为名词）。</p>
<p>在一度转位和二度转位内部还有简单转位和复杂转位。如果转位只是一个成分转位到另一个成分，就是简单转位；如果转位可连续从一个成分到另一个成分再到其他成分，就是复杂转位。</p>
<p>转位有六种类型：O&gt;A, O&gt;E, A&gt;O, A&gt;E, E&gt;O, E&gt;A，<strong>转位者</strong>或者是介词，或者是后缀，或者是加标记，也可以为空，分别记为 PREP, SUFF, INDICE 和 ∅。</p>
<h2 id="依存语法在自然语言处理中的应用"><a href="#依存语法在自然语言处理中的应用" class="headerlink" title="依存语法在自然语言处理中的应用"></a>依存语法在自然语言处理中的应用</h2><p>与短语结构语法相比，依存语法没有词组这个层次，每一个结点都与句子中的单词相对应，能直接处理句子中词与词之间的关系，而结点数目大大减少，便于直接标注词性。</p>
<p>短语结构树在确定结点依存关系后，可以转为依存树：</p>
<ul>
<li>从叶节点开始，首先把表示具体单词的结点归结到表示词类的结点上</li>
<li>自底向上把主词（处于支配地位的词）归结到父结点上</li>
<li>把全句的中心主词归结到根节点上</li>
</ul>
<p>学者对依存语法的研究：</p>
<ul>
<li><p>1960 年，美国语言学家 D. G. Hays 提出依存分析法，力图从形式上建立句中词与词之间的依存关系，提出了3 种规则。</p>
</li>
<li><p>1970 年，美国语言学家 J.Robinson 提出了依存语法的 4 条公理。</p>
</li>
<li><p>1987 年，K. Schubert 从语言信息处理角度出发，提出了用于语言信息处理的依存语法 12 条原则。</p>
</li>
<li><p>作者提出依存树应该满足的 5 个条件：</p>
<ul>
<li>单纯结点：所有结点都代表句中出现的词</li>
<li>单一父结点：除了根结点，所有结点都只有一个父结点</li>
<li>独根结点：只能有一个根结点</li>
<li>非交：树枝不能彼此相交</li>
<li>互斥：支配关系和前于关系互斥</li>
</ul>
<p>注：依存关系可以用树形图表示，叫做 “依存树”，树中结点之间的关系主要有支配关系和前于关系。</p>
</li>
</ul>
<p>用依存树进行自动生成时，必须把依存树转为句子，但从支配关系不能直接推出前于关系，所以还需按照自然语言中词序的特点，提出适当的生成规则。这方面各种自然语言的生成规则是不尽相同的。相比下，短语结构语法的成分结构树直接地反映了单词顺序。</p>
<p>1984 年，英国语言学家 Richard Hudson 提出了 “词语法”，是一种建立在依存语法基础上的形式语法。</p>
<ul>
<li>在词语法里，语法就是由一种语言所有的词构成的网络，语法和词汇没有本质区别，只不过前者处理的是一般性模式，后者描述的是有关单个词素的事实。在、</li>
<li>在词语法网络中，单词之间的关系有 isa 关系、part 关系和各种依存关系</li>
</ul>
<p>Stanford Parser 自动句法分析系统既可以进行短语结构分析，也可以进行依存分析。在依存分析时，使用谓词逻辑表达式表示支配词和从属词之间的依存关系：谓词（支配词，从属词）。</p>
<h2 id="配价语法"><a href="#配价语法" class="headerlink" title="配价语法"></a>配价语法</h2><p>德国的配价语法分别称为：莱比锡学派和曼海姆学派。</p>
<p>莱比锡学派的代表人物是 Gerhard Helbig，该学派主要贡献在配价理论方面：</p>
<ul>
<li>配价是指动词及受其支配成分之间的抽象关系；句法配价是指动词在其周围开辟一定数量的空位，并要求用必有或可选共演成分填补的能力。</li>
<li>Helbig 提出了 “补足语” 和 “说明语” 的概念，大致相当于 Tesnière 的行动元和状态元。</li>
<li>Helbig 还认为应该区分必有补足语和可有补足语。</li>
</ul>
<p>Zifonun 等引入了一种判定补足语、说明语和可有补足语的方法：</p>
<p><img src="http://qnimg.lovevivian.cn/book-2017-fengzhiwei-9.jpeg" alt=""></p>
<ul>
<li>R-Test：删除测试，<strong>不能删除</strong>判定为必有补足语</li>
<li>F-Test：替换测试，将测试对象用一个变元替换，被删掉的成分和带不定词的句子间形成推论关系；<strong>不能替换</strong>判定为说明语，可以替换进行 An-Test</li>
<li>An-Test：改写测试，把被测试的成分改写为 und das X，句子不成立（<strong>不能改写</strong>），则 X 为可有补足语</li>
</ul>
<p>曼海姆学派的核心人物是德语研究所的 Ulrich Engel，主要贡献在于研究并实践了是否可以用依存的原则来完整地描写一种语言中的主要现象。</p>
<ul>
<li>Engel 把价理解为动词在次范畴化时的一种支配能力。</li>
<li>他认为补足语和说明语的差别在于，补足语只是某个词类在次范畴化时所具有的，而一切的词类都可以有说明语。必有成分必然是补足语，可有成分可以是说明语或补足语，由支配者决定。</li>
</ul>
<p>配价可以从逻辑、句法和语义三个层次认识：</p>
<ul>
<li>配价逻辑：由词义的逻辑关系决定的配价。<ul>
<li>价是词义的一种特性，是词义开辟的一定数量的空位。</li>
<li>价体现的是一种逻辑语义关系。</li>
</ul>
</li>
<li>句法配价：逻辑配价在具体语言中的表现形式。</li>
<li>语义配价：充当补足语的词语在语义上是否与动词相容。</li>
</ul>
<p>Helbig 总结了构造配价词典条目的六个步骤：</p>
<ul>
<li>分析动词对应的谓词的逻辑语义结构，找出形成完整谓词结构的可词汇化论元的数量</li>
<li>标出动词具有的语义特征</li>
<li>为动词标示语义格，即为第一步得到的论元赋予明确的语义角色</li>
<li>对可词汇化的论元进行语义指称分析，并进行义位标识</li>
<li>处理从语义层到句法层的映射问题，要考虑两种情况：按照句子的功能成分（如主语、宾语等）；按照句子成分的形态表示（如名词是什么格，介词短语的类型等），是对行动元（补足语）的定性描述</li>
<li>给定词项行动元（补足语）的定量描述，即给出动词项的价数，应区分必有和可有补足语</li>
</ul>
<p>曼海姆学派的 Schumacher 对德语动词处理办法如下：</p>
<ul>
<li>给出动词的句子结构式</li>
<li>给出该结构式的句子格式</li>
<li>用改写法对该句子格式进行释义</li>
<li>对句子格式中出现的各种补足语进行语义描述</li>
<li>讨论动词构成被动态的能力</li>
<li>在对动词释义过程中，实例引证是其中一个中心部分</li>
<li>从构词法角度探讨动词派生的可能性</li>
<li>每一个词典项，只对应动词的一个义项，但需要本动词项中指出其他义项的所在</li>
</ul>
<p>Helbig 和 Schumacher 的共性是都含有句法和语义要素，配价结构没有体现表层线性顺序。不同是：Helbig 从语义到句法，语义是确定价的主要手段；Schumacher 从句子的表层（句法）得到句子结构式，语义的作用更多的是选择限制。</p>
<h2 id="配价语法在自然语言处理中的应用"><a href="#配价语法在自然语言处理中的应用" class="headerlink" title="配价语法在自然语言处理中的应用"></a>配价语法在自然语言处理中的应用</h2><p>捷克布拉格大学的计算语言学家 Petr Sgall，Jarmila Panevová 和 Eva Hajicová 提出了功能生成描述（Functional Generative Description，FGD），是一种多层级的自然语言处理的形式模型，配价占核心地位。</p>
<ul>
<li>如果将依存视为一种基本关系，那么词汇单元的句法特性就可以依据其可有或必有的依存成分进行描述，这种描述可包括词汇组合的限制，它们与句子表层结构的关系等</li>
<li>2003 年发布 Vellex 1.0（捷克语动词配价词表）</li>
</ul>
<p>Stanley Starosta 于 20 世纪 70 年代初创立了一种句法理论：Lexicase，可以有效处理配价分析中单词的线性顺序。</p>
<ul>
<li>语法就是词表。一个词与语法有关的属性都在其词汇矩阵里得到了说明。这些属性限定了词可以出现的环境（线性环境+依存环境）。</li>
<li>上下文属性作为词汇表示的组成部分使得短语结构语法规则不再有存在的必要，上下文属性申明哪些词可以作为依存者依附到给定的词身上形成句子，上下文属性既可属于语法，也可属于语义和词法。</li>
<li>配价在 Lexicase 中的定义：Lexicase 里的大多数词都用了一种或多种上下文属性来标识，这就限定了它们的价。价属性表明了该词和其他词的组合潜力，包括必需和可选的依存连接、线性前置等要求。</li>
</ul>
<p>德国计算语言学家 Hellwig 把配价和合一结合起来，提出了 “依存合一语法”（Dependency Unification Grammar，DUG），配价是其核心概念之一。</p>
<ul>
<li>句法差不多就是词的组合能力。词不仅是一种已有的结构模式的填充者，也是这种模式的真正源泉。</li>
<li>一个核心元素和一些可以完善核心元素的成分形成了一个标准的句法结构。</li>
<li>可将词分为：表示关系的和指代事物的两类。</li>
</ul>
<p>2007 年，作者与刘海涛提出了 “概率配价模型理论”（Probabilistic Valency Pattern Theory，PVP）的形式模型。</p>
<ul>
<li>配价是词的根本属性。广义的配价是指词具有的一种和其他词结合的能力，这种能力是一种潜在的能力，它在语句中的实现受句法、语义和语用等因素的限制；狭义的配价指动词等词类要求补足语的能力。</li>
<li>一个词的结合能力分为向心（输入）和离心（输出）两类。分别表示受别的词的支配能力（填补空位）和支配其他词（开辟空位）的能力。</li>
<li>在配价词表的词项里，除了对该词的价进行量的描述，还应该进行质的研究。具体来说，需要研究价的数量、种类、性质、实现的条件等。<ul>
<li>数量：综合考虑传统配价必需的名词性补足语和其他能够完善该词的成分</li>
<li>种类和性质：需要考虑语义格关系和语义特征</li>
<li>实现：句法、语义、语用的模式等都属考虑范围</li>
</ul>
</li>
<li>配价词表模式中，既可以只含有句法信息，也可以含有语义信息，甚至语用和场景信息，这些信息决定了词与词组合时的约束级别。几个层面的信息可以单用或联合起来用，是一种多层级的词类组合信息描写格式。</li>
<li>一个词类能（被）支配的依存关系是不均衡的，可以通过语料库获得更精确的定量描述。这样就很自然地引入了概率。一个词或词类的支配与被支配能力都不是呈均匀分布的，但被支配关系具有排他性：一个词不能同时有两个或两个以上的支配者。</li>
<li>价的实现过程由于生成和识别不同而略有不同<ul>
<li>生成：智能体根据预先的计划在词库中选取可表示生成核心内容的词（一般是动词）构成整个句子的基本框架，随后据此有针对性地从词库中选取其他词，选取的指标是词的结合能力。生成语句时，要将生成的二维或三维结构转为线性一维序列，需要一些约束条件完成，这些限制可以是词法、句法、语义和语用的。</li>
<li>识别：两种方法<ul>
<li>待输入的全部词语都进入工作区后，将它们具有的各种信息依据词库中对应的项一一赋予，然后开始组合，如果这些词语可以组合成一个有机整体则识别成功。</li>
<li>从收到第一个输入的词开始就从词库中提取信息，边读入边分析。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章的几个形式模型都是基于依存和配价的，也是目前句法分析的主要框架。</p>
<ul>
<li>Tesnière 的依存语法：<ul>
<li>依存语法又称 “从属关系语法”，由法国语言学家 L. Tesnière 提出，他的著作让 “配价” 这个术语广为人知，被称为 “配价理论之父”。</li>
<li>依存语法最基本的概念是 “关联” 和 “转位”。</li>
<li>关联：<ul>
<li>两个单独的形式通过句法联系起来表达，这种联系就叫 “关联”，类似化合物与组成的元素。</li>
<li>关联要服从于层次原则，即要建立起句子中词与词之间的依存关系，这种关系用 “图式” 表示。</li>
<li>在表示句子结构顺序的图式中，直接处于动词结点之下的是名词词组和副词词组，前者形成 “行动元”，后者形成 “状态元”。</li>
<li>行动元的数目决定了动词的配价的数目。</li>
</ul>
</li>
<li>转位：<ul>
<li>Tesnière 提出了四个基本词类：动词（I）、名词（O）、形容词（A）、副词（E），第一级是动词，第二级是名词和副词，第三级是形容词和副词，第四级是副词。</li>
<li>转位就是词类的转换，分为一度转位和二度转位。在一度转位和二度转位内部还有简单转位和复杂转位。</li>
<li>转位有六种类型：O&gt;A, O&gt;E, A&gt;O, A&gt;E, E&gt;O, E&gt;A，<strong>转位者</strong>或者是介词，或者是后缀，或者是加标记，也可以为空，分别记为 PREP, SUFF, INDICE 和 ∅。</li>
</ul>
</li>
<li>应用：<ul>
<li>与短语结构语法相比，依存语法没有词组这个层次，每一个结点都与句子中的单词相对应，能直接处理句子中词与词之间的关系，而结点数目大大减少，便于直接标注词性。短语结构树在确定结点依存关系后，可以转为依存树。</li>
<li>1960 年，美国语言学家 D. G. Hays 提出依存分析法，力图从形式上建立句中词与词之间的依存关系，提出了3 种规则。</li>
<li>1970 年，美国语言学家 J.Robinson 提出了依存语法的 4 条公理。</li>
<li>1987 年，K. Schubert 从语言信息处理角度出发，提出了用于语言信息处理的依存语法 12 条原则。</li>
<li>作者提出依存树应该满足的 5 个条件。</li>
<li>1984 年，英国语言学家 Richard Hudson 提出了 “词语法”，是一种建立在依存语法基础上的形式语法。</li>
<li>Stanford Parser 自动句法分析系统既可以进行短语结构分析，也可以进行依存分析。在依存分析时，使用谓词逻辑表达式表示支配词和从属词之间的依存关系：谓词（支配词，从属词）。</li>
</ul>
</li>
</ul>
</li>
<li>配价语法：<ul>
<li>德国的配价语法分别称为：莱比锡学派和曼海姆学派。</li>
<li>莱比锡学派的代表人物是 Gerhard Helbig，该学派主要贡献在配价理论方面：<ul>
<li>Helbig 提出了 “补足语” 和 “说明语” 的概念，大致相当于 Tesnière 的行动元和状态元。</li>
<li>Helbig 还认为应该区分必有补足语和可有补足语。</li>
<li>Zifonun 等引入了一种判定补足语、说明语和可有补足语的方法。</li>
</ul>
</li>
<li>曼海姆学派的核心人物是德语研究所的 Ulrich Engel，主要贡献在于研究并实践了是否可以用依存的原则来完整地描写一种语言中的主要现象。</li>
<li>配价可以从逻辑、句法和语义三个层次认识：<ul>
<li>配价逻辑：由词义的逻辑关系决定的配价。</li>
<li>句法配价：逻辑配价在具体语言中的表现形式。</li>
<li>语义配价：充当补足语的词语在语义上是否与动词相容。</li>
</ul>
</li>
<li>应用：<ul>
<li>捷克布拉格大学的计算语言学家 Petr Sgall，Jarmila Panevová 和 Eva Hajicová 提出了功能生成描述（Functional Generative Description，FGD），是一种多层级的自然语言处理的形式模型，配价占核心地位。</li>
<li>德国计算语言学家 Hellwig 把配价和合一结合起来，提出了 “依存合一语法”（Dependency Unification Grammar，DUG），配价是其核心概念之一。</li>
<li>2007 年，作者与刘海涛提出了 “概率配价模型理论”（Probabilistic Valency Pattern Theory，PVP）的形式模型。<ul>
<li>配价是词的根本属性。</li>
<li>一个词的结合能力分为向心（输入）和离心（输出）两类。分别表示受别的词的支配能力（填补空位）和支配其他词（开辟空位）的能力。</li>
<li>配价词表模式中，既可以只含有句法信息，也可以含有语义信息，甚至语用和场景信息，这些信息决定了词与词组合时的约束级别。</li>
<li>一个词类能（被）支配的依存关系是不均衡的，可以通过语料库获得更精确的定量描述。这样就很自然地引入了概率。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>这一章看完后能对 NLP 处理中的依存分析有比较详细地认识，就是不知道如今用的方法是不是来自于 PVP。难度和篇幅都不大。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/01/15/NLPFA/2019-01-15-Ch05-Formal-Model-Based-on-Dependence-and-Valence/">
    <time datetime="2019-01-15T03:32:00.000Z" class="entry-date">
        2019-01-15
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Dependence/">Dependence</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Model/">Formal Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Valence/">Valence</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-01-09-Ch04-Formal-Model-Based-on-Unity-Operation" class="post-NLPFA/2019-01-09-Ch04-Formal-Model-Based-on-Unity-Operation post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2019/01/09/NLPFA/2019-01-09-Ch04-Formal-Model-Based-on-Unity-Operation/">自然语言计算机形式分析的理论与方法笔记(Ch04)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/01/09/NLPFA/2019-01-09-Ch04-Formal-Model-Based-on-Unity-Operation/" data-id="cju9u9zc600395occ9yl6rihw" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第四章：基于合一运算的形式模型"><a href="#第四章：基于合一运算的形式模型" class="headerlink" title="第四章：基于合一运算的形式模型"></a>第四章：基于合一运算的形式模型</h1><p>短语结构语法有局限性，其中最大的问题就是生成能力过于强大，区分歧义结构能力很差，常常会产生大量的歧义句子或不合格句子。于是就出现了本章将要讨论的这些能够避免这种局限性的新的语法理论。</p>
<h2 id="中文信息-MMT-模型"><a href="#中文信息-MMT-模型" class="headerlink" title="中文信息 MMT 模型"></a>中文信息 MMT 模型</h2><p>作者在 20 世纪 80 年代初期提出了 “多叉多标记树形图分析法”，又叫做 “中文信息 MMT 模型”。</p>
<p>为了改进短语结构语法中的二叉树，MMT 模型使用多叉树。因为汉语的很多形式不便用二叉树描述：</p>
<ul>
<li>兼语式</li>
<li>状述宾式</li>
<li>双宾语</li>
</ul>
<p>MMT 模型提出了树形图的多值标记函数概念，采用多个标记来描述树形图中结点特性。主要针对短语结构语法单标记分析能力太弱生成能力太强的弱点，更适合汉语：</p>
<ul>
<li>汉语句子中的词组类型（或词类）与句法功能之间不存在简单的一一对应关系。因此描述句子时，除了组成成分的词类或词组类型特征外，还须给出句法功能特征，才不致产生歧义。</li>
<li>汉语句子中的词组类型（或词类）和句法功能都相同的成分，与句中其他成分的语义关系还可能不同，句法功能与语义关系也不是简单一一对应的。因此，还须给出语义关系特征。</li>
<li>航雨中单词固有的语法特征和语义特征，对于判断词组结构的性质往往有很大参考价值，因此额外标记语法和语义特征的多标记就便于判断词组性质。</li>
</ul>
<p>MMT 中采用若干特征和它们的值来描述汉语。由特征和它们的值构成的描述系统叫做 “特征/值” 系统。汉语的 “特征/值” 系统：</p>
<ul>
<li>词类特征和它的值：CAT<ul>
<li>名词、处所词、方位词、时间词、区别词、数词、量词、体词性代词、谓词性代词、动词、形容词、副词、介词、连词、助词、语气词、拟声词、感叹词、标点符号和公式等共 20 个值</li>
<li>每个特征可再取子值进一步分类</li>
</ul>
</li>
<li>词组类型特征和它的值：K<ul>
<li>动词词组、名词词组、形容词词组、数词词组，共 4 个</li>
</ul>
</li>
<li>单词的固有语义特征和它的值：SEM<ul>
<li>固有语义特征即单词的语义类别，表示孤立的单词的语义</li>
<li>物象、物资、现象、时空、测度、抽象、属性、行动，共 8 个</li>
</ul>
</li>
<li>单词的固有语法特征和它的值：GRM<ul>
<li>可以具有子值</li>
</ul>
</li>
<li>句法功能特征：SF<ul>
<li>句子自动分析中产生，不是单词或词组本身固有的。可以有子值</li>
<li>主语、谓语、宾语、定语、状语、补语、述语、中心语，共 8 个</li>
</ul>
</li>
<li>语义关系特征：SM<ul>
<li>句子自动分析中产生，孤立的单词谈不上语义关系。可以有子值</li>
<li>主体者、对象者、受益者、时刻、时段、时间起点、时间终点、空间点、空间段、空间起点、空间终点、初态、末态、原因、结果、目的、工具、方式、条件、内容、范围、比较、伴随、程度、附加、修饰等</li>
</ul>
</li>
<li>逻辑关系特征：LR<ul>
<li>单词与单词或词组与词组之间存在的逻辑关系就是 Chomsky 所说的 “题元关系”。一般没有子值</li>
<li>论元0（它是句子的深层主语）、论元1（它是句子的深层直接宾语）、论元2（它是句子的深层间接宾语），共 3 个值</li>
</ul>
</li>
</ul>
<p>MMT 的 “双态原则”（DSP）：</p>
<ul>
<li>静态特征：CAT、SEM、GRM</li>
<li>动态特征：K、SF、SM、LR<ul>
<li>K：一般根据静态特征可以推算</li>
<li>SF：通过上下文信息推算</li>
<li>SM 和 LR：多步演绎和推理</li>
</ul>
</li>
<li>实际操作时，先从词典中查询静态特征，然后在此基础上求解动态特征。</li>
</ul>
<p>汉语自动分析的步骤：</p>
<ul>
<li>切词</li>
<li>标注</li>
<li>将静态特征相容的单词结合成词组并求出词组类型特征</li>
<li>由静态特征和词组类型特征出发，计算句法功能特征，进一步计算语义关系特征和逻辑关系特征</li>
</ul>
<p>汉语的自动生成过程相反：根据分析得到的句法功能、语义关系和逻辑关系特征，并根据汉语单词的静态特征进行词序调整及必要的词性变化，产生出合格的句子。</p>
<h2 id="Kaplan-的词汇功能语法"><a href="#Kaplan-的词汇功能语法" class="headerlink" title="Kaplan 的词汇功能语法"></a>Kaplan 的词汇功能语法</h2><p>美国语言学家 R. M. Kaplan 和 J. Bresnan 于 1982 年在《词汇功能语法——一个语法表示的形式系统》一文中提出。不仅可以解释幼儿的语言习得的机制，而且还可以理解人类处理自然语言的行为。两个来源：</p>
<ul>
<li>Bresnan 感到转换生成语法把转换完全放在句法中不能对许多语言现象做出合适的解释，提出的将语法中的大部分放到词库内进行处理的模式</li>
<li>Kaplan 认为人脑对于语言处理并不完全按照转换生成语法的模式进行，用计算机模拟人脑处理语言</li>
</ul>
<blockquote>
<p>Chomsky 虽然认为语言知识存在于大脑和心理之中，但是认为语法规则并不直接体现在心理活动过程之中，语法的研究只能用数学、逻辑的方法模拟心理过程，而不能采用一般心理学实验的方法。</p>
<p>Bresnan 认为语法学应该是地道的心理学，每条语法规则都应该在心理活动中有所体现，都可以看作是行为的模型。</p>
<p>你怎么看呢？我自己好像更偏向于 Chomsky，借助《思考，快与慢》中的观点，大多数情况下，人都是偏好使用 “省事” 的方法，也就是说即便说话时我们的确可以 “思考” 一下语法规则，但大部分时候我们是 “脱口而出”。不过对他的后半段不是特别认同。</p>
</blockquote>
<h3 id="词汇功能语法的理论框架"><a href="#词汇功能语法的理论框架" class="headerlink" title="词汇功能语法的理论框架"></a>词汇功能语法的理论框架</h3><p><img src="http://qnimg.lovevivian.cn/book-2017-fengzhiwei-nlp-6.jpeg" alt=""></p>
<ul>
<li>概念结构：概念之间在逻辑上的关系</li>
<li>题旨结构：由不同的题旨角色构成<ul>
<li>任何概念所要表达的情景都要由一个或多个角色充当论元，而每一个概念转换成语言表达方式时规定要出现的论元就叫做题旨角色</li>
<li>题旨结构是语言经过筛选后保存下来的概念结构的骨架，是相对于抽象的句法结构时赋予每个论元不同语义角色的依据</li>
</ul>
</li>
<li>词汇映射理论：用于解释<strong>题旨结构与词汇项目的对应关系</strong>。经过词汇映射理论得到完整的词汇之后，便进入了句法的范围。</li>
</ul>
<p>词汇功能语法的一个基本思想：语法功能与表示语义的谓词论元结构一端的联系可以通过词汇规则改变，但是语法功能和表示句法结构一端的关系却不能通过任何规则加以改变。句法不存在任何的转换机制。这就是 “直接句法编码原则”：句法部分的句法功能不能被另一个语法功能所代替。因而词汇功能语法的成分结构是单一的。</p>
<ul>
<li>词汇功能语法由：词库、句法、语义解释三部分组成。<ul>
<li>表示语义的谓词论元结构首先从词库里通过词汇编码分配到一个语法功能。词条取得正确的语法功能编码后就可以构成词汇输入进入到句法部分。</li>
<li>句法部分有两个表达层次：成分结构和功能结构<ul>
<li>成分结构：表示句子成分先后次序，由一组短语结构规则映射而成的树形结构。代表句子的句法排列和语音表达。</li>
<li>功能结构：语言的内部结构，表述各语言成分之间的关系，代表句子的语义。</li>
</ul>
</li>
<li>功能结构具有普遍性，成分结构具有差异性。成分结构描述语言的表层结构，其中的单词承载了大多数语法信息，功能等式规定了语法信息的组合方法，经过有限步骤运算后，便得到最终组合结果——功能结构。为了确保功能结构的正确性，还需对合格性进行判别。Kaplan 和 Bresnan 证明了，由成分结构到功能结构的运算在数学上是 “有定解的”，而且所有的运算都只需要 “合一” 这种简单的运算方式（信息冲突时运算失败，不冲突时成功）。</li>
</ul>
</li>
</ul>
<h3 id="词汇功能语法的模式"><a href="#词汇功能语法的模式" class="headerlink" title="词汇功能语法的模式"></a>词汇功能语法的模式</h3><p><img src="http://qnimg.lovevivian.cn/book-2017-fengzhiwei-nlp-7.jpeg" alt=""></p>
<ul>
<li><strong>成分结构</strong>是句法描写的一个平面，由上下文无关的短语结构语法表示，形式是一般意义上的短语结构树。树形图上的结点带有句子中的词或短语预示的功能信息，这些信息由语法规则右部符号<strong>所带的</strong> “功能等式” 表示<ul>
<li>规则采用向上单箭头（表示直接支配成分）和向下单箭头（表示被支配成分）表示支配关系</li>
<li>带有功能等式 ↑=↓ 的结点称为 “功能中心语”，表示父结点和子结点共享了全部信息</li>
<li>（↑SUBJ）=↓ 和（↑OBJ）=↓ 功能等式表示相应结点所代表的功能信息在父结点中的具体功能<ul>
<li>（↑OBJ）=↓ 表示该结点继承了父结点的宾语（OBJ）特征</li>
<li>（↑OBJ）表示该结点的全部功能信息就是支配它的结点的宾语功能信息</li>
<li>↓ 表示该符号本身（即被直接支配的成分）</li>
</ul>
</li>
<li>双箭头 “⇑⇓” 表示成分结构中范畴之间非直接支配的依赖关系，特别是远距离的支配关系。必须成对使用：凡是有 “⇓” 关系的成分必须依附于有 “⇑” 关系的成分。</li>
<li>限制性功能等式：规定属性必须带某个指定的值，如规定某个 NP 是单数还是复数，可以根据语言分析的实际情况制定</li>
</ul>
</li>
<li><strong>词汇</strong>按词的不同意义立项，词汇项所含的信息有语法范畴和功能等式，功能等式形式与短语结构规则中的一致<ul>
<li>如：he: N，（↑PRED）= <code>&#39;he&#39;</code>，表示父结点具有功能 PRED（谓词），具体信息为 <code>he</code></li>
<li>注意动词的 PRED，采用 “谓词论元结构” 来表示谓词<strong>所带论元</strong>的多少及每个论元的逻辑语义，如：（↑PRED）=<code>&#39;read(SUBJ)(OBJ)&gt;&#39;</code> 表示 read 的论元分别是父结点的主语和宾语。</li>
<li>词汇中的信息以 “定义性功能等式” 和 “限制性功能等式”（带有符号 c）的形式来记录，详见 P.208 persuades 的例子</li>
</ul>
</li>
<li><strong>功能结构</strong>是词汇功能语法句法描写的另一个平面，是一个属性值矩阵，第一列表示属性，第二列表示相应属性所取的值<ul>
<li>左列首纵行：语法功能或语法特殊标记，叫做 “限定成分”</li>
<li>对应的是限定值，有三种形式：简单符号、语义形式和子功能结构（有自己的限定成分和限定值，具有递归性）</li>
<li>详见 P.209 “he reads the book” 的例子</li>
</ul>
</li>
<li>成分结构和功能结构之间存在对应关系，这是一个句子的成分结构能够转变为相应的功能结构的基本根据。将成分结构转为功能结构应该通过<strong>功能描述</strong>进行，三个步骤：<ul>
<li>将成分结构进行语法功能编码，并插入词项</li>
<li>把功能变项(f1,…fn)分配给 S 结点及其他各个附有 ↓ 的结点</li>
<li>用功能变项代替成分结构中的所有↑和↓，得到句子的功能描写</li>
</ul>
</li>
<li>由功能描写转为功能结构主要由定位和合并两个算子完成：定位算子首先定出功能描写中功能等式两边的名称在功能结构中所处的位置，然后由合并算子按照功能结构的格式将功能描写等式两边的内容横向排列。详见 P.214 例子。功能结构不仅可以描述完整的句子，也可以描述不成句子的短语。</li>
<li><strong>合格性条件</strong>制约：<ul>
<li>功能唯一性：任何功能结构中，每一个属性最多只能有一个值</li>
<li>功能完备性：<ul>
<li>任一功能结构为局部功能完备的，当且仅当该功能结构包含它的所有谓词所管辖的所有语法功能</li>
<li>任一功能结构为功能完备的，当且仅当该功能结构内的所有功能结构都是局部功能完备的</li>
<li>每个由谓词论元结构规定的语法功能都必须在功能结构中出现</li>
</ul>
</li>
<li>功能接应性：<ul>
<li>任一功能结构为局部功能接应的，当且仅当该功能结构所包含的可被管辖的语法功能都为一个局部谓词所管辖</li>
<li>任一功能结构为功能接应的，当且仅当该功能结构内所有功能结构都是局部功能接应的</li>
<li>每个出现的语法功能都是由谓词论元结构所管辖的</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="词汇映射理论"><a href="#词汇映射理论" class="headerlink" title="词汇映射理论"></a>词汇映射理论</h3><p>词汇映射理论（Lexical Mapping Theory，LMT）：题旨结构与词汇中的谓词-论元结构之间存在映射关系。两个特征给语法功能和题旨角色分类：</p>
<ul>
<li>+r（+restricted）表示语义是否受限，受限+不受限-</li>
<li>+o（objective）表示语法表现是否具备宾语性，具备+不具备-</li>
</ul>
<p>词汇映射理论还规定了题旨角色的层次：agent 施事者 &gt; beneficiary 受惠者/ recipient 接受者 &gt; experiencer 经验者 &gt; instrument 工具 &gt; patient 受事者 / theme 涉事者 &gt; locative 方位。/ 表示二者择一，&gt; 表示优先关系。</p>
<ul>
<li>理论依据是动词谓语与角色在语义组合上存在一种先后次序，即层级序列中较低位置上的角色与谓语动词的组合先于层级序列中处于较高位置上的角色与谓语动词的组合，因此，处于较低位置上的角色容易被词汇化。</li>
<li>另一个依据来自谓语动词一致关系标记的语法化序列：处于语法化序列较前位置上的角色也就是处于层级序列较高位置上的角色。即角色层级越高，越有资格跟动词发生一致关系。因此，题旨角色层级结构中最高层级上的角色叫做 “逻辑主语”，经常由施事者充当。</li>
</ul>
<p>根据词汇映射理论，只要知道了表达概念的题旨结构，就可以预测语法功能的表达方式和谓词论元结构，而知道了谓词论元结构就可以预测各种语言中可能出现的表层结构。</p>
<h2 id="Martin-Kay-的功能合一语法"><a href="#Martin-Kay-的功能合一语法" class="headerlink" title="Martin Kay 的功能合一语法"></a>Martin Kay 的功能合一语法</h2><p>美国计算语言学家 Martin Key 1985 年在 ”功能合一语法“ 这一新的语法理论中，提出了 ”复杂特征集“ 的概念，用功能描述表示。功能描述由一组描述元组成，每个描述元是一个成分集、一个模式或一个带值的属性，最主要的是 ”属性/值“ 偶对。属性是一个符号，值是一个符号或者是另一个功能描述。</p>
<p>把功能描述看作非结构性的特征集，就可用集合论的标准处理，但功能描述需要考虑相容性：如果两个功能描述都包含一个共同的属性但值不同，则不相容。</p>
<p>功能合一语法采用 ”合一“ 的运算方式对复杂特征集进行运算。寻找某种项对变量的置换，从而使表达式一致的过程叫做合一。功能合一语法使用合一运算合并<strong>相容的功能描述</strong>（合并原有特征，构造新的特征结构），不相容时合一失败（检查特征相容性和规则执行的前提条件），产生空集。</p>
<ul>
<li>合一运算可以对信息相加</li>
<li>合一运算是幂等的</li>
<li>空白项是合一运算的幺元</li>
<li>特征值相容时相同的特征可以合一</li>
</ul>
<p>功能合一语法最大的特点是在词条定义、句法规则、语义规则和句子的描述中，全面、系统地使用复杂特征集。</p>
<h2 id="Gazdar-的广义短语结构语法"><a href="#Gazdar-的广义短语结构语法" class="headerlink" title="Gazdar 的广义短语结构语法"></a>Gazdar 的广义短语结构语法</h2><p>改进的短语结构语法，初创于 20 世纪 70 年代，代表人物是英国语言学家 Gerald Gazdar，Ivan Sag，Ewan Klein 和 美国语言学家 Geoffrey Pullum。广义短语结构语法不仅主张句法结构只有一个平面，而且主张每一个句法结构都跟一个语义解释相对应。与传统短语结构语法的区别：</p>
<ul>
<li>短语结构语法：表示句子结构的树形图直接通过规则重写形成并得到解释，重写规则可以直接推导出树形结构</li>
<li>广义短语结构语法：规则系统要经过一系列合格性条件检验才能跟句子表层结构联系起来，每条规则只产生一个候选的局部树形结构，通过检验则接受。还参照 Montague 语法，接受了其 ”规则对规则假说“，认为语法中每条局法规则都必须有一条语义规则与之联系，语义规则的作用在于解释由局法规则得出的树形结构。</li>
</ul>
<p>广义短语结构语法在进行语义解释时，首先将树形结构中每一个父节点上的句法特征、句法范畴翻译成内涵逻辑表达式，再根据 Montague 语法对这些表达式进行模型论的解释。句法特征是进行特征制约的媒介，分为三类：</p>
<ul>
<li>主特征：17 个，可以从上而下扩散</li>
<li>次特征：3 个，可以从下而上渗透</li>
<li>一般特征：10 个</li>
</ul>
<p>广义短语结构采用复杂特征描述句法，所有句法特征都由 &lt;特征，特征值&gt; 构成。句法描写就是给树形图各结点标上特征值。特征可以通过两种途径进入树形图：</p>
<ul>
<li>通过句法中的直接支配规则，叫做 ”继承性特征“</li>
<li>不通过句法规则直接进入，叫做 ”获取性特征“</li>
</ul>
<p>广义短语结构语法通过短语结构规则描述句子的树形结构，同时又通过特征系统对树形结构进行制约，使其在整体上正确反映语言的现实。可以分为句法规则系统，特征制约系统和语义解释系统三部分。</p>
<h3 id="句法规则系统"><a href="#句法规则系统" class="headerlink" title="句法规则系统"></a>句法规则系统</h3><p>上下文无关的短语结构语法，三部分组成：编号、直接支配规则、语义解释。句法范畴主要以 ”X 阶标理论“ 为基础，词汇范畴（如 N, V, A, P）为 0 阶，短语范畴（如 NP, VP, AP, PP）为词汇范畴的 2 阶投射，中间层次为词汇范畴的 1 阶投射。</p>
<p>X 阶句法范畴分为两类：</p>
<ul>
<li>主范畴：N, V, A, P 及它们的 1 阶 或 2 阶投射范畴组成</li>
<li>小范畴：主范畴外的其他范畴，包括 DET, COMP, CONJ 等，没有投射</li>
</ul>
<p>根据是否有次范畴化特征（记为 SUBCAT，是该范畴在形成句子时所欠缺的所有范畴的集合）分为：</p>
<ul>
<li>词汇范畴：所有小范畴词汇和阶数为 1 的主范畴词汇在词库中有次范畴化特征</li>
<li>非词汇范畴：所有其他投射阶数为 1 或 2 的主范畴没有次范畴化特征</li>
</ul>
<p>如 N 的 0 阶，1 阶，2 阶投射分别为：N, N’, NP。</p>
<p>短语是 ”内部中心语“ 的投射，最高为 2 阶，中心语为 0 阶。广义短语结构规则有两种：</p>
<ul>
<li>直接支配规则 ID：表示直接支配关系，如 VP→V NP PP<ul>
<li>词汇直接支配规则：中心语有次范畴化特征</li>
<li>非词汇直接支配规则：中心语不具备范畴化特征</li>
</ul>
</li>
<li>线性前置规则 LP：表示前后位置关系，如 V &lt; NP &lt; PP</li>
</ul>
<h3 id="元规则"><a href="#元规则" class="headerlink" title="元规则"></a>元规则</h3><p>广义短语结构语法从规则生成规则的机制，主要用于描述某项父结点中子结点成分数量的增减或特征变化。由模式结构和目标结构两部分组成。如：</p>
<ul>
<li><p>模式结构：P0→W，Pm，P0 为父结点，W 为范畴的任何变项，Pm（m=0或1）为由 P0 直接支配的结点</p>
</li>
<li><p>目标结构：a0→a1,…,ak，a0 和 P0 属于同一主范畴，最多只能有一个 ai 是 W 的变项，最多只能有一个 ai 与 Pm 相对应的。</p>
</li>
</ul>
<p>以上形式为：如果 P0→W，Pm 是一条词汇直接支配规则，那么 a0→a1,…,ak 也是一条词汇直接支配规则。元规则的作用就是将所有符合模式结构的直接支配规则转变成由目标结构所表示的直接支配规则，扩大语法中直接支配规则的数量。</p>
<h3 id="特征制约系统"><a href="#特征制约系统" class="headerlink" title="特征制约系统"></a>特征制约系统</h3><p>规则和树形结构之间存在某种投射功能，决定哪些特征容许或不容许，保证了广义短语结构语法的正确性。有如下原则：</p>
<ul>
<li>特征共现原则（FCR）：22 条特征共现限制</li>
<li>默认特征规定（FSD）</li>
<li>主特征规约（HFC）：在直接支配规则 C0→…,Cn 中，如果 Cn 是 C0 的中心语，那么结点 Cn 的获取性主特征应该与 C0 的主特征保持一致。保证主特征在树形图中自上而下传递。</li>
<li>次特征原则（FFP）：只适用于次特征，规定了次特征在树形图中自下而上传递的原则。</li>
<li><p>控制一致原则（CAP）</p>
<ul>
<li>目标成分 C 在同一个局部树形结构有控制成分 C’，C 的控制特征的值和 C’ 范畴相同；如果 C 没有控制成分 C’，那么 C 的控制特征的值必须与 C 的父结点控制特征的值相同。</li>
<li>检验主谓语之间的一致关系</li>
</ul>
</li>
<li><p>线性前置陈述（LPS）：对经过直接支配规则（没有顺序）处理后的局部树形结构确定兄弟结点的顺序。</p>
</li>
</ul>
<p>进行句子剖析时，首先根据元规则展开直接支配规则，满足 CAP，HFC 和 FFP 的条件下，做出部分的剖析树，然后使用 FCR 和 FSD 检查范畴特征，最后使用 LPS 检验表层线性顺序，完成句子自动剖析。</p>
<h2 id="Shieber-的-PATR"><a href="#Shieber-的-PATR" class="headerlink" title="Shieber 的 PATR"></a>Shieber 的 PATR</h2><p>20 世纪 80 年代，斯坦福大学的 Stuart M. Shieber 研制了 PATR。一个 PATR 语法包括一套规则和一个词表；一个 PATR 的规则包括一个上下文无关的短语结构规则和一套特征约束，与短语结构规则的成分相联系的特征结构使用合一的方法进行运算。词表中的词项记录语言中的单词及其相关特征，用来替换短语结构规则中的终极符号。</p>
<ul>
<li><p>PATR 的基本数据结构是特征结构，用 “属性-值” 矩阵表示，属性值可以是简单值或复杂值，各组成部分可通过路径（特征结构中一个或多个属性名形成的序列）描述，不同路径可以共享相同的值。</p>
<p>特征结构的基本运算是 “合一”，合一表达式（规则）由左部和右部组成。左部是一个特征路径，右部是一个简单的值或另一个路径。路径的第一个成分是短语结构规则中的某一个符号。PATR 的规则也可以使用变量 X 抽象表示。</p>
</li>
<li><p>PATR 的词表也用复杂特征表示。</p>
</li>
</ul>
<p>PATR 的优点：</p>
<ul>
<li>简单：只使用合一运算</li>
<li>灵活：可以在 LFG，GPSG 中使用进行句法剖析</li>
<li>陈述：与顺序无关</li>
<li>模块：规则和词表模块化</li>
</ul>
<h2 id="Pollard-的中心驱动的短语结构语法"><a href="#Pollard-的中心驱动的短语结构语法" class="headerlink" title="Pollard 的中心驱动的短语结构语法"></a>Pollard 的中心驱动的短语结构语法</h2><p>1984 年，C. Pollard 和 I. A. Sag 提出了中心语驱动的短语结构语法（Head-Driven Phrase Structure Grammar, HPSG）。是在广义短语结构语法上提出的一种形式模型，强调中心语的作用，整个语法系统由中心语驱动，显示出强烈的词汇主义倾向。</p>
<ul>
<li><p>中心语驱动的短语结构语法重视词汇（特别是中心语）的作用，根据中心语的次范畴化特征就可以方便地把中心语的语法信息与句子中其他成分的语法信息联系起来，使得整个句子中的信息以中心语为核心而串通起来。他们把 SUBCAT 做成一个成分表来取值。</p>
</li>
<li><p>中心语驱动的短语结构语法采用复杂特征结构描述词语或短语信息。</p>
<ul>
<li>SYN 表示句法结构</li>
<li>ARG-ST 表示论元结构（也可以不用论元结构，使用指定语 SPR 和补足语 COMPS 来描述）</li>
<li>SEM 表示语义结构<ul>
<li>MODE：五个备选属性，陈述、疑问、祈使、指称、none</li>
<li>INDEX：对应所描述的情景或事件</li>
<li>RESTR：事件成立必须满足的条件</li>
</ul>
</li>
</ul>
</li>
<li><p>中心语驱动的短语结构语法采用 ”类特征结构“，语言中的语音、单词、短语、句子都属于不同的类，分别要求不同的属性特征相对应。语言中客观存在一个词类体系结构。词汇类体系结构中存在上层类和下层类：适合于上层类的每一个特征也适合于下层类；与上层类相关的每一个约束都影响到下层类，这种 ”约束承袭“ 是单值的（不允许例外发生），这与实际不符，因此 HPSG 提出 ”约束缺省承袭“ 的概念，上层类缺省的约束规则可以被下层类特殊的、例外的约束规则<strong>覆盖和否定</strong>。词汇类体系结构简化了 HPSG 的词汇操作：</p>
<ul>
<li>一定的类对应一定的属性特征，不需要描述所有特征</li>
<li>类及其属性特征是一个有规律的层级体系，只要知道了一个符号在整个层级体系中的位置，就可以自动获得它的大部分句法和语义特征，不需要逐一地去单独描述</li>
</ul>
</li>
<li><p>一个完整的词位描述包括两部分：词位的基础信息；上层类承袭来的信息（单亲承袭体系，即只能存一个父结点获取信息；多亲承袭体系）。</p>
</li>
<li><p>中心语驱动的短语结构语法的词汇规则是一个产生式的装置，其形式为：X→Y，词汇规则有两种：</p>
<ul>
<li>形态规则：说明如何从一个词位产生具有屈折变化的词项</li>
<li>派生规则：说明如何由一个词位产生另一个相关的词位</li>
</ul>
</li>
<li><p>中心语驱动的短语结构语法的词汇体系运作：</p>
<p><img src="http://qnimg.lovevivian.cn/book-2017-fengzhiwei-8.jpeg" alt=""></p>
</li>
</ul>
<ul>
<li>中心语驱动的短语结构语法中，所有语言单位通过特征结构来表示，特征结构要表示语音、句法和语义等信息；再把这些特征值结合起来，就可以确定语言单位的声音和意义之间在语法上的关系。语法也以特征结构的方式表示，这些结构也就是语言单位的合格性的限制条件。</li>
<li>中心语驱动的短语结构语法的早期模型中，一个句子的结构可以形式地用表示式来描述。最简单的表示式包括 <code>[PHON]</code> 和 <code>[SYSTEM]</code> 两大部分，分别表示语音和句法+语义部分（<code>(SYNtax), (SEMantics)</code>）。在句法语义部分：<ul>
<li>LOC 表示实位成分，用于记录在句子中实际位置的信息<ul>
<li>CAT：表示范畴，说明句子成分的形态和句法特征</li>
<li>CONTENT：表示含义，说明句子成分的语义特征</li>
</ul>
</li>
<li>NON-LOC 表示空位成分，用于记录有远距离关系的空位信息</li>
</ul>
</li>
<li>中心语驱动的短语结构语法词汇信息流通原则：<ul>
<li>中心语特征原则：在有中心语的短语中，父结点的 HEAD 的特征值同中心语子结点的 HEAD 的特征值在结构上共享。</li>
<li>奉献原则：在有中心语的短语中，父结点的 CONTENT 的特征值与中心语子结点的 CONTENT 的特征值等同。意味着父结点的 CONTENT 来自子结点。</li>
<li>饱和原则：在有中心语的短语中，父结点的 SUBCAT 之值等于其中心语子结点上的 SUBCAT 之值减去补足语子结点上的有关特征值。说明中心语子结点上的 SUBCAT 之值是饱和的。</li>
<li>暂存量词继承原则：说明中心补足语的信息暂存于逻辑量词中，在分析时可渗透到树形图的顶端。</li>
<li>修饰语原则：描述了非中心语子结点的修饰语的值与中心语子结点的语义特征关系。</li>
<li>空位特征原则：说明远距离空位成分的特征值与其父结点特征值的传递关系。</li>
</ul>
</li>
<li><p>1999 年，I. Sag 和 T. Wasow 在《句法理论的形式导论》中采用更直观和简洁的规则和原则：</p>
<ul>
<li>中心语—补足语原则：补足语是中心语在句法上要求的同现成分，用 COMPS 表示（一个属性特征列表），该原则要求所有的补足语实现为中心语的兄弟结点。</li>
<li>中心语—指定语原则：指定语就是动词中心语的主语，名词中心语的限定成分用 SPR（的属性）表示。中心语总是先与补足语捆绑，再作为一个 phrase 与指定语捆绑。</li>
<li>中心语—修饰语原则：修饰语是修饰中心语的成分，用 MOD（的属性）来表示。</li>
<li>中心语特征原则（句法）：父结点和中心子结点的中心语特征值合一。</li>
<li>值传递原则（句法）：除非特别说明，父结点的指定语 SPR 和补足语 COMPS 的特征值与中心子结点的特征值相同。</li>
<li>语义承袭原则：父结点 MODE 和 INDEX 特征值与中心子结点相同。</li>
<li>语义组合原则：父结点 RESTR 值等于所有子结点 RESTR 值之和。</li>
<li>论元实现原则</li>
<li>回指一致原则</li>
<li>约束原则</li>
</ul>
</li>
<li><p>中心语驱动的短语结构语法（HPSG）没有上下文无关短语结构语法（CFG）的 S 概念，初始符号是一个满足约束条件的 phrase，可参考 P270 图 4.77。HPSG 表示式包括 SYNSEM，DRTS 和 PHON 三个部分。本质上仍然是一种上下文无关的短语结构语法，但是包含了更加丰富的信息。</p>
<ul>
<li>SYNSEM 描述短语或单词的句法语义的限制特征，大致相当于 CFG 左边部分信息，但包含的信息更多。</li>
<li>DRTS 构成短语各组成成分的特征，每一个组成成分又可以是一个完全的 HPSG 表示式，大致相当于 CFG 右边部分信息，但是没有包含关于这些成分的前后顺序信息。</li>
<li>PHON 描述 DRTS 中各组成成分的前后顺序及这些成分的发音，相当于 CFG 右边部分的前后顺序信息。</li>
</ul>
</li>
<li><p>中心语驱动的短语结构语法自底向上分析算法大致过程：</p>
<ul>
<li>把输入句子中的单词的词汇表示式与词典中的词汇表示式合一；</li>
<li>直到没有单词可以合一时，把已经合一的表示式同短语的子结点的表示式同该语法中短语的表示式合一，直到句子饱和；</li>
<li>如果所有的表示式都合一结束，并且所有表示式中的 PHON 的值全部得到说明，则构造出句子 S 的整个结构；否则分析失败。</li>
</ul>
</li>
</ul>
<h2 id="Pereira-和-Warren-的定子句语法"><a href="#Pereira-和-Warren-的定子句语法" class="headerlink" title="Pereira 和 Warren 的定子句语法"></a>Pereira 和 Warren 的定子句语法</h2><p>1975 年 A. Colmerauer 证明了 CFG 可以翻译为 “定子句”，定子句是一阶谓词逻辑的一个受限子集。1980 年英国爱丁堡大学的 F.Pereira 和 D.Warren 正式提出定子句语法（DCG），并证明 DCG 是一种扩充的 CFG。但 CFG 只是一种语言的描述语法，DCG 则是一种语言的推理语法。另外，DCG 表达的语法规则经过简单转换可直接成为 Prolog 的可执行程序。定子句语法的基本思想是：语法的符号不仅仅是原子符号，而且可以使广义的逻辑项。</p>
<p>如 CFG 的规则：sentence → noun_phrase, verb_phrase 表示一个句子由名词短语和动词短语组成，在定子句语法中表示：如果存在一个名词短语和动词短语，那么存在一个句子的推理过程，用一阶谓词表示如下：</p>
<p><code>(∀U)(∀V)(∀W)[NP(U)∧VP(V)∧concatenate(U,V,W)→S(W)]</code></p>
<p>∀ 表示全称量词，∧ 表示逻辑合取，→ 表示蕴涵，具有三个变元的谓词 concatenate(U,V,W) 取真值当且仅当词串 W 是词串 U 和 V 经过毗连运算的结果。相当于：存在一个句法结构 S，使词串 W 成为满足语法规则集 P 的一个分析。</p>
<p>可以把定子句看成左部至多只含有一个谓词的规则，如上面的规则可以写为：</p>
<p><code>sentence(s0, s): - noun_phrase(s0, s1), verb_phrase(s1, s)</code></p>
<p>s s0 s1 为字符串的指针，解释为：如果 s0 到 s1 之间是一个名词短语，s1 到 s 是一个动词短语，那么 s0 到 s 就是一个句子。</p>
<p>Prolog 语言相关：</p>
<ul>
<li>数据对象叫 “项”，可以是一个常量、变量或者复合项<ul>
<li>常量：包括整数和原子</li>
<li>变量：首字母为英文大写字母的符号串</li>
<li>复合项：具有一定结构的数据对象<ul>
<li>谓词：由一个谓词名和一个或多个变元组成，每个变元也可以是一个复合项</li>
<li>表：用 <code>[]</code> 表示，表中元素之间用逗号分隔</li>
</ul>
</li>
</ul>
</li>
<li>子句：一个逻辑程序仅仅由一系列子句组成<ul>
<li>Horn 子句：也就是定子句，由一个头和一个体组成<ul>
<li>头只能包含零个或一个谓词</li>
<li>体可以包含零个或多个谓词</li>
</ul>
</li>
<li>三种形式：<ul>
<li>头和体非空：<code>P: - Q,R,S.</code>，如果 Q R S 均为真则 P 为真；“，” 表示合取，“—” 读作 “如果”。</li>
<li>体为空：<code>P.</code>，表示 P 无条件成立，是一条事实</li>
<li>头为空：<code>?: - P,Q.</code>，P 和 Q 是否为真？</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>定子句语法规则形式为：<code>&lt;非终极符号&gt; → &lt;规则体&gt;</code></p>
<ul>
<li>非终极符号是一个词或者短语标记，用 Prolog 的原子标记</li>
<li>规则体由一个或多个条目组成，用逗号隔开，每个条目或者是一个非终极符号，或者是一个终极符号的序列（用 Prolog 的表标记，其中每一个终极符号可以是任意 Prolog 的项）。</li>
</ul>
<p>定子句语法对 CFG 做了如下改进：</p>
<ul>
<li>在定子句语法的规则中，非终极符号可以是具有多个变元的复合项，可以携带有关上文、转换、结构等多方面信息，使得句法和语义信息像复杂特征一样可以作为变元在规则内部传递，从而实现了上下文相关的约束机制，增加了定子句语法描述自然语言复杂特征的能力。</li>
<li>定子句语法规则的右部可以引进不属于语法本身的测试条件和动作，进一步增加规则的约束能力。</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章介绍了几种对短语结构语法进行改进的形式模型，主要是基于合一运算（寻找某种项对变量的置换，从而使表达式一致的过程叫做合一）的形式模型。</p>
<ul>
<li><p>中文信息 MMT 模型：</p>
<ul>
<li>作者在 20 世纪 80 年代初期提出。</li>
<li>MMT 模型提出了树形图的多值标记函数概念，采用多个标记来描述树形图中结点特性。主要针对短语结构语法单标记分析能力太弱生成能力太强的弱点。</li>
<li>MMT 中采用若干特征和它们的值来描述汉语。由特征和它们的值构成的描述系统叫做 “特征/值” 系统。</li>
</ul>
</li>
<li><p>Kaplan 的词汇功能语法：</p>
<ul>
<li>美国语言学家 R. M. Kaplan 和 J. Bresnan 于 1982 年提出。不仅可以解释幼儿的语言习得的机制，而且还可以理解人类处理自然语言的行为。</li>
<li>词汇功能语法的一个基本思想：语法功能与表示语义的谓词论元结构一端的联系可以通过词汇规则改变，但是语法功能和表示句法结构一端的关系却不能通过任何规则加以改变。句法不存在任何的转换机制。</li>
<li>理论框架：<ul>
<li>概念结构</li>
<li>题旨结构</li>
<li>词汇映射理论：题旨结构与词汇中的谓词-论元结构之间存在映射关系</li>
<li>词汇功能语法由：词库、句法、语义解释三部分组成</li>
</ul>
</li>
<li>模式<ul>
<li>成分结构是句法描写的一个平面，由上下文无关的短语结构语法表示，形式是一般意义上的短语结构树</li>
<li>词汇按词的不同意义立项，词汇项所含的信息有语法范畴和功能等式，功能等式形式与短语结构规则中的一致</li>
<li>功能结构是词汇功能语法句法描写的另一个平面，是一个属性值矩阵，第一列表示属性，第二列表示相应属性所取的值</li>
<li>合格性条件制约包括功能唯一性、完备性和接应性</li>
</ul>
</li>
</ul>
</li>
<li><p>Martin Kay 的功能合一语法</p>
<ul>
<li>美国计算语言学家 Martin Key 1985 年在 ”功能合一语法“ 这一新的语法理论中，提出了 ”复杂特征集“ 的概念，用功能描述表示。</li>
<li>功能合一语法采用 ”合一“ 的运算方式对复杂特征集进行运算。</li>
<li>功能合一语法最大的特点是在词条定义、句法规则、语义规则和句子的描述中，全面、系统地使用复杂特征集。</li>
</ul>
</li>
<li>Gazdar 的广义短语结构语法<ul>
<li>初创于 20 世纪 70 年代，代表人物是英国语言学家 Gerald Gazdar，Ivan Sag，Ewan Klein 和 美国语言学家 Geoffrey Pullum。</li>
<li>改进的短语结构语法，不仅主张句法结构只有一个平面，而且主张每一个句法结构都跟一个语义解释相对应。在进行语义解释时，首先将树形结构中每一个父节点上的句法特征、句法范畴翻译成内涵逻辑表达式，再根据 Montague 语法对这些表达式进行模型论的解释。</li>
<li>广义短语结构采用复杂特征描述句法，所有句法特征都由 &lt;特征，特征值&gt; 构成。</li>
<li>广义短语结构语法通过短语结构规则描述句子的树形结构，同时又通过特征系统对树形结构进行制约，使其在整体上正确反映语言的现实。可以分为句法规则系统，特征制约系统和语义解释系统三部分。<ul>
<li>规则有两种：直接支配规则 ID 和线性前置规则 LP。</li>
<li>制约系统是：规则和树形结构之间存在某种投射功能，决定哪些特征容许或不容许，保证了广义短语结构语法的正确性。</li>
</ul>
</li>
</ul>
</li>
<li>Shieber 的 PATR<ul>
<li>20 世纪 80 年代，斯坦福大学的 Stuart M. Shieber 研制了 PATR。一个 PATR 语法包括一套规则和一个词表；<ul>
<li>一个 PATR 的规则包括一个上下文无关的短语结构规则和一套特征约束，与短语结构规则的成分相联系的特征结构使用合一的方法进行运算。</li>
<li>词表中的词项记录语言中的单词及其相关特征，用来替换短语结构规则中的终极符号。</li>
</ul>
</li>
<li>PATR 的基本数据结构是特征结构，用 “属性-值” 矩阵表示，特征结构的基本运算是 “合一”，PATR 的词表也用复杂特征表示。</li>
</ul>
</li>
<li>Pollard 的中心驱动的短语结构语法<ul>
<li>1984 年，C. Pollard 和 I. A. Sag 提出了中心语驱动的短语结构语法（Head-Driven Phrase Structure Grammar, HPSG）。是在广义短语结构语法上提出的一种形式模型，强调中心语的作用，整个语法系统由中心语驱动，显示出强烈的词汇主义倾向。</li>
<li>采用复杂特征结构描述词语或短语信息。<ul>
<li>SYN 表示句法结构</li>
<li>ARG-ST 表示论元结构</li>
<li>SEM 表示语义结构</li>
</ul>
</li>
<li>采用 ”类特征结构“，语言中的语音、单词、短语、句子都属于不同的类，分别要求不同的属性特征相对应。</li>
<li>一个完整的词位描述包括两部分：词位的基础信息；上层类承袭来的信息。</li>
<li>词汇规则是一个产生式的装置，其形式为：X→Y，词汇规则有两种：<ul>
<li>形态规则：说明如何从一个词位产生具有屈折变化的词项</li>
<li>派生规则：说明如何由一个词位产生另一个相关的词位</li>
</ul>
</li>
<li>所有语言单位通过特征结构来表示，特征结构要表示语音、句法和语义等信息；再把这些特征值结合起来，就可以确定语言单位的声音和意义之间在语法上的关系。</li>
<li>中心语驱动的短语结构语法（HPSG）没有上下文无关短语结构语法（CFG）的 S 概念，初始符号是一个满足约束条件的 phrase。</li>
</ul>
</li>
<li>Pereira 和 Warren 的定子句语法<ul>
<li>1975 年 A. Colmerauer 证明了 CFG 可以翻译为 “定子句”，定子句是一阶谓词逻辑的一个受限子集。1980 年英国爱丁堡大学的 F.Pereira 和 D.Warren 正式提出定子句语法（DCG），并证明 DCG 是一种扩充的 CFG。</li>
<li>DCG 是一种语言的推理语法。表达的语法规则经过简单转换可直接成为 Prolog 的可执行程序。</li>
<li>定子句语法的基本思想是：语法的符号不仅仅是原子符号，而且可以使广义的逻辑项。</li>
<li>定子句语法规则形式为：<code>&lt;非终极符号&gt; → &lt;规则体&gt;</code><ul>
<li>非终极符号是一个词或者短语标记，用 Prolog 的原子标记</li>
<li>规则体由一个或多个条目组成，用逗号隔开，每个条目或者是一个非终极符号，或者是一个终极符号的序列（用 Prolog 的表标记，其中每一个终极符号可以是任意 Prolog 的项）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>本章稍微有一点难，而且看起来特别复杂，尤其是词汇功能语法和中心驱动的短语结构语法，其中词汇功能语法有一小部分没有完全看懂。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/01/09/NLPFA/2019-01-09-Ch04-Formal-Model-Based-on-Unity-Operation/">
    <time datetime="2019-01-09T03:32:00.000Z" class="entry-date">
        2019-01-09
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Model/">Formal Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2018-12-22-Ch03-Formal-Model-Based-on-Phrase-Structure-Grammar" class="post-NLPFA/2018-12-22-Ch03-Formal-Model-Based-on-Phrase-Structure-Grammar post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/12/22/NLPFA/2018-12-22-Ch03-Formal-Model-Based-on-Phrase-Structure-Grammar/">自然语言计算机形式分析的理论与方法笔记(Ch03)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2018/12/22/NLPFA/2018-12-22-Ch03-Formal-Model-Based-on-Phrase-Structure-Grammar/" data-id="cju9u9zcd003n5occyqeahige" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第三章：基于短语结构语法的形式模型"><a href="#第三章：基于短语结构语法的形式模型" class="headerlink" title="第三章：基于短语结构语法的形式模型"></a>第三章：基于短语结构语法的形式模型</h1><h2 id="语法的-Chomsky-层级"><a href="#语法的-Chomsky-层级" class="headerlink" title="语法的 Chomsky 层级"></a>语法的 Chomsky 层级</h2><ul>
<li>W. Wundt 1990 年《大众心理学》提出把句子为成层次的思想；与此同时，传统的欧洲语法研究单词之间的关系，而不是单词所表示的成分之间的关系。</li>
<li>Leonard Bloomfield 1914 年在《语言研究导论》中将关于组成性的思想引入语言学。</li>
<li>1933 年《语言论》时，“直接成分分析法” 已经成为完善的方法。欧洲的句法学家仍然强调以词为基础的语法（依存语法）。</li>
<li>Z. Harris 使用 “可替换性” 试验检验单独的单位 “分布相似性”：如果一个简单形式可以替换一个复杂结构，这个复杂结构就可能是一个成分。</li>
<li>1956 年 N. Chomsky 定义短语结构语法，最早地形式化描述这种层次成分思想。</li>
</ul>
<p>短语结构语法是形式语言理论的主要内容，是自然语言处理中最重要的形式模型。</p>
<ul>
<li>Chomsky 的形式语法：G = (Vn, Vt, S, P)<ul>
<li>Vn 非终极符号</li>
<li>Vt 终极符号</li>
<li>S 是 Vn 的初始符号</li>
<li>P 是重写规则</li>
</ul>
</li>
<li>根据重写规则的形式，形式语法分四类：<ul>
<li>0 型语法：φ→ψ，并且要求 φ 不是空符号串</li>
<li>1 型语法（上下文有关语法）：φ1Aφ2→φ1ωφ2</li>
<li>2 型语法（上下文无关语法）：A→ω，A 重写为 ω，应用于自然语言的形式分析中： “短语结构语法”。</li>
<li>3 型语法（有限状态语法）：A→aQ 或 A→a，A 和 Q 是非终极符号，a 是终极符号。</li>
</ul>
</li>
</ul>
<p>每一个有限状态语法都是上下文无关的，每一个上下文无关语法都是上下文有关的，而每一个上下文有关语法都是 0 型的。</p>
<blockquote>
<p>一个人的语言知识是以某种方式体现在人脑这个有限的机体之中，语言知识就是一个由某种规则和原则构成的有限系统，但人却能讲出并理解他从未听到过的句子以及听到的不十分相似的句子，而且这种能力是无限的。——Chomsky</p>
<p>语言是有限手段的无限运用。——W. V. Humboldt</p>
</blockquote>
<h2 id="有限状态语法和它的局限性"><a href="#有限状态语法和它的局限性" class="headerlink" title="有限状态语法和它的局限性"></a>有限状态语法和它的局限性</h2><ul>
<li><p>有限状态 Markov 过程</p>
<ul>
<li>给出一个状态图，能从初始状态出发，按着状态图中的路，始终顺着箭头所指的方向来生成语言。</li>
<li>当达到图中某个状态时，可以沿着从这一状态引出的任何一条路前进，无论之前的生成过程是否走过。</li>
<li>从一个状态到另一个状态，可以允许若干种走法。</li>
<li>状态图中还可以允许任意有限长度的、任意有限数目的圈。</li>
</ul>
</li>
<li><p>由于真实的自然语言常有套叠、递归等结构，有限状态语法对这些结构处理能力不强，所以一般用来进行<strong>黏着语和屈折语的形态分析</strong>。</p>
</li>
<li><p>可以采用状态图清晰地描述屈折语单词的形态分析过程。</p>
<p><img src="http://qnimg.lovevivian.cn/book-2017-fengzhiwei-nlp-1.jpeg" alt=""></p>
<p>有以下缺陷：</p>
<ul>
<li>不能生成一些由非常简单的符号串构成的形式语言。如：<script type="math/tex">L_1={a^nb^n}, L_2={\alpha\alpha^*}, L_3={\alpha\alpha}</script></li>
<li>不能生成存在相依关系的句子（与具有镜像结构的 L2 相似）</li>
<li>不适合刻画句法结构</li>
<li>只能说明语言中各个符号的前后排列顺序，<strong>不能说明语言符号的层次</strong>，因此不能解释歧义现象。</li>
</ul>
</li>
</ul>
<h2 id="短语结构语法"><a href="#短语结构语法" class="headerlink" title="短语结构语法"></a>短语结构语法</h2><p>“上下文无关” 指语法中重写规则的形式，而不是指它所生成的语言。</p>
<h3 id="推导树与树形图"><a href="#推导树与树形图" class="headerlink" title="推导树与树形图"></a>推导树与树形图</h3><p>通过推导树描述上下文无关语法的推导过程：</p>
<ul>
<li>每个结点有一个标记，这个标记就是 Vn U Vt 中的符号</li>
<li>根节点是 S</li>
<li>如果结点 n 至少有一个异于其本身的后裔，并由标记 A，那么 A 必定是非终极符号集 V 中的符号</li>
<li>如果结点 n1,…nk 是结点 n 的直接后裔，从左到右分别标记为：A1,…Ak，那么 A→A1A2…Ak 必定是 P 中的重写规则</li>
</ul>
<p>树形图由结点和连接结点的枝组成，标记表示结点上的有关信息。各个结点间有两种关系要注意：<strong>支配关系和前于关系</strong>。</p>
<ul>
<li>从结点 X 到 Y 有一系列枝连接，X 支配 Y，Y 叫 X 的后裔</li>
<li>X 与 Y 之间没有另一个相异的结点，叫直接支配</li>
<li>不被任何其他结点支配的结点叫根</li>
<li>被其他结点支配而不支配任何其他结点的结点叫叶</li>
<li>两个结点<strong>没有支配关系</strong>时，才能从左到右排序，这两个结点之间就存在前于关系，左边结点前于右边结点</li>
<li>支配关系和前于关系互斥</li>
</ul>
<p>树形图可为语言自动分析提供的语言信息：</p>
<ul>
<li>句子中的词序</li>
<li>句子的层次</li>
<li>词类信息、词组类型信息、句法功能信息、词与词或词组与词组之间的语义关系信息和逻辑关系信息</li>
</ul>
<p>Chomsky 证明：任何由上下文无关语法生成的语言，均可由重写规则为 A→BC 或 A→a 的语法生成，其中 ABC 是非终极符号，a 是终极符号。具有这样重写规则的上下文无关语法的推导树均可简化为二元形式。这样就可以用二分法来分析自然语言，用二叉树来表示自然语言的句子结构。 A→BC 或 A→a 便叫做 <strong>Chomsky 范式</strong>。</p>
<p>上下文无关语法采用二分的层次分析法来揭示句子内部的句法结构规律。</p>
<h3 id="上下文无关语法与有限状态语法的关系"><a href="#上下文无关语法与有限状态语法的关系" class="headerlink" title="上下文无关语法与有限状态语法的关系"></a>上下文无关语法与有限状态语法的关系</h3><ul>
<li>每一个有限状态语法生成的语言都可以由上下文无关语法来生成</li>
<li>在上下文无关语法中，存在一个非终极符号 A 具有性质：A=&gt;φAψ，φψ 是非空字符串，这个语法叫 “自嵌入语法”。如果 G 是非自嵌入的上下文无关语法，那么由 G 生成的语言 L(G) 就是有限状态语言。如果 L(G) 是上下文无关语言，当且仅当 G 是具有自嵌入性质的上下文无关语法。所以 L1 L2 不能由有限状态语法生成。</li>
</ul>
<h3 id="上下文无关语法和上下文有关语法的关系"><a href="#上下文无关语法和上下文有关语法的关系" class="headerlink" title="上下文无关语法和上下文有关语法的关系"></a>上下文无关语法和上下文有关语法的关系</h3><p>上下文有关语法：重写规则 P 的形式：φ→ψ，φψ 是符号串，且 ψ 的长度不小于 φ 的长度</p>
<ul>
<li>每一个上下文无关语法都包含在上下文有关语法之中</li>
<li>存在不是上下文无关语言的上下文有关语言，如 L3</li>
</ul>
<h3 id="0-型语法"><a href="#0-型语法" class="headerlink" title="0 型语法"></a>0 型语法</h3><p>φ→ψ，φ≠ψ</p>
<p>Chomsky 证明：每一个 0 型语言都是符号串的 “递归可枚举集”；并且任何一个上下文有关语言同时又是 0 型语言，而且存在不是上下文有关语言的 0 型语言。</p>
<p>0 型语言几乎没有限制，用于描述自然语言比较困难，生成能力太强，会有很多不合格句子；有限状态语法生成能力太弱；上下文有关语法比上下文无关语法生成能力强，但由于上下文无关语法可以采用 Chomsky 范式来实现层次分析，所以在自然语言计算机处理中，人们一般乐于采用上下文无关语法。</p>
<p>四种类型语法分别与图灵机、线性有界自动机、后进先出自动机和有限自动机联系：若一语言能被某种自动机识别，就能用对应语法生成，反之亦然。</p>
<h2 id="递归转移网络和扩充转移网络"><a href="#递归转移网络和扩充转移网络" class="headerlink" title="递归转移网络和扩充转移网络"></a>递归转移网络和扩充转移网络</h2><ul>
<li><p>基于有限状态语法</p>
</li>
<li><p>1970 年，美国 W. Woods 在《自然语言分析的转移网络语法》中提出扩充转移网络（Augmented Transition Network，ATN）</p>
</li>
<li>有限状态转移图（Finite State Transition Diagram，FSTD）：用状态图形象地表示一个句子的分析过程。由于有限状态语法的局限性，难以识别复杂句子，需要扩充：加入一个递归机制，使其具备处理上下文无关语言的能力。</li>
<li>递归转移网络（Recursive Transition Networks，RTN）：经过扩充的 FSTD。<ul>
<li>弧的标记不仅仅是终极符号或词类符号，还可以是表示词组类型的非终极符号（如 NP，VP 等），每种类型的词组都可以单独用一个 FSTD 表示。</li>
<li>如果同时存在一条以上路径，RTN 无法分析，因此需要 “并行处理” 或先试探一条路，失败时可以 “回溯” 到另一条路。</li>
</ul>
</li>
<li>扩充转移网络（Augmented Transition Network，ATN）：对 RTN 进行改进扩充<ul>
<li>增加一组寄存器存储信息（因为回溯需要先前背景信息作为依据）</li>
<li>弧的标记除了终极符号和非终极符号，还可以附加一些条件判断</li>
<li>弧上附加一些动作，经过该弧要执行规定操作，通常是重新安排分析句子所生成的数据结构</li>
<li>ATN 改进<ul>
<li>识别功能可以提高到图灵机</li>
<li>过分依赖句法分析，处理有明显语义但不完全符合语法的话语时存在局限</li>
<li>过程性的，修改可能引发很大副作用</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="自顶向下分析和自底向上分析"><a href="#自顶向下分析和自底向上分析" class="headerlink" title="自顶向下分析和自底向上分析"></a>自顶向下分析和自底向上分析</h2><ul>
<li>基于上下文无关语法</li>
<li>自顶向下：根据重写规则，从初始符号开始，自顶向下进行搜索，构造推导树，直到句子结尾为止。</li>
<li>自底向上：从输入句子的句首开始顺次取词向前 “移进” 并根据语法的重写规则逐级向上 “归约”，直到构造出表示句子结构的整个推导树为止。<ul>
<li>移进：把一个尚未处理过的符号移入栈顶，并等待更多的信息到来之后再做决定；</li>
<li>归约：把栈顶部分的一些符号，由语法的某个重写规则的左边的符号来替代。</li>
<li>接受：输入的符号串处理完毕且栈内仅剩下初始符号 S</li>
<li>拒绝：无法移进，无法归约，且栈并非只有 S 或输入符号串还有符号未处理完毕</li>
</ul>
</li>
<li>移进-归约算法本质上是一种自左向右的 LR 算法，难以处理自然语言中的歧义问题。</li>
</ul>
<h2 id="通用句法处理器和线图分析法"><a href="#通用句法处理器和线图分析法" class="headerlink" title="通用句法处理器和线图分析法"></a>通用句法处理器和线图分析法</h2><p>1973 年，R. M. Kaplan 提出了通用句法处理器（General Syntacic Processor，GSP）</p>
<ul>
<li><p>元系统，不仅是一种处理方法，也是一个能形式地描写各种方法的系统</p>
</li>
<li><p>基本数据结构是 “线图”（把树形图加以修改而形成的一种图）</p>
</li>
<li><p>修改树形图以便直观地表示前于关系，修改后的图叫 “线图”</p>
<ul>
<li>可以表示不相连的子树</li>
<li>可以有多个解释的词</li>
</ul>
</li>
<li><p>线图的控制机构</p>
<ul>
<li>寄存器</li>
<li>层次栈：有递归功能先把线图及一些语法信息存在栈中，以供递归调用</li>
<li>非确定表：选择语法的指示表，方便回溯处理</li>
<li>过程栈：暂停处理表，有暂停和重新开始功能</li>
</ul>
</li>
</ul>
<p>1968 年，J. Earley 发表的博士论文《一种高效的上下文无关分析算法》提出了 Earley 算法和点规则的概念，为线图分析法奠定理论基础。1980 年，Martin Key 在《句法处理中的算法图和数据结构》中，提出了 “线图分析法”。</p>
<ul>
<li><p>线图边上的项</p>
<ul>
<li>有空所（空所是需要证实的部分）：活性边</li>
<li>没有空所：非活性边</li>
</ul>
</li>
<li><p>线图分析过程</p>
<ul>
<li>自顶向下</li>
<li>自底向上</li>
</ul>
</li>
<li><p>Martin Key 提出了 “点规则” 更加直观地表示 “活性边” 和 “非活性边”，修改如下：</p>
<ul>
<li>容许从某个结点出发，中间<strong>不经过</strong>其他结点又返回到这个结点的圈出现</li>
<li>线图的边上的标记，不仅可以是简单的范畴，还可以是语法规则。在这样规则的右边符号串中可以加圆点，叫 “点规则”。点规则中圆点后面的部分相当于 “项” 的空所。<ul>
<li>圆点后面不空，说明项中有空所，这样的项所在的边为活性边。</li>
<li>圆点后面为空，项中没有空所，这样的边必定是非活性边。</li>
</ul>
</li>
</ul>
</li>
<li><p>经过修改的线图比原来的线图功能更强，叫 “活性线图”。活性边和非活性边分别表示尚未证实的假设和已经证实的假设。活性线图的定义：</p>
<ul>
<li><p>由具有如下属性的边构成的图</p>
<ul>
<li>起点：<code>&lt;START&gt;</code>，整数</li>
<li>终点：<code>&lt;FINISH&gt;</code>，整数</li>
<li>标记：<code>&lt;LABEL&gt;</code>，范畴</li>
<li>已证实部分：<code>&lt;FOUND&gt;</code>，范畴系列</li>
<li>尚待证实的：<code>&lt;TOFIND&gt;</code>，范畴系列</li>
</ul>
</li>
<li><p>一个边可用五元组记录：<code>(&lt;START&gt;, &lt;FINISH&gt;, &lt;LABEL&gt;→&lt;FOUND&gt;.&lt;TOFIND&gt;)</code></p>
</li>
<li><p>力求找到能满足活性边条件的非活性边，以便推动分析进程。线图分析的基本规则：</p>
<ul>
<li><p><strong>如果一条活性边遇到了一条非活性边，而且这条非活性边的标记上的范畴满足活性边的要求，那么可以在线图上加一条新的边，这条边横跨在活性边和非活性边上。</strong></p>
</li>
<li><p>如果在线图中含有活性边 <code>(i,j,A-&gt;W1.BW2)</code> 和非活性边 <code>(j,k,B-&gt;W3.)</code>，其中，A 和 B 是范畴，W1，W2，W3（可能为空）是范畴序列或词，那么在线图中加一条新的边 <code>(i,k,A-&gt;W1B.W2)</code>。</p>
<p><img src="http://qnimg.lovevivian.cn/book-2017-fengzhiwei-nlp-2.jpeg" alt=""></p>
</li>
</ul>
</li>
<li><p>线图的另一个重要问题是启动问题：至少要有一条活性边和一条非活性边（详见书中例子 P.124）。</p>
<ul>
<li>查字典的方法，把单词在词典中的有关范畴记录到线图边上构造出非活性边</li>
<li>根据规则造出活性边</li>
<li>根据活性边和非活性边造出新的边</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Earley-算法"><a href="#Earley-算法" class="headerlink" title="Earley 算法"></a>Earley 算法</h2><p>美国学者 J. Earley 于 1968 年在博士论文中提出的算法，在左角分析法的基础上把自顶向下和自底向上结合起来，分析过程中交替使用。规则表示方法体现了<strong>点规则</strong>的基本精神，实质与线图分析法规则一致。表示方法是把点规则写成：<code>“A→α·β,[i,j]”</code> 的形式，该算法的核心是线图。对于句子中的单词，线图包含一个状态来表示在此之前已经生成的部分；在句子终点，线图提供出该句子所有可能的分析结果。</p>
<p>线图的状态包含三种信息：</p>
<ul>
<li>关于与语法的一个规则相对应的的子树的信息；</li>
<li>关于完成这个子树已经通过的进程的信息；</li>
<li>关于这个子树相对于输入的位置的信息。</li>
</ul>
<p>Earley 算法中的三种基本操作（详见书中例子）：</p>
<ul>
<li>Predictor（预示）：生成新的状态，预示下一步可以做什么；</li>
<li>Scanner（扫描）：判断将要分析的单词的词类是否与这个词类的范畴相匹配，如果匹配，就把点向右移动一个位置，并把新的状态加入到线图中；</li>
<li>Completer（完成）：当状态中的点的右边是非终极符号，而在输入句子中，这个非终极符号所跨越的输入符号串已经分析结束时，就把该状态中的点的位置向右移动到这个非终极符号的右边，并把新的状态加入到线图中。</li>
</ul>
<p>Earley 算法的分析过程没有回溯（通过遍历 predictor），改进了自顶向下分析的效果。</p>
<h2 id="左角分析法"><a href="#左角分析法" class="headerlink" title="左角分析法"></a>左角分析法</h2><p>左角分析法（left-corner parsing method）是一种自顶向下分析法和自底向上分析法结合起来的分析法。所谓 “左角” 是指表示句子句法结构的树形图的任何子树中左下角的那个符号。</p>
<ul>
<li>自顶向下分析过程是先上后下，从左往右</li>
<li>自底向上分析过程是从左往右，先下后上</li>
<li>左角分析法过程是先自下而上，然后再自顶向下</li>
</ul>
<p>左角分析法使用了回溯，M. Marcus 1980 年提出用人工方法对规约的条件加以控制，从而避免了回溯，这就是 “Marcus 确定性分析算法”。分为两部分：</p>
<ul>
<li>模式部分</li>
<li>行为部分</li>
</ul>
<h2 id="CYK-算法"><a href="#CYK-算法" class="headerlink" title="CYK 算法"></a>CYK 算法</h2><p>Cocke-Younger-Kasami 算法，一种并行的、以 Chomsky 范式为描述对象的句法分析算法。</p>
<p>CYK 步骤：</p>
<ul>
<li><p>第一步：从 i=1 开始，对长度为 n 的输入句子中的每一个单词 Wi，显然都有重写规则：A→Wi，因此，顺次把 Wi 相应的非终极符号 A 记入 bi1（第 i 列 第 1 行）中。</p>
</li>
<li><p>第二步：对于 1≤h&lt;j 以及所有的 i，造出 bih，这时，包含 bij 的非终极符号的集合定义如下：</p>
<p><code>bij = { A|对于 1≤k&lt;j，B 包含在 bik 中，C 包含在b(i+k,j-k) 中，并且存在语法规则 A→BC }</code></p>
</li>
</ul>
<p>CYK 算法由小型分析树逐渐扩大，同样的分析树不会重复计算，不需要进行回溯，规则都采用 Chomsky 范式。</p>
<h2 id="Tomita-算法"><a href="#Tomita-算法" class="headerlink" title="Tomita 算法"></a>Tomita 算法</h2><p>1985 年，卡内基-梅隆大学计算语言学家 M. Tomita 提出，是一种扩充的 LR 算法。引入了图结构栈、子树共享和局部歧义紧缩等技术，提高了算法效率。</p>
<ul>
<li>LR 分析方法：把分析状态和分析动作对应关系组织在一张分析表中<ul>
<li>动作表（ACTION）：描述某个状态下遇到某个展望符号时分析器所应该采取的分析动作；</li>
<li>转移表（GOTO）：描述当归约动作发生以后，分析状态应该怎样转移。</li>
</ul>
</li>
<li>LR 不适用于所有的上下文无关语法<ul>
<li>LR 是一个由分析表驱动的自底向上的分析算法，它的确定性表现在 LR 分析表的确定性上。每个单元格最多只有一个分析动作或状态。</li>
<li>LR 分析只能处理可以构造出确定分析表的上下文无关语法（LR 语法）。另外，如果语法有歧义，那肯定不是 LR 语法。所以 LR 不适合分析自然语言（分析表的某些单元格会出现一个以上的分析动作，分析表会出现多重入口，无法只按照一种路径分析）。</li>
</ul>
</li>
<li>因此，Tomita 引入图结构栈技术以对付多重的分析动作（LR 分析表的多重入口），图结构栈由栈表技术和树结构栈发展而来。<ul>
<li>栈表技术：每个进程对应一个栈，每个进程动作与标准 LR 分析一样。缺点：进程之间没有关系，出现歧义时栈表数目指数增长。</li>
<li>树结构栈：将进程处于相同状态的进程合并为栈顶顶点，当栈顶被弹出时又会分解为原来的几个栈。枝干数依然会随歧义的增加指数增长。</li>
<li>图结构栈：改变了树结构栈在分裂时复制若干个的做法，只将栈的某些部分分裂，表示为一棵树。利用栈合并技术可将其表示为 DAG（有向无环图）。不会对句子的任何部分以同样的方式做两次或两次以上的分析。</li>
</ul>
</li>
<li>歧义句子总数（形成 “森林”）可能随句子长度增加而指数增长，Tomita 提出了 “子树共享” 和 “局部歧义紧缩” 技术保证分析森林的大小不至于过快增加。<ul>
<li>子树共享：如果几棵树存在一个共同子树，则子树只表达一次，构成 “共享森林”。</li>
<li>局部歧义紧缩：当两个或两个以上的子树具有相同的叶节点，且这几棵子树的根具有相同的非终极符号（即句子某一部分能用两种或两种以上的方式归约为一个非终极符号时），这若干个子树就构成一个 “局部歧义”；因为局部歧义太多会导致总的歧义数量指数增长，需要把具有局部歧义的子树的若干个顶点结合为一体，进行 “局部歧义紧缩”。<ul>
<li>当出现局部歧义时，把表达局部歧义的子树的根合并为一个结点，该结点为 “紧缩结点”；</li>
<li>在图结构栈中，如果两个或两个以上的符号具有相同的状态顶点在紧缩结点左边，又有相同的状态在右边，就表示这几个符号具有 “局部歧义”。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="管辖-约束理论与最简方案"><a href="#管辖-约束理论与最简方案" class="headerlink" title="管辖-约束理论与最简方案"></a>管辖-约束理论与最简方案</h2><blockquote>
<p>普遍语法属于人类语言的共性。凡是能够用普遍语法原则说明的语言现象，就不必在个别语言的语法中分别做出具体的规定了。</p>
<p>我们所希望发现的是建立在几个基本原则上的高度结构化的普遍语法理论——这些原则严格限制可以获取的那些语法的种类，大力约束它们的形式，但是又具有必须用经验来确定的诸多参数。</p>
<p>——Chomsky</p>
</blockquote>
<p>Chomsky 基于那样的认识提出了 “管约论”，其核心是一系列普遍性的原则，称为 “原则子系统”。</p>
<h3 id="X-阶标理论（X-bar-theory）"><a href="#X-阶标理论（X-bar-theory）" class="headerlink" title="X 阶标理论（X-bar theory）"></a>X 阶标理论（X-bar theory）</h3><p>1970 年提出，认为：</p>
<ul>
<li>短语范畴应该分析为词汇范畴的阶标投射，阶标可以分为若干层次，最低层次的词 X 就是中心语，中心语带有若干个补足语，中心语管辖着补足语。</li>
<li>词汇范畴应该分析为一组特征。<ul>
<li>动词短语、名词短语、形容词短语和介词短语的重写规则都可以归纳为：<code>XP → X Comp</code>。</li>
<li>重写为：<code>X&#39; → X Comp</code>，<code>X&#39;</code> 表示比 X 更高一个层次的语类。</li>
<li>最高层写作 XP，中间需要加几层就加几个 <code>&#39;</code>，XP 下面的整个树形就是 XP 所属的 “最大投射”。</li>
<li>X 阶标语法比短语结构语法表达能力更强。短语结构语法只有单词型语类和短语型语类，缺乏中间层次。</li>
<li>X 阶标语法比短语结构语法更为严谨（层次严格）。</li>
</ul>
</li>
</ul>
<h3 id="题元理论（θ-theory）"><a href="#题元理论（θ-theory）" class="headerlink" title="题元理论（θ-theory）"></a>题元理论（θ-theory）</h3><p>Chomsky 把逻辑学的命题中的谓词和个体词的关系用 θ（题元）来表示，称为 “题元关系”。充当题元的词语称为 “主目”，不充当或不能充当的称为 “非主目”。</p>
<ul>
<li>每个主目必须，而且只许充当一个题元；</li>
<li>每个题元必须，而且只许由一个主目充当。</li>
</ul>
<p>如果在题元位置缺少有形词，就必须用无形词填充，叫 “空语类”，用 PRO 表示。</p>
<h3 id="格理论（Case-theory）"><a href="#格理论（Case-theory）" class="headerlink" title="格理论（Case theory）"></a>格理论（Case theory）</h3><p>“格” 是一个抽象的概念，只要名词处于一定的句法关系，不论有没有形态上的变化，就都有格。</p>
<p>动词和介词后面可以直接接一个名词短语做补语，而名词和形容词后面不可以直接跟补语，必须在中间插入一个介词。因为：动词和介词的补语有格，而名词和形容词的没有。<strong>所以，动词和介词能够指定格，名词和形容词不能。</strong>根据格理论可以解释语类的不同性质。</p>
<h3 id="管辖理论（government-theory）"><a href="#管辖理论（government-theory）" class="headerlink" title="管辖理论（government theory）"></a>管辖理论（government theory）</h3><p>所谓 “管辖” 就是成分之间的支配关系，它要说明短语中的各个成分是否在同一管辖区域之内，以及在管辖区域之内，什么是主管成分，什么是受管成分。</p>
<p>从 X 阶标理论角度看，主管成分就是 X 阶标结构中的最低一个层次 X，受管成分就是 X 的补语 Comp。</p>
<h3 id="约束理论（binding-theory）"><a href="#约束理论（binding-theory）" class="headerlink" title="约束理论（binding theory）"></a>约束理论（binding theory）</h3><p>所谓 “约束” 就是语义解释的照应关系。“约束” 要说明在管辖区域内的成分，在什么样的情况下是自由的，在什么样的情况下是受约束的。</p>
<p>三条约束原则：</p>
<ul>
<li>照应词（类似反身代词如 himself 这样的词）在管辖区域内受约束</li>
<li>代词（类似 him 这样的词）在管辖区域内是自由的</li>
<li>指称词（类似 John 这样的词）总是自由的</li>
</ul>
<p>所谓某个名词短语受到 “约束”，是指它与先于它的另外一个名词短语指同一客体；所谓某个名词短语 “自由”，是指它与先于它的名词短语不指同一客体；所谓 “管辖区域” 是指最底层的 S 和 NP。</p>
<h3 id="界限理论（bounding-theory）"><a href="#界限理论（bounding-theory）" class="headerlink" title="界限理论（bounding theory）"></a>界限理论（bounding theory）</h3><p>研究对转换范围的限制，重点讨论 wh-移位应该在什么样的区域范围内进行。</p>
<p>在英语中，S 和 NP 都是界点，领属条件规定，wh-移位时，不能一步越过两个界点。</p>
<h3 id="控制理论（control-theory）"><a href="#控制理论（control-theory）" class="headerlink" title="控制理论（control theory）"></a>控制理论（control theory）</h3><p>主要研究如何解释语音上是零的空语类 PRO。</p>
<p>控制理论的基本原则是 “最小距离原则”，也就是说，如果控制带有宾语，则定宾语为控制成分；如果不带宾语，则定主语为控制成分。大部分动词都已宾语为控制成分。</p>
<h3 id="Chomsky-语言哲学"><a href="#Chomsky-语言哲学" class="headerlink" title="Chomsky 语言哲学"></a>Chomsky 语言哲学</h3><p>20 世纪 90 年代，Chomsky 又提出了参数方法和最简方案，把生成语法的研究提高到一个新阶段。赋予生成语法以生命的是生成语法的语言哲学理论，其中最为重要的是关于人类知识的本质、来源和使用问题。</p>
<ul>
<li><p>语言知识的本质</p>
<p>Chomsky 把语言知识的本质问题叫做 “Humboldt（洪堡特）问题”。德国学者 W. Humboldt 提出 “语言绝不是产品，而是一种创造性活动”，语言实际是心智不断重复的活动，它使音节得以成为思想的表达。人类语言知识的本质就是语言知识如何构成的问题，其核心是 Humboldt 指出的 “有限手段的无限使用”。语言知识的本质在于人类成员的心智/大脑中，存在着一套语言认知系统，这样的认知系统表现为某种数量有限原则和规则体系。高度抽象的语法规则构成了语言应用所需要的语言知识，由于人们不能自觉地意识到这些抽象的语法规则，Chomsky 主张，这些语言知识是一些不言而喻的或者无意识的知识。<strong>我们应当把语言知识和语言的使用能力区分开，语言能力可以改进，而语言知识则保持不变；语言能力可以损伤或消失，但并不至于失去语言知识。所以，语言知识是内在于心智的特征和表现，语言能力是外在行为的表现。生成语法研究的是语言的心智知识，而不是语言的行为能力。语言知识体现为存在于心智/大脑中的认知系统。</strong></p>
</li>
<li><p>语言知识的来源</p>
<p>语言知识的来源在西方哲学中是 “Plato 问题”，与此相应，人类语言知识的来源问题是：为什么儿童在较少直接语言经验的情况下，能够快速一致地学会语言？</p>
<blockquote>
<p>这让我们想到了 <a href="https://www.wbur.org/edify/2018/02/01/mit-artificial-intelligence" target="_blank" rel="noopener">MIT Launches Initiative To Develop Artificial Intelligence That Learns Like Children | Edify</a> 中 Josh Tenenbaum 教授在 MIT 发布 的Intelligence Quest 项目后的采访中说过的话：“Imagine if we could build a machine that grows into intelligence the way a human being does, that starts off like a baby and that learns like a child. This is really the oldest dream of artificial intelligence”，他认为人类的孩子是宇宙中唯一真正已知的通往智能的捷径。</p>
</blockquote>
<p>Chomsky 对此的解释是：在人类成员的心智/大脑中，存在着由生物遗传而天赋决定的认知机制系统。这些认知系统叫做 “心智器官”，决定构成人类语言知识的是心智器官中的 “语言机能”，这个语言机能在经验环境引发下的生长和成熟，决定这人类语言知识的获得。语言机能有初始状态和获得状态。初始状态是人类共同的、普遍一致的；获得状态是具体的、个别的；初始状态叫做 “普遍语法”，获得状态叫做 “具体语法”。对普遍语法的本质特征及其与具体语法的关系的研究和确定，是解决语言知识 “柏拉图问题” 的关键。</p>
</li>
<li><p>生成语法的本质<br>Chomsky 坚持认为语言机能内在于心智/大脑，对语言的研究是对心智的研究，最终是在抽象的水平上对大脑结构的研究。因此，生成语法研究在学科上属于 “认知心理学”，最终属于 “人类生物学”。实际应当叫做 “生物语言学”。这是与其他传统语言研究的根本区别。生成语法追求的目标是在理想化和抽象化的条件下，构建关于语言和心智的理论，它期待与主体自然科学的统一。这就是生成语法 “方法论的自然主义”，Chomsky 力图把对于语言、心智的研究以及对于大脑的研究统一在一个共同的理论原则之下，最后把它纳入自然科学的总体研究之中。</p>
</li>
<li><p>生成语法的运算系统</p>
<p>Chomsky 主张，语言是语言机能或者语言机器所呈现的状态，某人具有语言 L，就是说他的语言机能处于状态 L。这个状态是一个生成系统或运算系统（能够生成无限多的语言表达式），Chomsky 把这样的运算系统叫做 “I 语言”（大写的 i）。</p>
<ul>
<li>内在的：心智的组成部分，最终表现在大脑的神经机制之中</li>
<li>个体的：直接与个体有关，与语言社团存在间接联系，后者的存在取决于其成员具有相似的 I 语言</li>
<li>内涵的：是一个函数或生成程序，生成一系列内在的表现于心智/大脑中的结构描写</li>
</ul>
</li>
<li><p>生成语法与结构主义语法的区别</p>
<ul>
<li>结构主义语法的方法是：在广泛搜集语言材料的基础上，通过切分、归类、替换等程序，概括出有关语言的语法规则。</li>
<li>结构主义语法是经验主义的方法，其基础是外在主义的语言观。</li>
</ul>
</li>
<li><p>生成语法的最简单主义</p>
<ul>
<li><p>生成语法的一个重要原则，可以分为 “方法论最简单主义” 和 “实体性最简单主义”。</p>
</li>
<li><p>方法论最简单主义：在科学研究中使用最小数量的理论原则和理论构件；最大限度地减少复杂性，消除冗余性，增加理论原则的抽象性和概括性；构建最简单的理论模式和最具有解释性的理论；寻求理论的对称性和完美性。</p>
<blockquote>
<p>这里提到的方法很容易让人产生联想，比如《最省力原则》中的哲学思考，比如梯度下降的实用方法，比如关于美感的计算。</p>
</blockquote>
</li>
<li><p>实体性最简单主义：要求科学研究对象本身在设计和结构方面具有简单性、优化性和完美性。</p>
</li>
<li><p>原因和动机</p>
<ul>
<li>什么是人类语言机能应该被期望去满足的一般条件？<ul>
<li>语言机能自身在心智/大脑认知系统序列中的位置是什么？Chomsky：是心智/大脑中其他认知系统对于语言机能所施加的界面条件。</li>
<li>那些具有某些独立性的一般概念自然性的考虑，即简单性、经济性、对称性、非冗余性等等，对于语言机能施加的是一些什么样的条件？Chomsky：科学研究对于客体对象所施加的一般性条件，属于方法论的 “最简单主义” 的范畴。</li>
</ul>
</li>
<li>在哪种程度上，语言机能是由这些条件所决定的，而不存在超出它们的特殊结构？Chomsky：语言机能可以很好地满足这些外界性条件，从这个意义上说，语言是一个 “完美的系统”。</li>
</ul>
</li>
</ul>
</li>
<li><p>生成语法的理论构建</p>
<ul>
<li>逐步抽象化、概括化和最简单化的过程。</li>
<li>Chomsky：<ul>
<li>早期的短语结构语法：生成能力过强，经常生成不符合语法的句子</li>
<li>转换的方法（转换规则系统）：规则系统越来越复杂</li>
<li>限制规则的条件理论：限制和减少规则数量</li>
<li>原则和参数方式：从语言本身的设计特征以及它与其他认知系统的相互关系出发，消除了一切只是服务于语言机能内部的理论构件，使得生成语法的整体模式达到了空前的简单性和完美性</li>
</ul>
</li>
<li>原则和参数阶段<ul>
<li>提出语法规则系统：由词库、句法和语音式、逻辑式构成</li>
<li>提出普遍语法的原则子系统：X 阶标理论、题元理论、格理论、管辖理论、约束理论、界限理论和控制理论</li>
<li>一般性原则：投射原则、准许原则和完全解释原则</li>
</ul>
</li>
</ul>
</li>
<li><p>生成语法的 Y 模式图式</p>
<ul>
<li>用来说明语法规则和运算的表现形式</li>
<li><img src="http://qnimg.lovevivian.cn/book-2017-fengzhiwei-nlp-5.jpeg" alt=""></li>
<li>I 表示语法基础部分的短语结构规则，II 表示转移规则移动，III 是音位规则，IV 是逻辑规则</li>
<li>I 生成 D-结构，II 把 D-结构转化为 S-结构，III 把 S-结构转化为语音表现形式 PF，IV 把 S-结构转化为逻辑表现形式 LF。</li>
<li>D 和 S 完全属于语言机能内部，D 结构与词库发生内在性界面关系，S 起着中心枢纽作用；PF 和 LF 分别与心智中的其他认知系统和信念系统形成界面关系。D 和 S 不存在先后顺序问题。</li>
<li>Σ = (D, S, P, L)</li>
</ul>
</li>
<li><p>最简方案的总体性考虑</p>
<ul>
<li>与内在语言有关的应用系统总体上可以分为两个：一个是发生感知系统（A-P）；一个是概念意向系统（C-I）。语言与他们形成的界面一般认为就是语音表现形式和逻辑式。</li>
<li>语言包括词库（描写词汇项目特征）和运算系统（使用这些词汇成分生成推导式和结构描写）两个组成部分。推导式是运算的规程，结构描写是运算的结果。最简方案的设想：除了语音形式的选择和词汇的任意性之外，语言变体只限于词库中那些非实体性的部分（功能成分）和词汇项目（词库）的一般性特征。</li>
<li>原则参数方法的题元理论、格理论、约束理论等，只能在界面上起作用，并通过界面获得它们存在的原因和动机。</li>
<li>由普遍语法的运算推导可产生 “收敛” 和 “破裂” 两个结果。如果一个推导式同时收敛于 PF 层面和 LF 层面，才可以算是真正的收敛。</li>
</ul>
</li>
</ul>
<h2 id="Joshi-的树邻接语法"><a href="#Joshi-的树邻接语法" class="headerlink" title="Joshi 的树邻接语法"></a>Joshi 的树邻接语法</h2><p>1975 年，A. K. Joshi, L. S. Levy, M. Takahashi 等提出 ”树邻接语法“（TAG, Tree Adjoining Grammar），可以识别和生成 ”树邻接语言“（Tree Adjoining Language, TAL）。以句法结构树作为核心操作对象，在树的基础上组织语言知识，产生规则对应着树结构，以线性的一维形式来表达二维的树结构。与短语结构语法的不同之处在于，树邻接语法的规则比短语结构语法的规则更加细致。前者是一个基于符号串的生成系统，后者是基于树的生成系统（当然，树邻接语言仍然是符号串语言）。基本构成要素和操作模式：</p>
<ul>
<li>一个五元组（Σ, NT, I, A, S）<ul>
<li>Σ 是终极符号的有限集合</li>
<li>NT 是非终极符号的有限集合</li>
<li>S 是初始符号，特殊的非终极符号</li>
<li>I 是初始树的有限集合，两个特征<ul>
<li>所有的非叶节点都用非终极符号标记</li>
<li>所有的叶节点，或者用终极符号标记，或者用带有下箭头的非终极符号标记；下箭头是初始树的标志，它的含义是 ”替换“，表示该节点可以被其他的树结构替换。</li>
<li>如果一个初始树的根节点为 X，则这个初始树在 TAG 系统中叫作 X 类型的初始树。</li>
</ul>
</li>
<li>A 是辅助树的有限集合，两个特征：<ul>
<li>所有的非叶节点都用非终极符号标记</li>
<li>辅助树叶上的结点用终极符号或非终极符号标记</li>
<li>A 树叶上的非终极符号结点带有插接符号，用星号标注将被插接的结点，这个结点叫做 ”落脚结点“，该节点标记的非终极符号（短语类符号）要跟它所在的树结构的根节点相同。</li>
</ul>
</li>
<li>I U A 中的所有树叫做基础树，TAG 是树生成系统，但生成的树可以分析和解释目标语言的串语言。</li>
</ul>
</li>
<li><p>如果一棵树由集合 I U A 中的任意两课树组合而成，这棵树叫做 ”推导树“。得到推导树的两个操作：插接和替换。</p>
<ul>
<li>插接：把辅助树插到任意树中而建立一棵新树的过程<ul>
<li>选择插接：辅助树可以插接到指定结点上，但这种辅助树上的插接不是强制的</li>
<li>空插接：在指定结点上不允许有任何邻接成分</li>
<li>强制插接：辅助树一定要插接在指定结点</li>
</ul>
</li>
<li>替换：用推导树替换初始树而建立一棵新树的过程<ul>
<li>标记了替换的结点（下箭头）不允许出anren现任何插接操作</li>
</ul>
</li>
</ul>
</li>
<li><p>推导关系树：确定推导树构成过程的树。</p>
<ul>
<li>一个父节点的所有子结点的地址都不相同</li>
</ul>
</li>
<li>TAG 生成的树集合<ul>
<li>从某个以 S 为根的初始树推导出的绝对初始树的集合。</li>
<li>绝对初始树：叶子上没有替换节点的初始树<ul>
<li>可识别树集合严格包含在树邻接语法的树集合中</li>
<li>在给定 TAG 树集合中，所有树的路径集合都是上下文无关语法</li>
<li>对于 TAG 中的每个语法 G，G 的树集合都可以多次被识别</li>
</ul>
</li>
</ul>
</li>
<li><p>TAG 中的串语言</p>
<ul>
<li>树邻接语法可以识别和最终生成的语言是树邻接语言，树邻接语言不再包含任何形式的树，是一种串语言。树邻接语言生成的树最终还是为了识别和生成这样的串语言。</li>
<li>T_G = \{ t | t 是从某个以 S 为根的初始树的推导结果\}，假设 L(G) 是 TAG 的串语言，则 L(G) 是树集合中所有树的生成结果的集合：L(G) = \{w | w 是 T_G 中某个树 t 的生成结果}</li>
<li>属性：<ul>
<li>树邻接语言完全包括上下文无关语言；</li>
<li>树邻接语言是半线性的；</li>
<li>树邻接语言是语言的完整抽象集合；</li>
<li>TAG 的自动机是嵌入式下推自动机；</li>
<li>树邻接语言都有一个启动词条；</li>
<li>树邻接语言可以被多次分析。</li>
</ul>
</li>
</ul>
</li>
<li><p>总的来说，一个 TAG 语法包括一组有限的初始树和辅助树，用一个 TAG 语法生成自然语言中的句子，就是从 S 类型的初始树开始，不断地进行替换和插接操作，直到所有带替换标记的结点已经被替换了；所有带插接标记的结点都已经被插接了，最后把所得到的树的叶节点按顺序列出，就可以得到该 TAG 语法所生成的句子集合。在自然语言句子分析时，从包含树中词语的树结构开始，通过替换和插接操作，形成一个以 S 为根节点的树结构。</p>
</li>
<li>LTAG：把词汇信息引入 TAG 的规则中。<ul>
<li>把每一个初始树和辅助树都与某一个或某一些具体的单词关联起来，带有词的结点叫做这个树的 ”抛锚点“。</li>
<li>进一步限制了短语结构语法过强的生成能力，提高了自然语言处理的精确度和效率。</li>
</ul>
</li>
</ul>
<h2 id="汉字结构的形式描述"><a href="#汉字结构的形式描述" class="headerlink" title="汉字结构的形式描述"></a>汉字结构的形式描述</h2><ul>
<li>汉字分为独体字和合体字。独体字只能分离出笔画，合体字由两个或以上的部件组成。合体字的三个层次：合体字、部件、笔画。部件是枢纽，是汉字形体结构的核心。</li>
<li>如果一个部件继续分解就成为笔画，这种部件叫做 ”末级部件“。独体字可看作由一个末级部件组成，所有的汉字均有部件组成。汉字：648 个末级部件，其中 327 个是独体字。</li>
<li>汉字结构的上下文无关语法：<ul>
<li>G = (Vn, Vt, S, P)</li>
<li>Vn = \{A, B, C, D, E, F, G, H, I, J, K\}，表示汉字的 11 种基本结构：上下、上中下、左右、左中右、左上包围、右上包围、左下包围、左上右包围、上左下包围、左下右包围、全包围</li>
<li>Vt = \{O\}，O 可以为各种终极符号，就是 648 个末级部件</li>
<li>S：初始符号，要分析其结构的汉字本身，只能取 Vn 中唯一的一个值，这是上下文无关语法描述汉字和句法结构时的差别</li>
</ul>
</li>
<li>括号表达式：树形图可以转写为等价的括号形式，如 ”霜“：A(O, C(O, O))<ul>
<li>三部件：15 个小类</li>
<li>四部件：19 个小类</li>
<li>五部件：19 个小类</li>
<li>六部件：10 个小类</li>
<li>七部件：4 个小类</li>
<li>八部件：1 个小类</li>
<li>九部件：1 个小类</li>
</ul>
</li>
</ul>
<h2 id="Hausser-的左结合语法"><a href="#Hausser-的左结合语法" class="headerlink" title="Hausser 的左结合语法"></a>Hausser 的左结合语法</h2><p>德国埃尔朗根-纽伦堡大学计算语言学教授 Roland Hausser 创立，他还提出了 “数据库语义学” 和完整的 “语表组合线性内部匹配” 理论。</p>
<p>2006 年，Hausser 出版的《自然语言交流的计算机模型：数据库语义学下的语言理解、推理和生成》中，他系统分析了自然语言的主要结构，分析了听话人模式和说话人模式下的示意推导。</p>
<p>Hausser 提出的 “语表组合线性内部匹配（SLIM）” 理论以人作为人机交流的主体，而不是以语言符号为主体，要求通过完全显化的机械步骤，使用逻辑和电子的方式来解释自然语言理解和自然语言的生成过程。</p>
<ul>
<li>表层成分（surface）：以语表组合性作为它的方法论原则；</li>
<li>线性（linearity）：以时间线性作为它的实证原则；</li>
<li>内部因素（internality）：以语言的内部因素作为它的本体论原则；</li>
<li>匹配（matching）：以语言和语境信息之间的匹配作为它的功能原则。</li>
</ul>
<p>SLIM 的技术实现手段叫做 “数据库语义学（DBS）”：把自然语言理解和生成重新建构为 “角色转换” 的规则体系，即 “说话人模式” 和 “听话人模式” 的互相转换。</p>
<ul>
<li>Step1：DBS 输入，要求计算机具备外部界面。听话人模式中的自然主体从另一个主体或语境获得信息。</li>
<li>Step2：左结合语法模拟，处于听话人模式，叫做 LA-hear。自然主体在自己的认知当中分析信息。</li>
<li>Step3：左结合语法的第二个变体负责在内存词库中搜索合适的内容，叫做 LA-think。自然主体思考如何做出反应。</li>
<li>Step4：左结合语法的第三个变体的任务是语言生成，叫做 LA-speak。自然主体用语言或行动作出反馈。</li>
</ul>
<p>DBS 的结果用 DBS 图表示，一种树结构，与短语结构语法和依存语法的树结构不同：</p>
<ul>
<li>短语结构语法：句子的层次和单词之间的前后线性关系很清楚，但句子中各个成分的中心不突出（没有说明哪个是中心词）。</li>
<li>依存语法：没有范畴结点，单词间依存关系很清楚，依存关系是二元关系，支配者是中心词，被支配者是从属词。但单词之间前后线性顺序不如短语结构语法明确。</li>
<li>DBS 图：着重分析语言内容，结点单词都用原型词表示，没有定冠词等结点。节点之间的连线有明确含义：<ul>
<li>竖线 “|” 表示修饰-被修饰关系</li>
<li>左斜线 “/” 表示主语-动词关系</li>
<li>右斜线 “\” 表示宾语-动词关系</li>
<li>水平线 “—” 表示并列关系</li>
</ul>
</li>
<li>词性关系图：DBS 图结点的词替换为词性，语义关系图就变成了 “词性关系图”。</li>
<li>编号弧图：表示激活语义关系图的时间线性顺序。所有表示推导的编号弧的方向是自底向上的。</li>
<li>语表实现图：表示如何按照遍历顺序生成语言的表层形式。</li>
<li>数据库语义学的两个基础：<ul>
<li>左结合语法。<ul>
<li>按照自然语言的时间线性顺序自左向右结合进行分析与计算的方法。</li>
<li>与短语结构语法是同质的，不同之处在于后者依据的是 “替换原则”，前者是 “可持续性原则”。</li>
<li>推导时，总是从左向右，自底向上，沿着树结构的左侧一步一步把单词逐一结合起来。</li>
<li>一种基于短语结构语法的形式模型，同时吸取了依存语法和数据库语义学的优点。</li>
</ul>
</li>
<li>单词数据库。存储单词的内容的存储形式是一种非递归的特征结构，叫做 “命题因子”，一个命题因子是 “属性-值偶对” 的集合。每个单词或句子元素的句法语义信息都体现为相应的属性-值矩阵。</li>
</ul>
</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章介绍了各种各样基于短语结构（“上下文无关” 指语法中重写规则的形式，而不是指它所生成的语言）的语法分析方法，不过无论哪种方法，最终的表现形式基本都是树（或类树）结构。</p>
<ul>
<li>递归转移网络和扩充转移网络：1970 年，美国 W. Woods 在《自然语言分析的转移网络语法》中提出扩充转移网络（Augmented Transition Network，ATN）。前者是基于有限状态语法、经过扩充的有限状态转移图。后者是基于前者的进一步扩充。</li>
<li>自顶向下和自底向上分析：基于上下文无关语法。</li>
<li>通用句法处理器和线图分析法：<ul>
<li>1973 年，R. M. Kaplan 提出了通用句法处理器（General Syntacic Processor，GSP），其基本数据结构是 “线图”（把树形图加以修改而形成的一种图，便于更直观地表示前于关系）。不仅是一种处理方法，也是一个能形式地描写各种方法的系统。</li>
<li>1968 年，J. Earley 发表的博士论文《一种高效的上下文无关分析算法》提出了 Earley 算法和点规则的概念，为线图分析法奠定理论基础。</li>
<li>1980 年，Martin Key 在《句法处理中的算法图和数据结构》中，提出了 “线图分析法”。Martin Key 提出的 “点规则” 更加直观地表示 “活性边” 和 “非活性边”，经过修改的线图比原来的线图功能更强，叫 “活性线图”。活性边和非活性边分别表示尚未证实的假设和已经证实的假设。</li>
</ul>
</li>
<li>Earley 算法：<ul>
<li>美国学者 J. Earley 于 1968 年在博士论文中提出的算法，在左角分析法的基础上把自顶向下和自底向上结合起来，分析过程中交替使用。</li>
<li>规则表示方法体现了<strong>点规则</strong>的基本精神，实质与线图分析法规则一致。表示方法是把点规则写成：<code>“A→α·β,[i,j]”</code> 的形式，该算法的核心是线图。对于句子中的单词，线图包含一个状态来表示在此之前已经生成的部分；在句子终点，线图提供出该句子所有可能的分析结果。</li>
<li>Earley 算法的分析过程没有回溯（通过遍历 predictor），改进了自顶向下分析的效果。</li>
</ul>
</li>
<li>左角分析法：<ul>
<li>左角分析法（left-corner parsing method）是一种自顶向下分析法和自底向上分析法结合起来的分析法。所谓 “左角” 是指表示句子句法结构的树形图的任何子树中左下角的那个符号。</li>
<li>左角分析法使用了回溯，M. Marcus 1980 年提出用人工方法对规约的条件加以控制，从而避免了回溯，这就是 “Marcus 确定性分析算法”。</li>
</ul>
</li>
<li>CYK 算法：<ul>
<li>一种并行的、以 Chomsky 范式为描述对象的句法分析算法。</li>
<li>CYK 算法由小型分析树逐渐扩大，同样的分析树不会重复计算，不需要进行回溯，规则都采用 Chomsky 范式。</li>
</ul>
</li>
<li><p>Tomita 算法：</p>
<ul>
<li>1985 年，卡内基-梅隆大学计算语言学家 M. Tomita 提出，是一种扩充的 LR 算法（LR 分析方法：把分析状态和分析动作对应关系组织在一张分析表中）。</li>
<li>引入了图结构栈、子树共享和局部歧义紧缩等技术，提高了算法效率。</li>
</ul>
</li>
<li><p>树邻接语法：</p>
<ul>
<li>1975 年，A. K. Joshi, L. S. Levy, M. Takahashi 等提出 ”树邻接语法“（TAG, Tree Adjoining Grammar），可以识别和生成 ”树邻接语言“（Tree Adjoining Language, TAL）。</li>
<li>以句法结构树作为核心操作对象，在树的基础上组织语言知识，产生规则对应着树结构，以线性的一维形式来表达二维的树结构。与短语结构语法的不同之处在于，树邻接语法的规则比短语结构语法的规则更加细致。前者是一个基于符号串的生成系统，后者是基于树的生成系统（当然，树邻接语言仍然是符号串语言）。</li>
</ul>
</li>
<li>左结合语法：<ul>
<li>德国埃尔朗根-纽伦堡大学计算语言学教授 Roland Hausser 创立，他还提出了 “数据库语义学” 和完整的 “语表组合线性内部匹配” 理论。</li>
<li>按照自然语言的时间线性顺序自左向右结合进行分析与计算的方法。与短语结构语法是同质的，不同之处在于后者依据的是 “替换原则”，前者是 “可持续性原则”。推导时，总是从左向右，自底向上，沿着树结构的左侧一步一步把单词逐一结合起来。</li>
</ul>
</li>
</ul>
<p>除此之外，还有两个特别值得一提的：</p>
<ul>
<li>Chomsky 的管辖-约束理论与最简方案，内容涉及 Chomsky 的语言哲学，其关于语言的本质思考及生成语法理论的分析和深入探讨引人深思。</li>
<li>作者关于汉字结构的形式描述，以部件为核心，构建了汉字结构的上下文无关语法，无疑是一种创新的方法。</li>
</ul>
<p>本章内容乍看非常复杂繁琐，但其实并不难，每块内容都有例子配套，很容易理解（虽然可能会有些耗时）。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/12/22/NLPFA/2018-12-22-Ch03-Formal-Model-Based-on-Phrase-Structure-Grammar/">
    <time datetime="2018-12-22T03:32:00.000Z" class="entry-date">
        2018-12-22
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Model/">Formal Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a></li></ul>

    </footer>
</article>





  
    <article id="post-2018-12-15-Text-Classification-Data-Plot" class="post-2018-12-15-Text-Classification-Data-Plot post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/12/15/2018-12-15-Text-Classification-Data-Plot/">绘制文本分类数据</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2018/12/15/2018-12-15-Text-Classification-Data-Plot/" data-id="cjtx2yw6m000jvcccjp2vl4ev" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>问题的起因是最近做的一个项目需要在后端绘制 Scatter，横轴是 float 数据，纵轴是分类的文本标签。具体的要求是：</p>
<ul>
<li>每个数据集可能有若干个主体，也就是一个画布可能需要绘制多幅图；</li>
<li>每幅图的分类类型并不一定相同，但整体类别是知道的；比如：共有 8 种颜色 ”红橙黄绿青蓝紫“，但主体 1 的类别可能是 ”红橙黄“，主体 2 的类别可能是 ”红黄绿“；</li>
<li>要保证同一种类别在图中的颜色标记是一样的，比如：红色类别是红色，那么如果某一个主体的类别中没有红色类别，其他类别在画图时也不应使用红色；</li>
<li>要保证类别的顺序按给定的顺序；比如给定的顺序是 ”红橙黄绿青蓝紫“，主体 1 的类别是 ”红橙黄“，那绘制出来的图像 y 轴必须是按照这个顺序下来的，如果是 ”红黄蓝“ 也是类似。</li>
</ul>
<p>本来项目是用 NodeJS 写的，后端画图找了不少工具都不太好用（前端工具巨多），后来用了 <a href="https://github.com/plotly/plotly-nodejs" target="_blank" rel="noopener">plotly/plotly-nodejs</a>，但是表现力方面差强人意，而且由于是调用 RESTFul API，数据点太多时会超时，本身也会有网络请求耗时。最后就想到用 Python 在内部起一个 server，使用 Matplotlib 或 Seaborn 绘图。</p>
<p>Seaborn 一行命令就可以绘制，而且参数可以自动把不同的主体区分开；Matplotlib 就稍微麻烦些，不能直接实现预期的目的，后来经过试验，发现可以将类别转为数字然后再将数字的 y 轴转为 string 即可。</p>
<p>Notebook 在这里：<a href="https://github.com/hscspring/Note_DS/blob/master/Visualization/text-classification-data.ipynb" target="_blank" rel="noopener">text-classification-data</a>，或用 nbviewer 打开：<a href="http://nbviewer.jupyter.org/github/hscspring/Note_DS/blob/master/Visualization/text-classification-data.ipynb" target="_blank" rel="noopener">Jupyter Notebook Viewer</a></p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/12/15/2018-12-15-Text-Classification-Data-Plot/">
    <time datetime="2018-12-15T06:00:00.000Z" class="entry-date">
        2018-12-15
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Coding/">Coding</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Matplotlib/">Matplotlib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Seaborn/">Seaborn</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Visualization/">Visualization</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2018-10-11-Ch02-Pioneers-in-Language-Computing" class="post-NLPFA/2018-10-11-Ch02-Pioneers-in-Language-Computing post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/10/11/NLPFA/2018-10-11-Ch02-Pioneers-in-Language-Computing/">自然语言计算机形式分析的理论与方法笔记(Ch02)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2018/10/11/NLPFA/2018-10-11-Ch02-Pioneers-in-Language-Computing/" data-id="cju9u9zao000o5occgtgnyzz6" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第二章：语言计算研究的先驱"><a href="#第二章：语言计算研究的先驱" class="headerlink" title="第二章：语言计算研究的先驱"></a>第二章：语言计算研究的先驱</h1><ul>
<li>A.A.Markov 的马尔科夫链</li>
<li>G.K.Zipf 的Zipf 定律</li>
<li>Shannon 的熵</li>
<li>Y.Bar-Hillel 的范畴语法</li>
<li>Z.Harris 的语言串分析</li>
<li>Kulakina 的语言集合论模型</li>
</ul>
<h2 id="Markov-链"><a href="#Markov-链" class="headerlink" title="Markov 链"></a>Markov 链</h2><p>可以将语言的使用看作一个随机过程：</p>
<ul>
<li>时间的函数，随时间改变而变</li>
<li>每时刻函数值不确定，符合一定概率分布</li>
</ul>
<p>Markov 链：每个语言符号出现概率不相互独立，每一个随机过程的结局依赖于前面随机过程的结果，这种链就叫做 Markov 链。</p>
<ul>
<li>一重 Markov 链，二元语法：只考虑前面一个语言符号对后面一个语言符号出现概率的影响</li>
<li>二重 Markov 链，三元语法：考虑前面两个语言符号对后面一个语言符号出现概率的影响</li>
<li>三重 Markov 链，四元语法：考虑前面三个语言符号对后面一个语言符号出现概率的影响</li>
</ul>
<h2 id="Zipf-定律"><a href="#Zipf-定律" class="headerlink" title="Zipf 定律"></a>Zipf 定律</h2><ul>
<li>1916 年，J.Estoup：<script type="math/tex">n_r · r = k</script>，n 为单词的绝对频率，r 为对应的序号</li>
<li>1928 年，E.Condon：<script type="math/tex">f_r = cr^{-1}, f = \frac{n_r}{N}, c = \frac{k}{N}</script>，N 为所考察文本的总长度</li>
<li>1935 年，G.K.Zipf<ul>
<li>检验 Condon 的结果，相同；t→∞ 时，频率 f 变成了概率 p：<script type="math/tex">p_r = cr^{-1}</script></li>
<li>r=1 时，c=p1，测出 c=0.1</li>
<li>修正：c 为一个参数：0&lt;c&lt;0.1，对于 r=1……n，c 使得 Σp=1，这个单参数频率分布定律即为 Zipf 定律</li>
</ul>
</li>
<li>1936 年，M.Joos：<script type="math/tex">p_r = cr^{-b}, b>0, c>0</script>，对于 r=1……n，b，c 使得 Σp=1，即为双参数频率分布定律，也叫双参数 Zipf 定律</li>
<li>20 世纪 50 年代初期，B.B.Mandelbrot：<script type="math/tex">p_r = c(r+a)^{-b}, 0≤a<1, b>0, c>0</script>，对于 r=1……n，a，b，c 使得 Σp=1，即为三参数 Zipf 定律<ul>
<li>c 与出现概率最高的单词的概率大小有关</li>
<li>b 与高概率单词的数量有关，对于 r&lt;50 的高概率单词，b 是非减函数，随着 r 的增大，b 并不减小</li>
<li>a 与单词的数量 N 有关</li>
</ul>
</li>
</ul>
<p>公式的性质（一个 r 对应一个 p）决定了文本中不能存在频率相同的单词，但实际上当单词频率比较小的时候，频率相同的词会大大增加，此时会出现数据稀疏问题。</p>
<h2 id="Shannon-关于-“熵”-的研究"><a href="#Shannon-关于-“熵”-的研究" class="headerlink" title="Shannon 关于 “熵” 的研究"></a>Shannon 关于 “熵” 的研究</h2><ul>
<li>什么是熵？</li>
</ul>
<p>自然语言是一种具有概率性的随机过程（信息的本质），每一个语言符号出现的不定度很大（Markov 链）。信息量的大小就是用在接收到消息之前，随机试验不定度（熵）的大小来度量的。接收到语言符号前，熵因语言符号数目和出现的概率不同而不同，接收到后，不定度被消除，熵等于零。<strong>信息量是被消除的熵，熵是信息量的度量。</strong></p>
<ul>
<li><p>如何衡量熵的大小？</p>
<ul>
<li>1928 年：L.Hartley：某装置有 D 个可能的位置或状态，两个组合会有：D^2 个状态。要使 2X 个装置的能力恰为 X 个装置的 2 倍，定义一个装置的信息能力为：logD，D 为整个系统可以进入的不同状态数。即：logD^2 = 2logD</li>
<li>1951 年：Shannon：如果随机试验有 n 个结局，而且是不等概率的，随机试验的熵为：<script type="math/tex">H_1 = - \sum^{n}_{i=1} p_i \log_2p_i</script>。<ul>
<li>可以想象为最优编码中一定的判断或信息编码的位数的下界。</li>
<li>当编码对象的先验概率不相等时，<strong>熵就是其最小编码位数</strong>（即对高概率的编码短，低概率的编码长，加权平均后为最小编码位数，比等概率平均要小）。</li>
</ul>
</li>
<li>Shannon：用 log n 度量<strong>某一个</strong>有 n 个可能等概率结果的随机试验的熵是合理的<ul>
<li>n 越大不确定度越大</li>
<li>两个试验，log n^2 = 2log n 是一个的两倍</li>
<li>两个试验，分别 m 和 n 个可能结局，复合试验 mn 个可能的等概率结局，log mn = log m + log n</li>
<li>log n = -(log 1 - log n) = - log 1/n，1/n 即为概率，- n × 1/n × log 1/n = - Σ 1/n × log 1/n = log n</li>
</ul>
</li>
</ul>
</li>
<li><p>困惑度</p>
<ul>
<li>2^H，熵越大，困惑度越大</li>
<li>均概率时，熵最大（最大熵原理），便可以通过优化熵达到优化模型的目的</li>
<li>随着 Markov 链重数增大，条件熵越来越小</li>
<li>熵有下限，随着重数增加，熵逐渐趋于稳定而不再减少，此时的熵就是包含在自然语言一个符号中的真实信息量，叫做极限熵。</li>
</ul>
</li>
<li><p>熵率：单词数除序列熵（可以想象成每个单词的熵）</p>
</li>
<li><p>把语言想象成产生单词序列的随机过程 L，则 <script type="math/tex">H(L) = \lim_{n\to\infin} \frac{1}{n} H(w_1, w_2, ..., w_n)</script> </p>
<p>= <script type="math/tex">\lim_{n\to\infin} -\frac{1}{n} \sum_{w \in L} P(w_1, w_2, ..., w_n) \log_{2} P(w_1, w_2, ..., w_n)</script>（注：原书没有 负号）</p>
<p>根据 Shannon-McMillan-Breiman 定理，如果语言在某种意义下是正则的（既是平稳的，又是遍历的），那么有：<script type="math/tex">H(L) = \lim_{n \to \infin} (-\frac{1}{n} \log_2 P(w_1, w_2, ..., w_n))</script></p>
<ul>
<li>意味着可以取语言中的一个足够长的序列来替代语言中所有可能的序列总和。</li>
<li>定理的直觉理解：一个足够长的单词序列可以在其中包含其他很多较短的序列，而且每一个这些较短的序列都可以按照各自的概率重复地出现在较长的序列之中。</li>
<li>自然语言不平稳，所以统计模型对自然语言分布和熵的描述都是近似的。</li>
</ul>
</li>
<li><p>语言的极限熵</p>
<ul>
<li>可靠下界</li>
<li>理解语言中哪一部分提供的信息量最大（高频字熵值大，能够提供的信息量小）</li>
</ul>
</li>
</ul>
<blockquote>
<p>关于熵，会专门撰文一篇。</p>
</blockquote>
<h2 id="Bar-Hillel-的范畴语法"><a href="#Bar-Hillel-的范畴语法" class="headerlink" title="Bar-Hillel 的范畴语法"></a>Bar-Hillel 的范畴语法</h2><ul>
<li>1935 年，数理逻辑学家 Kazimierz Ajukievicz 提出了范畴语法的基本概念</li>
<li>1958 年，数学家 J. Joachim Lambek 在《句子结构的数学》中提出了句法类型演算的理论，可以判断一个符号串是不是语言中成立的句子</li>
<li>1959 年，Bar-Hillel 在《自然语言结构的判定程序》中进一步发展了句法类型演算的理论</li>
<li>1960 年，Bar-Hillel 等在《论范畴语法和短语结构语法》中把这种语法命名为 “范畴语法”</li>
<li>20 世纪 90 年代，M. Steedman 和 J. Baldridge 提出了组合范畴语法（Combinatory Categorial Grammar，CCG）：添加了函子范畴的组合运算。</li>
</ul>
<h3 id="句法类型"><a href="#句法类型" class="headerlink" title="句法类型"></a>句法类型</h3><p>任何词都可以根据它在句子中的功能归入一定的句法类型，如果用 n 表示名词的句法类型，S 表示句子，则其他的一些句法类型都可以用 n 和 S 以不同的方式结合起来表示。规则：</p>
<ul>
<li>有词 B，其后面词 C 的句法类型为 γ，词序列 BC 的功能与 β 相同，则词 B 的句法类型为：β/γ</li>
<li>有词 B，其前面词 A 的句法类型为 α，词序列 AB 的功能与 β 相同，则词 B 的句法类型为：α\β</li>
<li>有词 B，其前面词 A 的句法类型为 α，后面词 C 句法类型为 γ，词序列 ABC 的功能与 β 相同，则词 B 的句法类型为 α\β/γ</li>
</ul>
<p>英语的句法类型：</p>
<ul>
<li>名词：<strong>n</strong>，John</li>
<li>形容词：<strong>n/n</strong>，poor，即 poor John 也为名词</li>
<li>不及物动词：<strong>n\S</strong>，works，即 John works 功能与句子相同</li>
<li>及物动词：<strong>n\S/n</strong>，likes，即 John likes Jane 功能与句子相同</li>
<li>副词：<strong>(n\S)\n\S</strong>，soundly，即 John slept soundly，slept soundly 功能与句子相同</li>
<li>副词：<strong>S\S</strong>，here，即 John works here，John works 为 S，John works here 为新的 S</li>
<li>副词：<strong>n\S/(n\S)</strong>，never，即 John never works，works 为 n\S，never works 也为 n\S</li>
<li>介词：<strong>S\S/n</strong>，for，即 John works for Jane，John works 为 S</li>
<li>连词：<strong>S\S/S</strong>，and，即 John works and Jane resets</li>
</ul>
<p>S 和 n 为原子范畴，其他此类为复合范畴。S 代表了陈述句所表示的真值命题，n 代表了该命题中的论元。</p>
<p>范畴语法是词汇主义的典型代表之一，因为任何一个单词的语法特征都可以通过原子范畴和复合范畴表示出来。</p>
<p>英语动词短语增加的句法类型符号：</p>
<ul>
<li>i 表示不及物动词的不定式</li>
<li>p 表示不及物动词的现在分词</li>
<li>q 表示不及物动词的过去分词</li>
</ul>
<h3 id="句法类型演算"><a href="#句法类型演算" class="headerlink" title="句法类型演算"></a>句法类型演算</h3><ul>
<li><p>如果有形如 α，α\β/γ，γ 的符号序列，用 β 替换，包括：</p>
<ul>
<li>用 β 替换形如 α，α\β 的符号序列：(α)(α\β) → β</li>
<li>用 β 替换形如 β/γ，γ 的符号序列：(β/γ)(γ) → β</li>
</ul>
</li>
<li><p>补充规则：</p>
<ul>
<li>(α\β)(β\γ) → α\γ</li>
<li>(α/β)(β/γ) → α/γ</li>
</ul>
</li>
<li><p>如果语言中的词标上句法类型，通过有穷个演算步骤，可以把词序列化为 S，则这个序列便是该语言中的合格句子。</p>
</li>
<li><p>一个词可以属于几个句法类型，实际演算中应该把每个词可能有的句法类型全部列出来。</p>
</li>
<li><p>范畴语法的规则考虑了语义，不过是通过句法类型以及反映这些类型的语义连锁的演算规则潜在地表示出来。</p>
</li>
<li><p>范畴语法与短语结构语法风格不同：</p>
<ul>
<li>短语结构语法力图对句子进行切分，采用的是<strong>解析模式</strong>。</li>
<li>范畴语法力图反映句法类型的语义连锁，采用的是<strong>构造模式</strong>。设法把语义直接表示在句法之中。</li>
</ul>
</li>
<li><p>演算乘法表</p>
<p>前向/后项|i/i|i|i/n|i/q|i/p|i/(q/n)<br>—-|—-|—-|—-|—-|—-|—-<br><strong>i/i</strong>|i/i|i|i/n|i/q|i/p|i/(q/n)<br><strong>p/i</strong>|p/i|p|p/n|p/q|p/p|p/(q/n)<br><strong>q/i</strong>|q/i|q|q/n|q/q|q/p|q/(q/n)<br><strong>n\S/i</strong>|n\S/i|n\S|n\S/n|n\S/q|n\S/p|n/S/(q/n)</p>
<p>相交处的值可以反向展开，可以对语言现象获得新的认识。</p>
</li>
</ul>
<h2 id="Harris-的语言串分析法"><a href="#Harris-的语言串分析法" class="headerlink" title="Harris 的语言串分析法"></a>Harris 的语言串分析法</h2><ul>
<li>1962 年发表《句子结构的串分析》提出了 “语言串理论” 和 “语言串分析法”，最早在计算机上实现的自然语言处理的形式模型。</li>
<li>串的两种表示<ul>
<li>词串：任何一个句子或其组成部分中按线性顺序排列的一个或多个词。</li>
<li>串式：用词类或其次类替换词串中的具体得出的单词而形成的符号串。</li>
</ul>
</li>
<li>句子由若干个基本串通过附加、连接和替换等方式组合而成。组成句子的基本串中至少有一个中心串，代表句子的基干，也代表一种语言的基本句式。基本串包括中心串、附加串、连接串和替换串。从中心串出发通过扩展生成无限的句子。</li>
<li>语言串分析法的句法规则：<ul>
<li>词串替换为串式</li>
<li>逐步切除附加串以获取中心串</li>
<li>写出过程的句法规则</li>
</ul>
</li>
<li>英语语言串分析系统：<ul>
<li>N. Sager 20 世纪 80 年代：语言串分析器 LSP</li>
<li>T. Strzalkowski：英语句法分析器 TTP</li>
</ul>
</li>
</ul>
<h2 id="O-C-K-的语言集合论模型"><a href="#O-C-K-的语言集合论模型" class="headerlink" title="O.C.K 的语言集合论模型"></a>O.C.K 的语言集合论模型</h2><ul>
<li>1958 年发表《根据集合论定义语法概念的一种方法》，用集合论的方法建立自然语言的数学模型。以此模拟机器翻译中从词归约为词组，从词组归约为句子的层次分析过程。</li>
<li>通过毗连运算而形成的词的一切组合可以分为两个子集：<ul>
<li>成立句子（语法正确，不是语义）的子集</li>
<li>不成立句子的子集</li>
</ul>
</li>
<li>词的集合 W 和在 W 上成立的句子集 θ，就有了语言 L = {W, θ}，称为词汇集合 W 上的一种语言。</li>
<li>词域：某个词的完整的形式系统（即词形变化的全部形式）。词域可将 W 划分为彼此不想交的子集之并，得出域的分划，记为 𝜞 分划。</li>
<li>族：对 A1xA2（A、A2 为任意词串），x 替换为 y 句子成立，则 x 和 y 等价；等价的元素进入同一族。族可将 W 分割为彼此不想交的子集之并，得出族的分划，记为 <strong>S</strong> 分划。</li>
<li>不考虑分划标准，只要用彼此不想交之并的形式表示 W：<strong>B</strong> 分划。一个子集合只有一个词：<strong>E</strong> 分划。</li>
<li>对任意句子 A=x1…xn，给定 B 分划，词 xi 所进入的子集合的序列，称为句子 A 的 B 结构，记为 B(A)。</li>
<li>同一句子在不同分划下有不同的 B 结构（ES𝜞）形式。其实分别是：单个词、单个词的各种变形、可替换的同一类词。</li>
<li>过程：递归将复杂结构按其层次化为不能再归约的简单结构。<ul>
<li>n 级 B 格式 B~(n)：<ul>
<li>含有的元素不少于两个</li>
<li>存在一个元素 Bαn 使得 n-1 级 B 结构 B(A1) B~(n) B(A2) 和 B 结构 B(A1) Bαn B(A2) 在任何词串 A1 和 A2 中同时成立或不成立</li>
</ul>
</li>
<li>n 级 B 结构：不包含 n 级 B 格式的 B 结构：B(A1) Bαn B(A2)</li>
<li>B~(n) 可以替换为 Bαn，Bαn 称为 “结果元”，可以不唯一。</li>
</ul>
</li>
</ul>
<p><strong>小结：</strong>Markov 链和熵的使用非常广泛深入，但 Zipf 定律可能只是作为分析特定语言规律的一个工具，在实际应用场景中比较少见。范畴语法试图通过语法表达语义，采用构造模式，通过简单的原子范畴可以演算出所有的句子，而且对句法类型反向展开还有可能获得关于语言的新认识，可能是一种不错的方式。语言串分析法感觉和句法分析类似，串式具有抽象性，句法同样如此。语言集合论其实也是从词的角度（单个词、单个词+变种、一类词）试图将句子不断递归归约为简单结构，可以看成是机器翻译句法分析过程的数学模拟，具有一定参考价值。本章内容（尤其后面三节）表面看起来非常枯燥难懂，其实认真看下书还是很容易理解的，作者的解释非常细致。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/10/11/NLPFA/2018-10-11-Ch02-Pioneers-in-Language-Computing/">
    <time datetime="2018-10-11T03:32:00.000Z" class="entry-date">
        2018-10-11
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Entropy/">Entropy</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Zipf/">Zipf</a></li></ul>

    </footer>
</article>





  
    <article id="post-2018-09-30-The-Science-of-Artificial" class="post-2018-09-30-The-Science-of-Artificial post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/09/30/2018-09-30-The-Science-of-Artificial/">西蒙《人工科学》笔记</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2018/09/30/2018-09-30-The-Science-of-Artificial/" data-id="cjtx2yw8c0043vcccyq2lx85c" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><div class="toc"><ul class="toc-item"><li><span><a href="#1.-理解自然界和人工界" data-toc-modified-id="1.-理解自然界和人工界-1">1. 理解自然界和人工界 </a></span><ul class="toc-item"><li><span><a href="#1.1-人工界" data-toc-modified-id="1.1-人工界-1.1">1.1 人工界 </a></span></li><li><span><a href="#1.2-作为泛型的环境" data-toc-modified-id="1.2-作为泛型的环境-1.2">1.2 作为泛型的环境 </a></span></li><li><span><a href="#1.3-通过模拟获得理解" data-toc-modified-id="1.3-通过模拟获得理解-1.3">1.3 通过模拟获得理解 </a></span></li><li><span><a href="#1.4-作为人工物的计算机" data-toc-modified-id="1.4-作为人工物的计算机-1.4">1.4 作为人工物的计算机 </a></span></li><li><span><a href="#1.5-符号系统：合理的人工物" data-toc-modified-id="1.5-符号系统：合理的人工物-1.5">1.5 符号系统：合理的人工物 </a></span></li></ul></li><li><span><a href="#2.-经济合理性：适应性的手段" data-toc-modified-id="2.-经济合理性：适应性的手段-2">2. 经济合理性：适应性的手段 </a></span><ul class="toc-item"><li><span><a href="#2.1-经济活动者" data-toc-modified-id="2.1-经济活动者-2.1">2.1 经济活动者 </a></span></li><li><span><a href="#2.2-市场与组织" data-toc-modified-id="2.2-市场与组织-2.2">2.2 市场与组织 </a></span><ul class="toc-item"><li><span><a href="#2.2.1-看不见的手" data-toc-modified-id="2.2.1-看不见的手-2.2.1">2.2.1 看不见的手 </a></span></li><li><span><a href="#2.2.2-不确定性与预期" data-toc-modified-id="2.2.2-不确定性与预期-2.2.2">2.2.2 不确定性与预期 </a></span></li><li><span><a href="#2.2.3-企业组织" data-toc-modified-id="2.2.3-企业组织-2.2.3">2.2.3 企业组织 </a></span></li><li><span><a href="#2.2.4-组织忠诚与认同" data-toc-modified-id="2.2.4-组织忠诚与认同-2.2.4">2.2.4 组织忠诚与认同 </a></span></li><li><span><a href="#2.2.5-小结：市场与组织的作用" data-toc-modified-id="2.2.5-小结：市场与组织的作用-2.2.5">2.2.5 小结：市场与组织的作用 </a></span></li></ul></li><li><span><a href="#2.3-进化模型" data-toc-modified-id="2.3-进化模型-2.3">2.3 进化模型 </a></span><ul class="toc-item"><li><span><a href="#2.3.1-关于经济人的另类理论" data-toc-modified-id="2.3.1-关于经济人的另类理论-2.3.1">2.3.1 关于经济人的另类理论 </a></span></li><li><span><a href="#2.3.2-局部极大与全局极大" data-toc-modified-id="2.3.2-局部极大与全局极大-2.3.2">2.3.2 局部极大与全局极大 </a></span></li><li><span><a href="#2.3.3-进化的近视性" data-toc-modified-id="2.3.3-进化的近视性-2.3.3">2.3.3 进化的近视性 </a></span></li><li><span><a href="#2.3.4-经济进化的机制" data-toc-modified-id="2.3.4-经济进化的机制-2.3.4">2.3.4 经济进化的机制 </a></span></li></ul></li><li><span><a href="#2.4-人类社会" data-toc-modified-id="2.4-人类社会-2.4">2.4 人类社会 </a></span></li><li><span><a href="#2.5-小结" data-toc-modified-id="2.5-小结-2.5">2.5 小结 </a></span></li></ul></li><li><span><a href="#3.-思维心理学：将智慧嵌入自然" data-toc-modified-id="3.-思维心理学：将智慧嵌入自然-3">3. 思维心理学：将智慧嵌入自然 </a></span><ul class="toc-item"><li><span><a href="#3.1-作为人工科学的心理学" data-toc-modified-id="3.1-作为人工科学的心理学-3.1">3.1 作为人工科学的心理学 </a></span><ul class="toc-item"><li><span><a href="#3.1.1-搜索策略" data-toc-modified-id="3.1.1-搜索策略-3.1.1">3.1.1 搜索策略 </a></span></li><li><span><a href="#3.1.2-绩效的限度" data-toc-modified-id="3.1.2-绩效的限度-3.1.2">3.1.2 绩效的限度 </a></span></li></ul></li><li><span><a href="#3.2-概念获得速度的限度" data-toc-modified-id="3.2-概念获得速度的限度-3.2">3.2 概念获得速度的限度 </a></span></li><li><span><a href="#3.3-记忆的参量——每块-8-秒" data-toc-modified-id="3.3-记忆的参量——每块-8-秒-3.3">3.3 记忆的参量——每块 8 秒 </a></span></li><li><span><a href="#3.4-记忆参量——-7-块还是-2-块" data-toc-modified-id="3.4-记忆参量——-7-块还是-2-块-3.4">3.4 记忆参量—— 7 块还是 2 块 </a></span></li><li><span><a href="#3.5-记忆的组织" data-toc-modified-id="3.5-记忆的组织-3.5">3.5 记忆的组织 </a></span><ul class="toc-item"><li><span><a href="#3.5.1-刺激物的组块" data-toc-modified-id="3.5.1-刺激物的组块-3.5.1">3.5.1 刺激物的组块 </a></span></li><li><span><a href="#3.5.2-视觉记忆" data-toc-modified-id="3.5.2-视觉记忆-3.5.2">3.5.2 视觉记忆 </a></span></li><li><span><a href="#3.5.3-脑海" data-toc-modified-id="3.5.3-脑海-3.5.3">3.5.3 脑海 </a></span></li></ul></li><li><span><a href="#3.6-处理自然语言" data-toc-modified-id="3.6-处理自然语言-3.6">3.6 处理自然语言 </a></span><ul class="toc-item"><li><span><a href="#3.6.1-语言处理中的语义学问题" data-toc-modified-id="3.6.1-语言处理中的语义学问题-3.6.1">3.6.1 语言处理中的语义学问题 </a></span></li></ul></li><li><span><a href="#3.7-结论" data-toc-modified-id="3.7-结论-3.7">3.7 结论 </a></span></li></ul></li><li><span><a href="#4-记忆与学习：作为思想环境的记忆" data-toc-modified-id="4-记忆与学习：作为思想环境的记忆-4">4 记忆与学习：作为思想环境的记忆 </a></span><ul class="toc-item"><li><span><a href="#4.1-语义丰富的领域" data-toc-modified-id="4.1-语义丰富的领域-4.1">4.1 语义丰富的领域 </a></span><ul class="toc-item"><li><span><a href="#4.1.1-长时记忆" data-toc-modified-id="4.1.1-长时记忆-4.1.1">4.1.1 长时记忆 </a></span></li><li><span><a href="#4.1.2-直觉" data-toc-modified-id="4.1.2-直觉-4.1.2">4.1.2 直觉 </a></span></li><li><span><a href="#4.1.3-多少信息？" data-toc-modified-id="4.1.3-多少信息？-4.1.3">4.1.3 多少信息？</a></span></li><li><span><a href="#4.1.4-为处理的记忆" data-toc-modified-id="4.1.4-为处理的记忆-4.1.4">4.1.4 为处理的记忆 </a></span></li></ul></li><li><span><a href="#4.2-理解与表现" data-toc-modified-id="4.2-理解与表现-4.2">4.2 理解与表现 </a></span><ul class="toc-item"><li><span><a href="#4.2.1-一个有理解力的程序" data-toc-modified-id="4.2.1-一个有理解力的程序-4.2.1">4.2.1 一个有理解力的程序 </a></span></li><li><span><a href="#4.2.2-理解物理学" data-toc-modified-id="4.2.2-理解物理学-4.2.2">4.2.2 理解物理学 </a></span></li><li><span><a href="#4.2.3-规模与简单性" data-toc-modified-id="4.2.3-规模与简单性-4.2.3">4.2.3 规模与简单性 </a></span></li></ul></li><li><span><a href="#4.3-学习" data-toc-modified-id="4.3-学习-4.3">4.3 学习 </a></span><ul class="toc-item"><li><span><a href="#4.3.1-理解式学习" data-toc-modified-id="4.3.1-理解式学习-4.3.1">4.3.1 理解式学习 </a></span></li><li><span><a href="#4.3.2-产生系统" data-toc-modified-id="4.3.2-产生系统-4.3.2">4.3.2 产生系统 </a></span></li><li><span><a href="#4.3.3-从例子中学习" data-toc-modified-id="4.3.3-从例子中学习-4.3.3">4.3.3 从例子中学习 </a></span></li></ul></li><li><span><a href="#4.4-发现过程" data-toc-modified-id="4.4-发现过程-4.4">4.4 发现过程 </a></span><ul class="toc-item"><li><span><a href="#4.4.1-无目标的解决问题过程" data-toc-modified-id="4.4.1-无目标的解决问题过程-4.4.1">4.4.1 无目标的解决问题过程 </a></span></li><li><span><a href="#4.4.2-重新发现经典物理学" data-toc-modified-id="4.4.2-重新发现经典物理学-4.4.2">4.4.2 重新发现经典物理学 </a></span></li><li><span><a href="#4.4.3-找到新的问题表象" data-toc-modified-id="4.4.3-找到新的问题表象-4.4.3">4.4.3 找到新的问题表象 </a></span></li></ul></li><li><span><a href="#4.5-结论" data-toc-modified-id="4.5-结论-4.5">4.5 结论 </a></span></li></ul></li><li><span><a href="#5-设计科学：创造人工物" data-toc-modified-id="5-设计科学：创造人工物-5">5 设计科学：创造人工物 </a></span><ul class="toc-item"><li><span><a href="#5.1-设计的逻辑——固定了的备择方案" data-toc-modified-id="5.1-设计的逻辑——固定了的备择方案-5.1">5.1 设计的逻辑——固定了的备择方案 </a></span><ul class="toc-item"><li><span><a href="#5.1.1-命题性逻辑的悖论" data-toc-modified-id="5.1.1-命题性逻辑的悖论-5.1.1">5.1.1 命题性逻辑的悖论 </a></span></li><li><span><a href="#5.1.2-还原至叙述逻辑" data-toc-modified-id="5.1.2-还原至叙述逻辑-5.1.2">5.1.2 还原至叙述逻辑 </a></span></li><li><span><a href="#5.1.3-计算最优状况" data-toc-modified-id="5.1.3-计算最优状况-5.1.3">5.1.3 计算最优状况 </a></span></li><li><span><a href="#5.1.4-找出令人满意的行动方案" data-toc-modified-id="5.1.4-找出令人满意的行动方案-5.1.4">5.1.4 找出令人满意的行动方案 </a></span></li></ul></li><li><span><a href="#5.2-设计逻辑：找出备择方案" data-toc-modified-id="5.2-设计逻辑：找出备择方案-5.2">5.2 设计逻辑：找出备择方案 </a></span><ul class="toc-item"><li><span><a href="#5.2.1-手段——目的分析" data-toc-modified-id="5.2.1-手段——目的分析-5.2.1">5.2.1 手段——目的分析 </a></span></li><li><span><a href="#5.2.2-搜索逻辑" data-toc-modified-id="5.2.2-搜索逻辑-5.2.2">5.2.2 搜索逻辑 </a></span></li></ul></li><li><span><a href="#5.3-作为资源分配方法的设计" data-toc-modified-id="5.3-作为资源分配方法的设计-5.3">5.3 作为资源分配方法的设计 </a></span><ul class="toc-item"><li><span><a href="#5.3.1-公路设计的例子" data-toc-modified-id="5.3.1-公路设计的例子-5.3.1">5.3.1 公路设计的例子 </a></span></li><li><span><a href="#5.3.2-指导搜索的方法" data-toc-modified-id="5.3.2-指导搜索的方法-5.3.2">5.3.2 指导搜索的方法 </a></span></li></ul></li><li><span><a href="#5.4-设计的形态：层级结构" data-toc-modified-id="5.4-设计的形态：层级结构-5.4">5.4 设计的形态：层级结构 </a></span><ul class="toc-item"><li><span><a href="#5.4.1-产生者——检验周期" data-toc-modified-id="5.4.1-产生者——检验周期-5.4.1">5.4.1 产生者——检验周期 </a></span></li><li><span><a href="#5.4.2-作为风格决定者的过程" data-toc-modified-id="5.4.2-作为风格决定者的过程-5.4.2">5.4.2 作为风格决定者的过程 </a></span></li></ul></li><li><span><a href="#5.5-设计的表象" data-toc-modified-id="5.5-设计的表象-5.5">5.5 设计的表象 </a></span><ul class="toc-item"><li><span><a href="#5.5.1-作为表象变换的解决问题过程" data-toc-modified-id="5.5.1-作为表象变换的解决问题过程-5.5.1">5.5.1 作为表象变换的解决问题过程 </a></span></li><li><span><a href="#5.5.2-空间表象" data-toc-modified-id="5.5.2-空间表象-5.5.2">5.5.2 空间表象 </a></span></li><li><span><a href="#5.5.3-表象的分类" data-toc-modified-id="5.5.3-表象的分类-5.5.3">5.5.3 表象的分类 </a></span></li></ul></li><li><span><a href="#5.6-小结——设计理论中的论题" data-toc-modified-id="5.6-小结——设计理论中的论题-5.6">5.6 小结——设计理论中的论题 </a></span></li><li><span><a href="#5.7-设计在精神生活中的作用" data-toc-modified-id="5.7-设计在精神生活中的作用-5.7">5.7 设计在精神生活中的作用 </a></span></li><li><span><a href="#5.8-小结" data-toc-modified-id="5.8-小结-5.8">5.8 小结 </a></span></li></ul></li><li><span><a href="#6-社会计划：进化着的人工物的设计" data-toc-modified-id="6-社会计划：进化着的人工物的设计-6">6 社会计划：进化着的人工物的设计 </a></span><ul class="toc-item"><li><span><a href="#6.1-设计问题的表象" data-toc-modified-id="6.1-设计问题的表象-6.1">6.1 设计问题的表象 </a></span><ul class="toc-item"><li><span><a href="#6.1.1-作为表象的组织" data-toc-modified-id="6.1.1-作为表象的组织-6.1.1">6.1.1 作为表象的组织 </a></span></li><li><span><a href="#6.1.2-找出限制性因素" data-toc-modified-id="6.1.2-找出限制性因素-6.1.2">6.1.2 找出限制性因素 </a></span></li><li><span><a href="#6.1.3-无数字的表象" data-toc-modified-id="6.1.3-无数字的表象-6.1.3">6.1.3 无数字的表象 </a></span></li></ul></li><li><span><a href="#6.2-计划用的资料" data-toc-modified-id="6.2-计划用的资料-6.2">6.2 计划用的资料 </a></span><ul class="toc-item"><li><span><a href="#6.2.1-预测" data-toc-modified-id="6.2.1-预测-6.2.1">6.2.1 预测 </a></span></li><li><span><a href="#6.2.2-反馈" data-toc-modified-id="6.2.2-反馈-6.2.2">6.2.2 反馈 </a></span></li></ul></li><li><span><a href="#6.3-谁是客户？" data-toc-modified-id="6.3-谁是客户？-6.3">6.3 谁是客户？</a></span><ul class="toc-item"><li><span><a href="#6.3.1-专业人员——客户关系" data-toc-modified-id="6.3.1-专业人员——客户关系-6.3.1">6.3.1 专业人员——客户关系 </a></span></li><li><span><a href="#6.3.2-作为客户的社会" data-toc-modified-id="6.3.2-作为客户的社会-6.3.2">6.3.2 作为客户的社会 </a></span></li></ul></li><li><span><a href="#6.4-社会设计中的组织" data-toc-modified-id="6.4-社会设计中的组织-6.4">6.4 社会设计中的组织 </a></span></li><li><span><a href="#6.5-设计的时间期和空间域" data-toc-modified-id="6.5-设计的时间期和空间域-6.5">6.5 设计的时间期和空间域 </a></span><ul class="toc-item"><li><span><a href="#6.5.1-贴现未来" data-toc-modified-id="6.5.1-贴现未来-6.5.1">6.5.1 贴现未来 </a></span></li><li><span><a href="#6.5.2-时间视界的变化" data-toc-modified-id="6.5.2-时间视界的变化-6.5.2">6.5.2 时间视界的变化 </a></span></li><li><span><a href="#6.5.3-给进步下定义" data-toc-modified-id="6.5.3-给进步下定义-6.5.3">6.5.3 给进步下定义 </a></span></li><li><span><a href="#6.5.4-注意力管理" data-toc-modified-id="6.5.4-注意力管理-6.5.4">6.5.4 注意力管理 </a></span></li></ul></li><li><span><a href="#6.6-无最后目标的设计" data-toc-modified-id="6.6-无最后目标的设计-6.6">6.6 无最后目标的设计 </a></span><ul class="toc-item"><li><span><a href="#6.6.1-出发点" data-toc-modified-id="6.6.1-出发点-6.6.1">6.6.1 出发点 </a></span></li><li><span><a href="#6.6.2-设计是有价值的活动" data-toc-modified-id="6.6.2-设计是有价值的活动-6.6.2">6.6.2 设计是有价值的活动 </a></span></li><li><span><a href="#6.6.3-社会计划与进化" data-toc-modified-id="6.6.3-社会计划与进化-6.6.3">6.6.3 社会计划与进化 </a></span></li></ul></li><li><span><a href="#6.7-社会课程体系" data-toc-modified-id="6.7-社会课程体系-6.7">6.7 社会课程体系 </a></span></li><li><span><a href="#6.8-小结" data-toc-modified-id="6.8-小结-6.8">6.8 小结 </a></span></li></ul></li><li><span><a href="#7-复杂性面面观" data-toc-modified-id="7-复杂性面面观-7">7 复杂性面面观 </a></span><ul class="toc-item"><li><span><a href="#7.1-复杂性的概念" data-toc-modified-id="7.1-复杂性的概念-7.1">7.1 复杂性的概念 </a></span><ul class="toc-item"><li><span><a href="#7.1.1-整体论与还原论" data-toc-modified-id="7.1.1-整体论与还原论-7.1.1">7.1.1 整体论与还原论 </a></span></li><li><span><a href="#7.1.2-控制论和一般系统论" data-toc-modified-id="7.1.2-控制论和一般系统论-7.1.2">7.1.2 控制论和一般系统论 </a></span></li></ul></li><li><span><a href="#7.2-对复杂性的当前兴趣" data-toc-modified-id="7.2-对复杂性的当前兴趣-7.2">7.2 对复杂性的当前兴趣 </a></span><ul class="toc-item"><li><span><a href="#7.2.1-突变论" data-toc-modified-id="7.2.1-突变论-7.2.1">7.2.1 突变论 </a></span></li><li><span><a href="#7.2.2-复杂性与混沌" data-toc-modified-id="7.2.2-复杂性与混沌-7.2.2">7.2.2 复杂性与混沌 </a></span></li><li><span><a href="#7.2.3-突变世界或混沌世界中的理性" data-toc-modified-id="7.2.3-突变世界或混沌世界中的理性-7.2.3">7.2.3 突变世界或混沌世界中的理性 </a></span></li><li><span><a href="#7.2.4-复杂性与进化" data-toc-modified-id="7.2.4-复杂性与进化-7.2.4">7.2.4 复杂性与进化 </a></span></li></ul></li><li><span><a href="#7.3-结论" data-toc-modified-id="7.3-结论-7.3">7.3 结论 </a></span></li></ul></li><li><span><a href="#8-复杂性的构造：层级系统" data-toc-modified-id="8-复杂性的构造：层级系统-8">8 复杂性的构造：层级系统 </a></span><ul class="toc-item"><li><span><a href="#8.1-层级系统" data-toc-modified-id="8.1-层级系统-8.1">8.1 层级系统 </a></span><ul class="toc-item"><li><span><a href="#8.1.1-社会系统" data-toc-modified-id="8.1.1-社会系统-8.1.1">8.1.1 社会系统 </a></span></li><li><span><a href="#8.1.2-生物系统和物质系统" data-toc-modified-id="8.1.2-生物系统和物质系统-8.1.2">8.1.2 生物系统和物质系统 </a></span></li><li><span><a href="#8.1.3-符号系统" data-toc-modified-id="8.1.3-符号系统-8.1.3">8.1.3 符号系统 </a></span></li></ul></li><li><span><a href="#8.2-复杂系统的进化" data-toc-modified-id="8.2-复杂系统的进化-8.2">8.2 复杂系统的进化 </a></span><ul class="toc-item"><li><span><a href="#8.2.1-生物进化" data-toc-modified-id="8.2.1-生物进化-8.2.1">8.2.1 生物进化 </a></span></li><li><span><a href="#8.2.2-多细胞生物的进化" data-toc-modified-id="8.2.2-多细胞生物的进化-8.2.2">8.2.2 多细胞生物的进化 </a></span></li><li><span><a href="#8.2.3-起到自然选择作用的解决问题过程" data-toc-modified-id="8.2.3-起到自然选择作用的解决问题过程-8.2.3">8.2.3 起到自然选择作用的解决问题过程 </a></span></li><li><span><a href="#8.2.4-选择性的来源" data-toc-modified-id="8.2.4-选择性的来源-8.2.4">8.2.4 选择性的来源 </a></span></li><li><span><a href="#8.2.5-关于帝国与帝国的兴建" data-toc-modified-id="8.2.5-关于帝国与帝国的兴建-8.2.5">8.2.5 关于帝国与帝国的兴建 </a></span></li><li><span><a href="#8.2.6-结论：层级结构的进化论解释" data-toc-modified-id="8.2.6-结论：层级结构的进化论解释-8.2.6">8.2.6 结论：层级结构的进化论解释 </a></span></li></ul></li><li><span><a href="#8.3-近可分解系统" data-toc-modified-id="8.3-近可分解系统-8.3">8.3 近可分解系统 </a></span><ul class="toc-item"><li><span><a href="#8.3.1-社会系统的近可分解性" data-toc-modified-id="8.3.1-社会系统的近可分解性-8.3.1">8.3.1 社会系统的近可分解性 </a></span></li><li><span><a href="#8.3.2-物理化学系统" data-toc-modified-id="8.3.2-物理化学系统-8.3.2">8.3.2 物理化学系统 </a></span></li><li><span><a href="#8.3.3-对层级结构广度的一些看法" data-toc-modified-id="8.3.3-对层级结构广度的一些看法-8.3.3">8.3.3 对层级结构广度的一些看法 </a></span></li><li><span><a href="#8.3.4-小结：近可分解性" data-toc-modified-id="8.3.4-小结：近可分解性-8.3.4">8.3.4 小结：近可分解性 </a></span></li></ul></li><li><span><a href="#8.4-再谈生物进化" data-toc-modified-id="8.4-再谈生物进化-8.4">8.4 再谈生物进化 </a></span></li><li><span><a href="#8.5-复杂性的描述" data-toc-modified-id="8.5-复杂性的描述-8.5">8.5 复杂性的描述 </a></span><ul class="toc-item"><li><span><a href="#8.5.1-近可分解性和可理解性" data-toc-modified-id="8.5.1-近可分解性和可理解性-8.5.1">8.5.1 近可分解性和可理解性 </a></span></li><li><span><a href="#8.5.2-复杂系统的简单描述" data-toc-modified-id="8.5.2-复杂系统的简单描述-8.5.2">8.5.2 复杂系统的简单描述 </a></span></li><li><span><a href="#8.5.3-状态描述和过程描述" data-toc-modified-id="8.5.3-状态描述和过程描述-8.5.3">8.5.3 状态描述和过程描述 </a></span></li><li><span><a href="#8.5.4-自我复制系统的复杂性的描述" data-toc-modified-id="8.5.4-自我复制系统的复杂性的描述-8.5.4">8.5.4 自我复制系统的复杂性的描述 </a></span></li><li><span><a href="#8.5.5-个体发生重演种系发生" data-toc-modified-id="8.5.5-个体发生重演种系发生-8.5.5">8.5.5 个体发生重演种系发生 </a></span></li><li><span><a href="#8.5.6-小结：复杂性的描述" data-toc-modified-id="8.5.6-小结：复杂性的描述-8.5.6">8.5.6 小结：复杂性的描述 </a></span></li></ul></li><li><span><a href="#8.6-结论" data-toc-modified-id="8.6-结论-8.6">8.6 结论 </a></span></li></ul></li><li><span><a href="#后记" data-toc-modified-id="后记-9">后记 </a></span></li><li><span><a href="#CHANGELOG" data-toc-modified-id="CHANGELOG-10">CHANGELOG</a></span></li></ul></div>
        
          <p class="article-more-link">
            <a href="/2018/09/30/2018-09-30-The-Science-of-Artificial/#more" class="more-link">More <span class="meta-nav">→</span></a>
          </p>
        
      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/09/30/2018-09-30-The-Science-of-Artificial/">
    <time datetime="2018-09-30T06:00:00.000Z" class="entry-date">
        2018-09-30
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Economics/">Economics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Simon/">Simon</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2018-09-19-Ch01-Orientation-of-NLP" class="post-NLPFA/2018-09-19-Ch01-Orientation-of-NLP post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/09/19/NLPFA/2018-09-19-Ch01-Orientation-of-NLP/">自然语言计算机形式分析的理论与方法笔记(Ch01)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2018/09/19/NLPFA/2018-09-19-Ch01-Orientation-of-NLP/" data-id="cju9u9zam000n5occkug1yigb" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第一章：自然语言处理的学科定位"><a href="#第一章：自然语言处理的学科定位" class="headerlink" title="第一章：自然语言处理的学科定位"></a>第一章：自然语言处理的学科定位</h1><p>从自然语言处理的过程、范围和历史三个角度考察学科定位问题。</p>
<ul>
<li>过程：纵的角度</li>
<li>范围：横的角度</li>
<li>历史：发展的角度</li>
</ul>
<h2 id="从自然语言处理的过程考察"><a href="#从自然语言处理的过程考察" class="headerlink" title="从自然语言处理的过程考察"></a>从自然语言处理的过程考察</h2><ul>
<li>计算机对自然语言研究和处理的四个过程：<ul>
<li>语言 “形式化”</li>
<li>形式 “算法化”</li>
<li>算法 “程序化”</li>
<li>程序 “实用化”</li>
</ul>
</li>
<li>建立自然语言处理模型需要不同平面的知识：<ul>
<li>声学和韵律学的知识：描述语言的节奏、语调和声调的规律，说明语音怎样形成音位</li>
<li>音位学的知识：描述音位的结合规律，说明音位怎样形成语素</li>
<li>形态学的知识：描述语素的结合规律，说明语素怎样形成单词</li>
<li>词汇学的知识：描述词汇系统的规律，说明单词本身固有的语义特性和语法特性</li>
<li>句法学的知识：描述单词（或词组）之间的结构规则，说明单词（或词组）怎样形成句子</li>
<li>语义学的知识：描述句子中各个成分之间的语义关系，这样的语义关系是与情景无关的，说明怎样从构成的句子的各个成分中推导出整个句子的语义</li>
<li>话语分析的知识：描述句子与句子之间的结合规律，说明怎样由句子形成话语或对话</li>
<li>语用学的知识：描述与情境有关的情景语义，说明怎样推导出句子具有的与周围话语有关的各种含义</li>
<li>外界世界的常识性知识：描述关于语言使用者和语言使用环境的一般性常识，例如语言使用者的信念和目的，说明怎样推导出这样的信念和目的的内在内容</li>
</ul>
</li>
</ul>
<h2 id="从自然语言处理的范围考察"><a href="#从自然语言处理的范围考察" class="headerlink" title="从自然语言处理的范围考察"></a>从自然语言处理的范围考察</h2><ul>
<li>归纳为四个大方向：<ul>
<li>语言学方向</li>
<li>数据处理方向</li>
<li>人工智能和认知科学方向</li>
<li>语言工程方向</li>
</ul>
</li>
<li>具体细分为 13 个方面：<ul>
<li>口语输入</li>
<li>书面语输入</li>
<li>语言分析和理解</li>
<li>语言生成</li>
<li>口语输出技术</li>
<li>话语分析与对话</li>
<li>文献处理</li>
<li>多语</li>
<li>多模态</li>
<li>信息的传输与存储</li>
<li>自然语言处理中的数学方法</li>
<li>语言资源</li>
<li>自然语言处理系统评测</li>
</ul>
</li>
</ul>
<h2 id="从自然语言处理的历史考察"><a href="#从自然语言处理的历史考察" class="headerlink" title="从自然语言处理的历史考察"></a>从自然语言处理的历史考察</h2><h3 id="萌芽期"><a href="#萌芽期" class="headerlink" title="萌芽期"></a>萌芽期</h3><ul>
<li>20 世纪 40 年代到 50 年代末<ul>
<li>A.M.Turing 算法计算模型</li>
<li>N.Chomsky 形式语言理论</li>
<li>C.E.Shannon 概率和信息论模型</li>
<li>机器翻译</li>
</ul>
</li>
<li>20 世纪 50 年代末到 60 年代中期<ul>
<li>自然语言处理分成两个阵营：符号派和随机派</li>
<li>符号派<ul>
<li>Chomsky 等的形式语言理论和生成句法研究<ul>
<li>早期的自顶向下和自底向上算法研究</li>
<li>后期的动态规划研究</li>
<li>Zelig Harris 的 “转换与话语分析课题”</li>
</ul>
</li>
<li>人工智能的研究<ul>
<li>着重研究推理和逻辑问题</li>
<li>Newell 和 Simon 关于 “逻辑理论家” 和 “通用问题解答器”</li>
<li>把模式匹配和关键词搜索与简单试探的方法结合起来进行推理和自动问答</li>
</ul>
</li>
</ul>
</li>
<li>随机派<ul>
<li>贝叶斯方法被用于解决最优字符识别问题</li>
<li>基于转换语法的第一个人类语言计算机处理的可严格测定的心理模型</li>
<li>第一个联机语料库——布朗语料库</li>
</ul>
</li>
<li>机器翻译<ul>
<li>Yngve 主张机器翻译分三个阶段：（1）用代码化的结构标志来表示原语文句的结构；（2）把原语的结构标志转换为译语的结构标志；构成译语的输出问句。把句法分析放在第一位，促进了句法的形式化研究。</li>
<li>语法与算法分开，即语言分析和程序设计分开</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="发展期"><a href="#发展期" class="headerlink" title="发展期"></a>发展期</h3><p>20 世纪 60 年代中期到 80 年代末期。</p>
<ul>
<li><p>B.Vauquois 的 “机器翻译金字塔”  六个步骤</p>
<ul>
<li>原语词法分析</li>
<li>原语句法分析</li>
<li>原语译语词汇转换</li>
<li>原语译语机构转换</li>
<li>译语句法生成</li>
<li><p>译语词法生成</p>
</li>
<li><p>翻译软件 ARIANE-78</p>
</li>
</ul>
</li>
<li>斯坦福大学 Y.A.Wilks 提出了 “优选语义学”，强调无论在原语还是译语生成阶段都要把语义问题放在第一位。</li>
<li>1976 年，蒙特利尔大学与联邦政府翻译局：TAUM-METEO 系统，里程碑</li>
<li>1978 年，欧共体（欧盟）提出多语种机器翻译计划 EUROTRA，至今未取得预期效果</li>
<li>1982-1986 年，日本 Mu 系统；随后原定于 1987-1992 年完成实际延迟到 1995 年完成的日本多语言机器翻译 ODA 计划，实验效果不尽如人意</li>
<li>1987 年，TELECOM’87 会议，自动翻译电话通话试验</li>
<li>中科院 NLPR 与韩国 ETRI 合作进行了汉韩口语翻译实验</li>
<li>1991 年，成立国际语音翻译先进研究联盟（C-STAR）</li>
<li>2000 年，中科院 NLPR 成为该组织的核心成员之一，汉语成为 C-STAR 多语言语音翻译系统的主要语言之一</li>
</ul>
<p><strong>统计学方法在语音识别算法研制中取得成功</strong>：</p>
<ul>
<li>隐马尔科夫模型、噪声信道与解码模型<ul>
<li>Jelinek、Bahl、Mercer 和 IBM 华生研究中心</li>
<li>卡内基梅隆大学的 Baker</li>
<li>AT&amp;T 的贝尔实验室</li>
<li>科大讯飞</li>
</ul>
</li>
</ul>
<p><strong>逻辑方法在 NLP 中取得成绩</strong>：</p>
<ul>
<li>1970 年 A.Colmerauer 及同事研制的 Q 系统和变形文法</li>
<li>1980 年 Pereira 和 Warren 剔除的定子句文法</li>
<li>1979 年 M.Kay 对功能语法的研究，1982 年 Bresnan 和 Kaplan 在词汇功能语法方面的工作，都是特征结构合一方面的研究成果</li>
</ul>
<p><strong>自然语言理解也取得成绩</strong>：</p>
<ul>
<li><p>1972 年 Terry 研制的 SHRDLU 系统，能够模拟一个嵌入玩具积木世界的机器人的行为；该系统首次尝试建立基于 Halliday 系统语法的英语语法；该系统说明，句法剖析也应该重视语义和话语的形式模型的研究。</p>
</li>
<li><p>1977 年 R.Schank 与其在耶鲁大学的同事和学生建立了一些语言理解程序；他们使用基于网络的语义学理论，并在表达方式中引进 C.Fillmore 在 1968 年提出的关于<strong>格角色</strong>的概念。</p>
</li>
<li><p>自然语言理解中的逻辑方法：1967 年 Woods 研制的 LUNAR 问答系统使用谓词逻辑进行语义解释</p>
</li>
<li><p>话语分析四个关键领域：话语子结构的研究，话语焦点的研究，自动参照消解研究和基于逻辑的言语行为研究</p>
<ul>
<li>1977 年，Crosz 和同事研究了话语子结构和话语焦点</li>
<li>1972 年，Hobbs 开始研究自动参照消解</li>
<li>1980 年，Perrault 和 Allen 建立了 “信念-愿望-意图”（BDI）框架</li>
</ul>
</li>
</ul>
<p><strong>1983-1993 十年中：NLP 又回到了 20 世纪 50 年代末期 60 年代初期几乎被否定的有限状态模型和经验主义方法上</strong></p>
<ul>
<li><p>重新评价有限状态模型</p>
<ul>
<li>Kaplan 和 Key 在有限状态音系学和形态学方面的工作</li>
<li>Church 在句法的有限状态模型方面的工作</li>
</ul>
</li>
<li><p>重新回到经验主义</p>
<ul>
<li>语音和语言处理的概率模型</li>
<li>传播到连接主义方法的研究中</li>
</ul>
</li>
</ul>
<h3 id="繁荣期"><a href="#繁荣期" class="headerlink" title="繁荣期"></a>繁荣期</h3><p>1989 年，机器翻译进入新纪元：基于规则的技术中引入了语料库方法。</p>
<p>1994-1999 年以及 21 世纪初期，自然语言处理的研究出现了空前的繁荣：</p>
<ul>
<li>概率和数据驱动的方法几乎成了 NLP 标准方法</li>
<li>由于计算机速度和存储量增加，在语音和语言处理的一些子领域有可能进行商品开发</li>
<li>网络技术的发展对 NLP 产生了巨大推力</li>
</ul>
<h2 id="当前自然语言处理发展的几个特点"><a href="#当前自然语言处理发展的几个特点" class="headerlink" title="当前自然语言处理发展的几个特点"></a>当前自然语言处理发展的几个特点</h2><ul>
<li>基于句法-语义规则的理性主义方法受到质疑，随着语料库建设和语料库语言学的崛起，随着 Web 的日益普及，大规模真实文本的处理成为 NLP 的主要战略目标。<ul>
<li>基于规则的理性主义方法，哲学基础是逻辑实证主义：智能的基本单位是符号，认知过程就是在符号的表征下进行符号运算，因此思维就是符号运算。语言学家 J.A.Fodor 认为心理操作和图灵机的操作十分类似。</li>
<li>反驳与弱点<ul>
<li>塞尔 “中文屋子” 质疑，详见：<a href="http://yam.gift/2018/04/07/2018-04-07-AI-Philosophy-Note/#%E6%B1%89%E5%AD%97%E5%B1%8B%E5%AE%9E%E9%AA%8C" target="_blank" rel="noopener">人工智能哲学笔记 | Yam</a></li>
<li>实践方面，在处理大规模真实文本时有很大困难</li>
</ul>
</li>
</ul>
</li>
<li>NLP 中越来越多地使用机器学习的方法来获取语言知识。<ul>
<li>有监督</li>
<li>无监督</li>
<li>半监督</li>
</ul>
</li>
<li>统计数学方法越来越受到重视。<ul>
<li>估计语言成分出现的可能性，而不是单纯地判断这样的语言成分是否符合语言学规则（与传统的规则型语言模型对比）</li>
<li>语言统计模型已经相当成熟，如：隐马尔科夫模型、概率上下文无关语法、基于决策树的语言模型、最大熵语言模型、条件随机场等</li>
</ul>
</li>
<li>NLP 中越来越重视词汇的作用，出现了强烈的 “词汇主义” 倾向。<ul>
<li>Chomsky 提出的 “最简方案” 将所有重要的语法原则直接运用于表层，把具体的规则减少到最低限度，不同语言之间的差异由词汇来处理。</li>
<li>词汇知识库的构建成为普遍关注的问题。</li>
</ul>
</li>
</ul>
<p><strong>小结：</strong>主要介绍了自然语言处理的历史、研究对象、特点等，可以让我们对自然语言处理的横向、纵向有更加深刻的认识。书籍介绍的非常详细，作者熟知整个脉络，严谨又开放，熟悉各种技术且均能做出中立和中肯的判断及评价，可谓大师。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/09/19/NLPFA/2018-09-19-Ch01-Orientation-of-NLP/">
    <time datetime="2018-09-19T03:32:00.000Z" class="entry-date">
        2018-09-19
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Orientation/">Orientation</a></li></ul>

    </footer>
</article>





  
    <article id="post-NLPFA/2019-04-04-Ch16-Formal-Model-in-Statistical-Machine-Translation" class="post-NLPFA/2019-04-04-Ch16-Formal-Model-in-Statistical-Machine-Translation post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title">
      <a class="article-title" href="/2018/09/05/NLPFA/2019-04-04-Ch16-Formal-Model-in-Statistical-Machine-Translation/">自然语言计算机形式分析的理论与方法笔记(Ch16)</a>
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2018/09/05/NLPFA/2019-04-04-Ch16-Formal-Model-in-Statistical-Machine-Translation/" data-id="cju9u9zbw002l5occttsh5vqf" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第十六章：统计机器翻译中的形式模型"><a href="#第十六章：统计机器翻译中的形式模型" class="headerlink" title="第十六章：统计机器翻译中的形式模型"></a>第十六章：统计机器翻译中的形式模型</h1><p>基于语料库的机器翻译方法可分为两种：基于统计的机器翻译方法和基于实例的机器翻译方法。前者的知识表示是统计数据，后者语料库本身就是翻译知识的一种表现形式。</p>
<h2 id="机器翻译与噪声信道模型"><a href="#机器翻译与噪声信道模型" class="headerlink" title="机器翻译与噪声信道模型"></a>机器翻译与噪声信道模型</h2><p>1947 年，美国洛克菲勒基金会自然科学部主任 Weaver 提出使用解读密码的方法进行机器翻译，这实际上是一种统计的方法，需要大量的数学运算，在当时的技术条件下难以实现。</p>
<p>20 世纪 90 年代，在 Weaver 思想的基础上，IBM 的 Peter F. Brown 等提出了统计机器翻译的数学模型。</p>
<p>基于统计的机器翻译把机器翻译看成一个噪声信道问题：根据观察到的语言 T，恢复最为可能的语言 S。S 是信道意义上的源语言，在翻译意义上就是目标语言；T 在信道意义上是目标语言，在翻译意义上就是源语言。比如法译英翻译器，信道意义上看，S 为源语言法语，T 为目标语言英语；翻译意义上看，要翻译成的英语才是源语言。</p>
<script type="math/tex; mode=display">P(T|S) = \frac {P(T) P(S|T)}{P(S)}</script><p>即：<script type="math/tex">\hat{T} = arg max P(T) P(S|T)</script></p>
<p>三个问题：</p>
<ul>
<li>语言模型 P(T) 的参数估计：计算句子概率，N 元语法问题</li>
<li>翻译模型 P(S|T) 的参数估计：使用 EM 发现隐藏在两种语言结构后面的单词之间的对应关系，进行单词对齐</li>
<li>快速有效的搜索算法（解码器）：求解 T’，使得 P(T)P(T|S) 最大</li>
</ul>
<p>在目标语言句子 T 的长度为 l，源语言句子 S 的长度为 m 时，T 和 S 之间有 l×m 种对应关系。解码器搜索时，需要在所有的 <script type="math/tex">t_1^l</script> 中搜索使 <script type="math/tex">p(t_1^l) \times p(s_1^m|t_1^l)</script> 最大的结果（1 应该表示一对对齐的句子，显然有 l×m 中结果）。因此单词对齐是一个关键问题，引入隐变量 A 表示对齐：</p>
<script type="math/tex; mode=display">P(S|T) = \sum_A P(S, A|T)</script><p>假设源语言 <script type="math/tex">S = s_1^m = s_1^1s_1^2…s_1^m</script> 有 m 个单词，目标语言句子 <script type="math/tex">T = t_1^l = t_1^1t_1^2…t_1^l</script>，</p>
<p>对齐序列 <script type="math/tex">A=a_1^m = a_1^1a_1^2…a_1^m</script>，<script type="math/tex">a_1^j=(0,1,…,l), j=(1,2,…,m)</script>，如果源语言第 j 个单词与目标语言第 i 个单词对齐，则：<script type="math/tex">a_1^j = i</script>，如果没有单词与它对齐，则：<script type="math/tex">a_1^j = 0</script>，于是有：</p>
<script type="math/tex; mode=display">P(S, A|T) = P(m|T) \prod_{j=1}^m P(a_1^j|a_1^{j-1}, s_1^{j-1}, m, T)P(s_j|a_1^j, s_1^{j-1}, m, T)</script><p>首先根据已有的关于目标语言句子的知识，考虑源语言句子长度的概率；然后，再选择在给定目标语言句子和源语言句子长度情况下，目标语言句子中与源语言句子的第一个单词的位置以及对齐的概率；再考虑在给定目标语言句子和源语言句子长度，并且目标语言句子中与源语言句子的第一个单词对齐的那个位置的情况下，源语言句子第一个单词的概率。以此类推分别计算其他单词的概率。</p>
<p>对于翻译模型 P(F|E)（T 是英语 E，S 是法语 F），IBM 提出了五个复杂程度递增的数学模型：</p>
<ul>
<li>模型 1：只考虑词对词相互翻译的概率 <script type="math/tex">P(f_j|e_j)</script></li>
<li>模型 2：考虑翻译过程中单词位置的变化，引入参数 <script type="math/tex">P(a_j|j,m,l)</script>，其中 m 是源语言法语句子的长度，l 是目标语言英语句子的长度，j 是法语单词的位置，aj 是位置为 j 的法语单词对应的英语单词的位置</li>
<li>模型 3：考虑源语言中一个单词翻译为目标语言中的多个单词的概率，以及目标语言中的一个单词对应于源语言中的多个单词的概率</li>
<li>模型 4：不仅考虑在对齐时单词位置的变化，而且还考虑在该位置上单词类别的差异，建立了一个基于类的模型，自动把源语言和目标语言的单词划分到 50 个不同的类别中。U. Germann 基于该模型提出了贪心爬山解码算法，这种算法不是通过在每一时刻处理一个输入单词的方法最终建立一个优化完整的翻译假设，而是直接从输入句子的一个完整可能的翻译结果开始，不断地调整源语言（法语）单词和目标语言（英语）单词之间的对应关系，逐步得到最优结果。</li>
<li>模型 5：修正了模型 4 的一些缺陷，避免对于一些不可能出现的对齐给出非零的概率</li>
</ul>
<p>这些模型的主要区别在于它们在计算源语言单词与目标语言单词之间连接的概率的方式不同。</p>
<p>IBM 的英法机器翻译系统：Candide 分为分析——转换——生成三个阶段，中间表示是线性的，分析和生成都是可逆的。</p>
<ul>
<li>分析阶段，需要对于输入的法语文本预处理</li>
<li>转换阶段，使用基于统计的方法解码，又分为两个阶段：<ul>
<li>第一阶段使用粗糙模型的堆栈搜索，输出 140 个评分最高的译文，语言模型为三元语法，翻译模型使用 EM</li>
<li>第二阶段使用精细模型的扰动搜索，对第一阶段的结果先扩充再重新评分，语言模型采用链语法，翻译模型采用最大熵模型</li>
</ul>
</li>
</ul>
<p>1999 年 JHU 夏季研讨班的 EGYPT 包括四个模块：</p>
<ul>
<li>GIZA++：语料库工具，用于从双语并行语料库中抽取统计知识，进行参数训练</li>
<li>Decoder：解码器，执行翻译过程</li>
<li>Cairo：可视化界面</li>
<li>Whittle：语料库预处理工具</li>
</ul>
<p>CMU 王野翊 和 Alex Waible 提出基于结构的对齐模型改进 IBM 模型：</p>
<ul>
<li>德语英语语法结构差异较大，训练数据有限，有严重的数据稀疏问题</li>
<li>使用两个层次对齐模型，一个是短语之间的粗对齐模型，一个是短语之内单词的细对齐模型</li>
<li>粗对齐过程中引入了一种短语的语法推导算法，在语料库基础上，通过基于互信息的双语词语聚类和短语归并反复迭代，得到一组基于词语聚类的短语规则，再用这些规则进行句子的短语分析</li>
</ul>
<h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h2><p>统计机器翻译需要训练两个不同的知识源：一个是语言模型 P(T)，一个是翻译模型 P(S|T)，比如法译英翻译器中，要把源语言法语翻译成英语的句子的时候，使用如下模型训练：</p>
<script type="math/tex; mode=display">\hat e_1^l = \arg\max\ P(e_1^l)P(f_1^j|e_1^l)​</script><p>其中，<script type="math/tex">f_1^j = f_1,…,f_j</script> 表示法语句子，<script type="math/tex">e_1^l = e_1,…,e_l</script> 表示英语句子。</p>
<p>F. J. Och 在 1999 年指出可以通过以下式子得到一样好的结果：</p>
<script type="math/tex; mode=display">\hat e_1^l = \arg\max\ P(e_1^l)P(e_1^l|f_1^j)</script><p>这用噪声信道模型是解释不通的，因为法语被看成是目标语言英语在噪声信道中受到干扰形成的，我们首先要做的是从英语的角度出发，来训练把受到干扰形成的法语翻译为英语的统计参数。因此可以不拘泥于该模型，使用直接翻译模型替代：</p>
<p>假设 e 和 f 是目标语言和源语言的句子，<script type="math/tex">h_1(e,f),…h_m(e,f)</script> 是 e 和 f 上的 M 个特征函数，λ1…λm 是与这些特征函数分别对应的 M 个模型参数，直接翻译概率可以用下面的公式模拟：</p>
<script type="math/tex; mode=display">P(e|f) \approx p_{\lambda_1…\lambda_m} (e|f)$$ = 

$$\exp[\sum_{m=1}^M \lambda_m h_m(e,f)] \sum_{e'} \exp[\sum_{m=1}^M \lambda_m h_m(e',f)]​</script><p>对于给定法语句子 f，最佳的英语译文 e 可用下式表示：</p>
<script type="math/tex; mode=display">\hat e = \arg\max_e P(e|f) = \arg\max_e \sum_{m=1}^M \lambda_m h_m (e,f)</script><p>只要不断调整特征函数 h 和模型参数 λ，进行参数训练和全局搜索，获得最合适的参数值，从而得到：</p>
<script type="math/tex; mode=display">\arg\max_e \sum_{m=1}^M \lambda_m h_m (e, f)</script><p>参数训练可以使用最大后验概率标准作为训练标准，最大后验概率标准也就是最大互信息标准（MMI），它使得系统的熵最大，所以模型叫作 ”最大熵模型“（ME）。</p>
<p>参数训练问题也就是如何获得模型参数 <script type="math/tex">\lambda_1^M</script> 的问题：</p>
<script type="math/tex; mode=display">\lambda_1^M = \arg \max \sum_{s=1}^S \log_2 P_{\lambda_1}^M (e_s|f_s)</script><p>最大熵模型直接使用统计翻译模型，是一种比噪声信道模型更具有一般性的模型，噪声信道模型是最大熵模型的一个特例。</p>
<blockquote>
<p>最大熵模型的基本原理是：在只掌握关于未知分布的部分信息的情况下，符合已知知识的概率分布可能有多个，但使熵值最大的概率分布最真实地反映了事件的分布情况，因为熵定义了随机变量的不确定性，熵最大时，随机变量最不确定，最难准确预测其行为。也就是说，在已知部分信息的前提下，关于未知分布最合理的推断应该是符合已知信息最不确定或最大随机的推断。——宗成庆《统计自然语言处理》</p>
</blockquote>
<h2 id="基于平行概率语法的形式模型"><a href="#基于平行概率语法的形式模型" class="headerlink" title="基于平行概率语法的形式模型"></a>基于平行概率语法的形式模型</h2><p>基本思想是在源语言和目标语言之间建立一套平行的语法规则，规则一一对应，两套规则服从同样的概率分布。这样源语言的句法分析和目标语言的生成是同步进行的，分析的过程决定了生成的过程。主要有 H. Alshawi 的基于 “中心词转录机” 的模型、同步上下文无关语法（Synchronous Context-Free Grammar，SCFG）模型、吴德恺的 “反向转录语法（Inverse Transduction Grammar，ITG）” 模型。</p>
<h3 id="中心词转录机"><a href="#中心词转录机" class="headerlink" title="中心词转录机"></a>中心词转录机</h3><p>一种有限状态转录机，定义为：<code>&lt;q, q&#39;, w, v, α, β, c&gt;</code>，其中 q 是输入状态，q’ 是输出状态，w 是输入符号，v 是输出符号，α 是输入位置，β 是输出位置，c 是转录的权重或者代价。</p>
<p>与一般有限状态转录机的区别是：</p>
<ul>
<li>中心词转录机每条边上不仅有输入，也有输出</li>
<li>中心转录机不是从左至右输入，而是以中心词为坐标往两边输入</li>
</ul>
<p>中心词转录机可以把源语言的一个中心词以及它左右侧依存的单词构成的序列转换成目标语言的单词 v 及它左右侧依存的单词构成的另一个序列。转录时权重的参数需要根据语料库训练。</p>
<p>中心词转录机对齐的结果是依存树，依存树中所有结点都是具体的单词，不使用词性标记和短语标记。</p>
<p>使用中心词转录机进行统计翻译时，所有的语言知识（词典、规则）都表现为中心词转录机。可以嵌套。</p>
<p>参数使用统计方法训练，根据依存树库中的数据获取统计规则。</p>
<h3 id="同步上下文无关语法（SCFG）"><a href="#同步上下文无关语法（SCFG）" class="headerlink" title="同步上下文无关语法（SCFG）"></a>同步上下文无关语法（SCFG）</h3><p>由两个上下文无关语法 G1 和 G2 构成，G1 表示源语言的上下文无关语法，G2 表示目标语言的上下文无关语法，G1 和 G2 中所有规则彼此对应，在句法剖析时同步进行。在对源语言分析时，同步生成了目标语言的句子。</p>
<p>规则左部是非终极符号，右部是用尖括号表示的符号串，逗号前的符号串表示源语言中的重写结果，逗号后的符号串表示目标语言中相应的重写结果。右部可以由非终极符号组成，下标数字表示该符号在符号串中的顺序。右部也可以是具体的单词，表示源语言与目标语言单词的对应关系。</p>
<p>规则使用统计方法，通过训练双语树库获取。</p>
<h3 id="反向转录语法（ITG）"><a href="#反向转录语法（ITG）" class="headerlink" title="反向转录语法（ITG）"></a>反向转录语法（ITG）</h3><p>引入了反向产生式，对源语言和目标语言句子进行同步处理，ITG 可以定义为一个五元组：</p>
<p>G = (N, W1, W2, R, S)</p>
<p>G 表示反向转录语法，N 是非终极符号的有限集合，W1 是语言 1 的终极符号（单词）有限集合，W2 是语言 2 的，R 是重写规则有限集合，S∈N 是初始符号。</p>
<p>两种语言的终极符号偶对（单词偶对）<script type="math/tex">X = (W_1 \cup \{\epsilon\}) \times (W_2 \cup \{\epsilon\})</script> 之间含有一个表示翻译的符号，表示为：x/y, x/ε, ε/y 的形式，x ∈ W1，y ∈ W2，x/y 表示语言 1 中的单词 x 在语言 2 中翻译为 y，x/ε 表示 x 没有对应的翻译，ε/y 表示某个空位置被翻译成 y，这时在语言 2 需要插入一个单词（一般是虚词）。后两种在 ITG 中被叫作 “单身汉”。</p>
<p>每一个产生式的右部生成的符号都可以有两个方向：一个是正常的顺向，从左到右连接，表示为：<code>A → [a1,a2…ar]</code>；一个是反向，从右到左连接，表示为：<code>A → &lt;a1, a2…ar&gt;</code>。ITG 转录结果记为 T(G)，语言 1 的字符串集合记为 L1(G)，语言 2 的记为 L2(G)。</p>
<p>对于任何一个反向转录语法 G，存在着一个等价的反向转录语法 G‘，可以把一个反向转录语法改成同步上下文无关语法。</p>
<p>反向转录语法句法分析过程中，输入的是两种语言的句子偶对，而不是单个句子，句法分析的过程就是为输入的句子偶对建立彼此匹配的结构成分的过程。</p>
<p>吴德恺还提出随机反向转录语法（Stochastic ITG，SITG），每一条重写规则都有一个概率。重写规则概率需要在双语并行树库中用统计方法获取。</p>
<h2 id="基于短语的统计机器翻译"><a href="#基于短语的统计机器翻译" class="headerlink" title="基于短语的统计机器翻译"></a>基于短语的统计机器翻译</h2><p>前面的翻译模型是建立在单词基础上的，可以叫作基于单词的统计机器翻译模型（Word-Based SMT，WBSMT），有以下不足：</p>
<ul>
<li>当源语言中多个单词对应目标语言一个单词时（多对一），无法处理</li>
<li>无法处理源语言中的固定短语</li>
</ul>
<p>因此有必要建立基于短语的统计机器翻译系统（Phrase-Based SMT，PBSMT）。源语言的句子首先切分为短语和单词的组合，然后根据从双语语料库中获取短语翻译的知识，把每一个源语言短语翻译成目标语言短语的可能性用概率表示。好处是：</p>
<ul>
<li>可以实现双语多对多映射（多对多时可以当作短语处理）</li>
<li>利用短语的局部上下文排歧</li>
</ul>
<p>实践证明，基于短语的技术可以改善统计机器翻译的质量，但<strong>当短语的长度扩大到 3 个以上的单词时</strong>（可以理解为 Bi-gram 和 Tri-gram），翻译系统性能就很难得到提高。</p>
<p>David Chiang 提出基于层次短语的统计翻译模型，基本思想是：在不干预基于短语的机器翻译方法的前提下，第一遍调整短语内部单词的顺序，第二遍调整短语之间的顺序。短语由单词和子短语构成，这样短语内就出现了子短语这个层次，这种基于层次短语的翻译模型在形式上是一个同步的上下文无关语法（SCFG），这种句法是从没有任何句法信息标注的双语语料库中通过机器学习获得的。</p>
<p>这种模型主要依靠源语言和目标语言的短语对应表进行翻译，短语对应表要通过双语并行语料库进行抽取，而关键的问题是要进行 “短语对齐”。Och 提出了建造短语 “对齐模板” 的方法。</p>
<p>首先实现子短语的对齐，然后再实现整个短语对齐。注意保持两种语言的短语中所包含的单词的一致性。短语对齐建立在单词对齐的基础上。</p>
<ul>
<li>对可能产生的很多对齐偶对，可以使用高频词过滤掉多余的短语偶对。</li>
<li>如果一个源语言短语对应于目标语言中若干个短语，就会出现对齐歧义，可以根据上下文排歧。</li>
</ul>
<p>进行基于短语的机器翻译时，先把源语言句子切分成短语串，然后按照短语对应表映射，最后对映射后的目标语言短语串进行排序，得到目标语言的输出。短语中包含了局部的单词选择和局部顺序以及很多习惯表达和搭配信息，这是基于单词的统计机器翻译不具备的。</p>
<h2 id="基于句法的统计机器翻译"><a href="#基于句法的统计机器翻译" class="headerlink" title="基于句法的统计机器翻译"></a>基于句法的统计机器翻译</h2><p>基于短语的机器翻译没考虑短语与短语之间的关系，因此难以处理短语之间重新排序的问题，另外对于短语之间的长距离依存关系也难以处理。</p>
<p>2001 年 Yamada 和 Knight 提出了基于句法的统计机器翻译（syntax-based SMT，SBSMT），输入源语言的句法树，输出目标语言的句子。</p>
<ul>
<li>调序：输入树形图的每个子树要根据概率重新排列，进行顺序调整<ul>
<li>根据双语并行语料库中两种语言调序关系的概率</li>
<li>需要建立调序表：记录着调序规则的概率</li>
</ul>
</li>
<li>插入：在子树结点的左边或右边随机插入恰当的功能词，左插、右插或不插的概率取决于父结点和当前结点的标记，概率只与该单词有关，与位置无关<ul>
<li>根据目标语言语法规则插入</li>
<li>需要建立结点表：记录非终极符号插入树形图中有关结点的概率，还要考虑功能词本身的插入概率</li>
<li>整个树的插入概率等于 “插入结点位置概率的乘积” 与 “插入功能词概率的乘积” 相乘</li>
</ul>
</li>
<li>翻译：根据词对词的翻译概率把途中每一个叶子结点上的单词翻译为目标语言的相应单词<ul>
<li>需要建立翻译表：记录源语言单词翻译为目标语言单词的概率</li>
<li>单词的翻译概率等于源语言句子中单词翻译为目标语言的单词的翻译概率的乘积</li>
</ul>
</li>
<li>输出：输出译文句子<ul>
<li>计算 “调序-插入-翻译” 联合概率：三者概率的乘积</li>
</ul>
</li>
</ul>
<p>使用期望最大算法进行概率估计：</p>
<ul>
<li>初始化所有概率表（通常设置为均匀分布）</li>
<li>清楚每种操作的计数器</li>
<li>对每一个源语言句法树和目标语言句子的偶对执行：<ul>
<li>计算调序、插入、翻译三中操作每种可能的组合方式的概率</li>
<li>把每种操作的概率分别加到相应的计数器</li>
</ul>
</li>
<li>根据计数器的值重新计算三张概率表的概率值</li>
<li>重复上述三步直至收敛</li>
</ul>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>机器翻译与噪声信道模型<ul>
<li>1947 年，美国洛克菲勒基金会自然科学部主任 Weaver 提出使用解读密码的方法进行机器翻译。20 世纪 90 年代，在 Weaver 思想的基础上，IBM 的 Peter F. Brown 等提出了统计机器翻译的数学模型。</li>
<li>基于统计的机器翻译把机器翻译看成一个噪声信道问题：根据观察到的语言 T，恢复最为可能的语言 S。S 是信道意义上的源语言，在翻译意义上就是目标语言；T 在信道意义上是目标语言，在翻译意义上就是源语言。</li>
<li>三个问题：<ul>
<li>语言模型的参数估计：计算句子概率，N 元语法问题</li>
<li>翻译模型的参数估计：使用 EM 发现隐藏在两种语言结构后面的单词之间的对应关系，进行单词对齐</li>
<li>快速有效的搜索算法（解码器）</li>
</ul>
</li>
<li>IBM 提出了五个复杂程度递增的数学模型，这些模型的主要区别在于它们在计算源语言单词与目标语言单词之间连接的概率的方式不同。</li>
<li>IBM 的英法机器翻译系统：Candide 分为分析——转换——生成三个阶段，中间表示是线性的，分析和生成都是可逆的。</li>
<li>1999 年 JHU 夏季研讨班的 EGYPT 包括四个模块：GIZA++, Decoder, Cairo 和 Whittle。</li>
<li>CMU 王野翊 和 Alex Waible 提出基于结构的对齐模型改进 IBM 模型，使用两个层次对齐模型，一个是短语之间的粗对齐模型，一个是短语之内单词的细对齐模型。</li>
</ul>
</li>
<li>最大熵模型<ul>
<li>最大熵模型直接使用统计翻译模型，是一种比噪声信道模型更具有一般性的模型，噪声信道模型是最大熵模型的一个特例。</li>
</ul>
</li>
<li><p>基于平行概率语法的形式模型</p>
<ul>
<li>基本思想是在源语言和目标语言之间建立一套平行的语法规则，规则一一对应，两套规则服从同样的概率分布。这样源语言的句法分析和目标语言的生成是同步进行的，分析的过程决定了生成的过程。</li>
<li>H. Alshawi 的基于 “中心词转录机” 的模型<ul>
<li>与一般有限状态转录机的区别是：中心词转录机每条边上不仅有输入，也有输出；中心转录机不是从左至右输入，而是以中心词为坐标往两边输入。</li>
<li>中心词转录机可以把源语言的一个中心词以及它左右侧依存的单词构成的序列转换成目标语言的单词 v 及它左右侧依存的单词构成的另一个序列。转录时权重的参数需要根据语料库训练。</li>
<li>中心词转录机对齐的结果是依存树，依存树中所有结点都是具体的单词，不使用词性标记和短语标记。</li>
<li>使用中心词转录机进行统计翻译时，所有的语言知识（词典、规则）都表现为中心词转录机。可以嵌套。</li>
<li>参数使用统计方法训练，根据依存树库中的数据获取统计规则。</li>
</ul>
</li>
<li>同步上下文无关语法（Synchronous Context-Free Grammar，SCFG）模型<ul>
<li>由两个上下文无关语法 G1 和 G2 构成，G1 表示源语言的上下文无关语法，G2 表示目标语言的上下文无关语法，G1 和 G2 中所有规则彼此对应，在句法剖析时同步进行。在对源语言分析时，同步生成了目标语言的句子。</li>
<li>规则使用统计方法，通过训练双语树库获取。</li>
</ul>
</li>
<li>吴德恺的 “反向转录语法（Inverse Transduction Grammar，ITG）” 模型<ul>
<li>引入了反向产生式，对源语言和目标语言句子进行同步处理。</li>
<li>每一个产生式的右部生成的符号都可以有两个方向：一个是正常的顺向，从左到右连接；一个是反向，从右到左连接。</li>
<li>对于任何一个反向转录语法 G，存在着一个等价的反向转录语法 G‘，可以把一个反向转录语法改成同步上下文无关语法。</li>
<li>反向转录语法句法分析过程中，输入的是两种语言的句子偶对，而不是单个句子，句法分析的过程就是为输入的句子偶对建立彼此匹配的结构成分的过程。</li>
</ul>
</li>
</ul>
</li>
<li><p>基于短语的统计机器翻译</p>
<ul>
<li>基于单词的统计机器翻译模型无法处理多对一和固定短语问题，David Chiang 提出基于层次短语的统计翻译模型，基本思想是：在不干预基于短语的机器翻译方法的前提下，第一遍调整短语内部单词的顺序，第二遍调整短语之间的顺序。</li>
<li>这种模型主要依靠源语言和目标语言的短语对应表进行翻译，短语对应表要通过双语并行语料库进行抽取，而关键的问题是要进行 “短语对齐”。Och 提出了建造短语 “对齐模板” 的方法：首先实现子短语的对齐，然后再实现整个短语对齐。注意保持两种语言的短语中所包含的单词的一致性。短语对齐建立在单词对齐的基础上。</li>
</ul>
</li>
<li>基于句法的统计机器翻译<ul>
<li>基于短语的机器翻译没考虑短语与短语之间的关系，因此难以处理短语之间重新排序的问题，另外对于短语之间的长距离依存关系也难以处理。2001 年 Yamada 和 Knight 提出了基于句法的统计机器翻译（syntax-based SMT，SBSMT），输入源语言的句法树，输出目标语言的句子。</li>
<li>步骤如下：<ul>
<li>调序：输入树形图的每个子树要根据概率重新排列，进行顺序调整</li>
<li>插入：在子树结点的左边或右边随机插入恰当的功能词，左插、右插或不插的概率取决于父结点和当前结点的标记，概率只与该单词有关，与位置无关</li>
<li>翻译：根据词对词的翻译概率把途中每一个叶子结点上的单词翻译为目标语言的相应单词</li>
<li>输出：输出译文句子</li>
</ul>
</li>
<li>使用期望最大算法进行概率估计。</li>
</ul>
</li>
</ul>
<p>本章中的内容现在看来已经有点过时了，事实上本书大部分内容都是自然语言处理的形式模型，这在这个深度学习满天飞的时代多少显得有些另类。但其中的一些思想并不过时，能给人不少启发，比如基于短语的翻译；而且涉及到的一些模型和算法也是现在常用的，比如 Viterbi，前向后向算法，beamsearch 等等。不过对于最新的一些技术我觉得更加需要了解掌握，正好收集整理了一些，罗列如下：</p>
<ul>
<li><a href="https://github.com/Microsoft/NPMT" target="_blank" rel="noopener">Microsoft/NPMT: Towards Neural Phrase-based Machine Translation</a> Microsoft 基于短语的翻译系统</li>
<li><a href="https://github.com/tensorflow/nmt" target="_blank" rel="noopener">tensorflow/nmt: TensorFlow Neural Machine Translation Tutorial</a> Google 基于 Seq2Seq 和 Attention 的翻译系统</li>
<li><a href="https://github.com/tensorflow/models/tree/master/official/transformer" target="_blank" rel="noopener">models/official/transformer at master · tensorflow/models</a> Google 基于纯 Attention 的翻译模型</li>
<li><a href="https://github.com/pytorch/fairseq" target="_blank" rel="noopener">pytorch/fairseq: Facebook AI Research Sequence-to-Sequence Toolkit written in Python.</a> Facebook 基于 CNN 的翻译系统</li>
<li><a href="https://github.com/facebookresearch/UnsupervisedMT" target="_blank" rel="noopener">facebookresearch/UnsupervisedMT: Phrase-Based &amp; Neural Unsupervised Machine Translation</a> Facebook 基于无监督的翻译系统</li>
<li><p><a href="https://www.deepl.com/translator" target="_blank" rel="noopener">DeepL Translator</a> DeepL 基于 CNN 的翻译工具</p>
</li>
<li><p><a href="https://github.com/OpenNMT/OpenNMT" target="_blank" rel="noopener">OpenNMT/OpenNMT: Open Source Neural Machine Translation</a></p>
</li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2018/09/05/NLPFA/2019-04-04-Ch16-Formal-Model-in-Statistical-Machine-Translation/">
    <time datetime="2018-09-05T01:53:41.562Z" class="entry-date">
        2018-09-05
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Model/">Formal Model</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>





  
  
    <nav id="pagination">
      <nav id="page-nav">
        <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
      </nav>
    </nav>
  
</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">25</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">8</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/">自然语言计算机形式分析的理论与方法笔记(Ch18)</a>
          </li>
        
          <li>
            <a href="/2019/03/31/2019-03-31-Nabokov-Favorite-Word/">《纳博科夫最喜欢的词》读书笔记与思考</a>
          </li>
        
          <li>
            <a href="/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/">自然语言计算机形式分析的理论与方法笔记(Ch15)</a>
          </li>
        
          <li>
            <a href="/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/">自然语言计算机形式分析的理论与方法笔记(Ch14)</a>
          </li>
        
          <li>
            <a href="/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/">自然语言计算机形式分析的理论与方法笔记(Ch13)</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a><span class="tag-list-count">28</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backward/">Backward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Business/">Business</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calculus/">Calculus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DataClearing/">DataClearing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DataScience/">DataScience</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/">DeepLearning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependence/">Dependence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Diary/">Diary</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Economics/">Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/">Entropy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Evaluation/">Evaluation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FSM/">FSM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Model/">Formal Model</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forward/">Forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gan/">Gan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HMM/">HMM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Industry/">Industry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalism/">Lexicalism</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LinearAlgebra/">LinearAlgebra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine/">Machine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MachineLearning/">MachineLearning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markov/">Markov</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">23</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ngram/">Ngram</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Orientation/">Orientation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ReinforcementLearning/">ReinforcementLearning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simon/">Simon</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SimpsonParadox/">SimpsonParadox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smoothing/">Smoothing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spell-Check/">Spell Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Style/">Style</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/">System</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valence/">Valence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VirtualBox/">VirtualBox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Viterbi/">Viterbi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZhouZhihua/">ZhouZhihua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zipf/">Zipf</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Business/" style="font-size: 10px;">Business</a> <a href="/tags/C/" style="font-size: 11.67px;">C</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/DataClearing/" style="font-size: 10px;">DataClearing</a> <a href="/tags/DataScience/" style="font-size: 13.33px;">DataScience</a> <a href="/tags/DeepLearning/" style="font-size: 13.33px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 10px;">Diary</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Entropy/" style="font-size: 10px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10px;">Evaluation</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Formal-Model/" style="font-size: 16.67px;">Formal Model</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/LinearAlgebra/" style="font-size: 10px;">LinearAlgebra</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/MachineLearning/" style="font-size: 10px;">MachineLearning</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Math/" style="font-size: 11.67px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/NLP/" style="font-size: 18.33px;">NLP</a> <a href="/tags/Ngram/" style="font-size: 10px;">Ngram</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Philosophy/" style="font-size: 11.67px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/ReinforcementLearning/" style="font-size: 10px;">ReinforcementLearning</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/SimpsonParadox/" style="font-size: 10px;">SimpsonParadox</a> <a href="/tags/Smoothing/" style="font-size: 10px;">Smoothing</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/System/" style="font-size: 11.67px;">System</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10px;">Viterbi</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2019 Yam
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>