<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>Vector Semantics Note (SLP Ch06) | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis.">
<meta name="keywords" content="NLP,AI,Embeddings,Lexical Semantics,Vector Semantics,Word2vec,Cosine,PPMI,TF-IDF">
<meta property="og:type" content="article">
<meta property="og:title" content="Vector Semantics Note (SLP Ch06)">
<meta property="og:url" content="http://www.yam.gift/2019/05/16/SLP/2019-05-16-Vector-Semantics/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis.">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-06-18T03:29:02.014Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Vector Semantics Note (SLP Ch06)">
<meta name="twitter:description" content="Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis.">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="http://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-SLP/2019-05-16-Vector-Semantics" class="post-SLP/2019-05-16-Vector-Semantics post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      Vector Semantics Note (SLP Ch06)
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/05/16/SLP/2019-05-16-Vector-Semantics/" data-id="cjxvnz7xd0076y1ccd5fz4wor" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the <strong>distributional hypothesis</strong>.</p>
<ul>
<li>words which are synonyms tended to occur in the same environment</li>
<li>with the amount of meaning difference between two words “corresponding roughly to the amount of difference in their environments”</li>
</ul>
<p><strong>vector semantics</strong> instantiates this linguistic hypothesis by learning representations of the meaning of words directly from their distributions in texts.</p>
<a id="more"></a>
<h2 id="Lexical-Semantics"><a href="#Lexical-Semantics" class="headerlink" title="Lexical Semantics"></a>Lexical Semantics</h2><p><strong>Lexical semantics</strong> is the linguistic study of word meaning.</p>
<ul>
<li>Lemmas and Senses<ul>
<li>A <strong>lemma</strong> always have different <strong>wordforms</strong> with different meanings.</li>
<li>Each meaning is a <strong>word sense</strong>, lemmas can be <strong>homonymous</strong>.</li>
</ul>
</li>
<li>Relationships between words or senses<ul>
<li>An important component of word meaning</li>
<li>Two words are <strong>synonyms</strong> if they are substitutable in any sentence, they have the same <strong>propositional meaning</strong>.</li>
<li>No two words are absolutely identical in meaning. One of the fundamental tenets of semantics called the <strong>principle of contrast</strong>, is the assumption that a difference in linguistic form is always associated with at least some difference in meaning.</li>
<li>In practice, synonym is commonly used to describe a relationship of approximate or rough synonymy.</li>
<li><strong>Antonyms</strong> are words with an opposite meaning.</li>
</ul>
</li>
<li>Word Similarity<ul>
<li>Most words have lots of similar words. cat is not a synonym of dog, but are similar words.</li>
<li>Useful in question answering, paraphrasing, and summarization.</li>
</ul>
</li>
<li>Word Relatedness<ul>
<li>The meaning of two words are related in ways other than similarity is called <strong>relatedness</strong>, also called <strong>association</strong> in psychology. coffee and cup are related.</li>
<li>One common relatedness is if they belong to the same <strong>semantic field</strong>.</li>
<li>Semantic fields are also related to <strong>topic models</strong>, like <strong>Latent Dirichlet Allocation, LDA</strong>.</li>
<li>Useful tool for discovering topical structure in documents.</li>
</ul>
</li>
<li>Semantic Frames and Roles<ul>
<li>Closely related to semantic fields is the idea of a <strong>semantic frame</strong>.</li>
<li>A semantic frame is a set of words that denote perspectives or participants in a particular type of event. </li>
<li>Important for question answering, and can help in shifting perspective for machine translation.</li>
</ul>
</li>
<li>Taxonomic Relations<ul>
<li>A word (or sense) is a <strong>hyponym</strong> of another word or sense if the first is more specific, denoting a subclass of the other.</li>
<li>Conversely is <strong>hypernym</strong>, often use <strong>superordinate</strong> instead.</li>
<li>We can define hypernymy more formally by saying that the class denoted by the superordinate extensionally includes the class denoted by the hyponym.</li>
<li>Hypernymy can also be defined in terms of entailment. A is a hyponym of B if everything that is A is also B. It is also the <strong>IS-A</strong> hierarchy, A IS-A B, or B <strong>subsumes</strong> A.</li>
<li>Useful for textual entailment or question answering.</li>
</ul>
</li>
<li>Connotation<ul>
<li>Words have affective meanings or <strong>connotations</strong>.</li>
<li>Positive or negative evaluation expressed through language is called <strong>sentiment</strong>.</li>
<li>Words varied along three important dimensions of affective meaning<ul>
<li><strong>valence</strong>: the pleasantness of the stimulus</li>
<li><strong>arousal</strong>: the intensity of emotion provoked by the stimulus</li>
<li><strong>dominance</strong>: the degree of control exerted by the stimulus</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Vector-Semantics"><a href="#Vector-Semantics" class="headerlink" title="Vector Semantics"></a>Vector Semantics</h2><p><strong>Vector semantics</strong> is the current best model to deal with all the aspects of word meaning, based on Ludwig Wittgenstein’s “the meaning of a word is its use in the language”, that is defining a word by the environment or distribution it occurs in language use. A word’s distribution is the set of contexts in which it occurs, the neighboring words or grammatical environments.</p>
<p>Vector semantics combines two intuitions:</p>
<ul>
<li>the distributionalist intuition: defining a word by counting what other words occur in its envrionment</li>
<li>the vector intuition: defining the meaning of a word w as a vector</li>
</ul>
<p>Vectors for representing words are generally called <strong>embeddings</strong>.</p>
<h2 id="Words-and-Vectors"><a href="#Words-and-Vectors" class="headerlink" title="Words and Vectors"></a>Words and Vectors</h2><p>Vector or distributional models of meaning are generally based on a <strong>co-occurrence matrix</strong>, a way of representing how often words co-occur.</p>
<h3 id="Vectors-and-documents"><a href="#Vectors-and-documents" class="headerlink" title="Vectors and documents"></a>Vectors and documents</h3><p>In a <strong>term-document matrix</strong>, each row represents a word in the vocabulary and each column represents a document from some collection of documents. The matrix was first defined as part of the vector space model of information retrieval.</p>
<p>A <strong>vector space</strong> is a collection of vectors, characterized by their <strong>dimension</strong>.</p>
<p>Term-document matrices were originally defined as a means of finding similar documents for the task of document <strong>information retrieval</strong>.</p>
<p><strong>Information retrieval (IR)</strong> is the task of finding the document d from the D documents in some collection that best matches a query q.</p>
<h3 id="Words-and-vectors"><a href="#Words-and-vectors" class="headerlink" title="Words and vectors"></a>Words and vectors</h3><p>The word vector is a row vector. </p>
<p>For documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents.</p>
<p>However, it is most common to use the <strong>word-word matrix</strong> or the <strong>term-context matrix</strong>, in which the columns are labeled by words rather than documents.</p>
<p>This matrix is thus of dimensionality |V| × |V| and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus. It is most common to use smaller contexts, generally a window around the word.</p>
<h2 id="Cosine-for-measuring-similarity"><a href="#Cosine-for-measuring-similarity" class="headerlink" title="Cosine for measuring similarity"></a>Cosine for measuring similarity</h2><p>It is based on the <strong>dot product</strong> operator from linear algebra, also called the <strong>inner product</strong>.</p>
<p>The raw dot-product has a problem as a similarity metric: it favors <strong>long</strong> vectors. Most frequent words have long vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them.</p>
<p>The simplest way to modify the dot product to normalize for the vector length is to divide the dot product by the lengths of each of the two vectors. This is <strong>normalized dot product</strong>.</p>
<script type="math/tex; mode=display">cosine(v,w) = \frac {v·w}{|v||w|} = \frac {\sum_{i=1}^N v_iw_i} {\sqrt{\sum_{i=1}^N v_i^2} \sqrt{\sum_{i=1}^N w_i^2}}</script><p>For some applications we pre-normalize each vector, by dividing it by its length, creating a <strong>unit vector</strong> of length 1. Thus we could use <code>v·w</code> to compute.</p>
<h2 id="TF-IDF-Weighing-terms-in-the-vector"><a href="#TF-IDF-Weighing-terms-in-the-vector" class="headerlink" title="TF-IDF: Weighing terms in the vector"></a>TF-IDF: Weighing terms in the vector</h2><p>Raw frequency is very skewed and not very discriminative.</p>
<p>It’s a bit of a paradox. Word that occur nearby frequently are more important than words that only appear<br>once or twice. Yet words that are too frequent—ubiquitous, like the or good— are unimportant.</p>
<p>The <strong>tf-idf algorithm</strong> algorithm is the product of two terms, each term capturing one of these two intuitions:</p>
<ul>
<li><strong>term frequency</strong>: frequency of the word in the document. $tf_{t,d} = 1 + \log_{10} count(t,d) ; 0$</li>
<li><strong>document frequency</strong>: is simply the number of documents a term occurs in. It give a higher weight to words that occur only in a few documents. <ul>
<li>the <strong>collection frequency</strong> of a term is the total number of times the word appears in the whole collection in any document.</li>
<li>the <strong>inverse document frequency</strong> or <strong>idf</strong> is <code>N/df_t</code> where N is the total number of documents in the collection, and dft is the number of documents in which term t occurs. A document is a play or a wiki page or a single article. $idf_t = \log_{10}(\frac {N}{df_t})$ </li>
</ul>
</li>
</ul>
<p>The <strong>tf-idf</strong> weighting of the value for word t in document d: $w_{t, d} = tf_{t,d} × idf_t$</p>
<h2 id="Applications-of-the-tf-idf-vector-model"><a href="#Applications-of-the-tf-idf-vector-model" class="headerlink" title="Applications of the tf-idf vector model"></a>Applications of the tf-idf vector model</h2><ul>
<li>compute word similarity</li>
<li>compute document similarity<ul>
<li>compute the <strong>centroid</strong> of all words (vectors) in the document as the <strong>document vector</strong></li>
<li><script type="math/tex; mode=display">d = \frac {w_1+w_2+…+w_k}{k}</script></li>
<li>Document similarity is useful for all sorts of applications; information retrieval, plagiarism detection, news recommender systems, and even for digital humanities tasks like comparing different versions of a text to see which are similar to each other.</li>
</ul>
</li>
</ul>
<h2 id="Pointwise-Mutual-Information-PMI"><a href="#Pointwise-Mutual-Information-PMI" class="headerlink" title="Pointwise Mutual Information (PMI)"></a>Pointwise Mutual Information (PMI)</h2><p>PPMI (positive pointwise mutual information) draws on the intuition that best way to weigh the association between two words is to ask how much <strong>more</strong> the two words co-occur in our corpus than we would have a priori expected them to appear by chance.</p>
<p>PMI is a measure of how often two events x and y occur, compared with what we would expect if they were independent:</p>
<script type="math/tex; mode=display">I(x,y) = \log_2 \frac{P(x,y)}{P(x)P(y)}</script><ul>
<li>numerator tells us how often we observed the two words together</li>
<li><p>denominator tells us how often we would <strong>expect</strong> the two words to co-occur assuming they each occurred independently</p>
</li>
<li><p>ratio gives us an estimate of how much more the two words co-occur than we expect by chance</p>
</li>
</ul>
<p>Negative PMI tend to be unreliable unless enormous corpora, also it’s not clear whether possible to evaluate with human. So more common to use Positive PMI:</p>
<script type="math/tex; mode=display">PPMI(w,c) = \max(\log_2 \frac{P(w,c)}{P(w)P(c)}, 0)</script><p>PMI is negative means two words co-occur in the corpus sightly less often we would expect by chance.</p>
<p>PMI has the problem of being biased toward infrequent events; very rare words tend to have very high PMI values. </p>
<ul>
<li>One way is using a different function to compute P(c):</li>
</ul>
<script type="math/tex; mode=display">PPMI_\alpha (w,c) = \max (\log_2 \frac{P(w,c)}{P(w)P_\alpha(c)}, 0)</script><script type="math/tex; mode=display">P_\alpha (c) = \frac{count(c)^\alpha}{\sum_c count (c)^\alpha}</script><p>α = 0.75 improved performance of embeddings on a wide range of tasks. It is the same as the discount d in Kneser-Ney Smoothing, how amazing!</p>
<ul>
<li>Another solution is Laplace smoothing: a small k is added to each of the counts</li>
</ul>
<h2 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h2><ul>
<li>short</li>
<li>dense</li>
</ul>
<p><strong>skip-gram with negative sampling</strong>, also called <strong>SGNS</strong>, kind of <strong>word2vec</strong>. The intuition is not to count how often each word w occurs near x, instead by a classifier on a binary prediction task: Is word w likely to show up near x? Prediction task is unimportant, we’ll take the learned classifier <strong>weights</strong> as the word embeddings.</p>
<p>Word2vec is a much simpler model:</p>
<ul>
<li>simplifies the task: binary classification instead of word prediction</li>
<li>simplifies the architecture: logistic regression classifier instead of multi-layer neural network</li>
</ul>
<h3 id="The-classifier"><a href="#The-classifier" class="headerlink" title="The classifier"></a>The classifier</h3><script type="math/tex; mode=display">P(+|t,c)</script><p>Use similarity to compute the P (the intuition is: a word is likely to occur near the target if its embedding is similar to the target embedding).</p>
<script type="math/tex; mode=display">Similarity(t,c) ≈ t · c</script><p>Use logistic or sigmoid function:</p>
<script type="math/tex; mode=display">P(+|t,c) = \frac{1}{1+e^{-t·c}}</script><p>Multiple context words:</p>
<script type="math/tex; mode=display">\log P(+|t, c_{1:k}) = \sum_{i=1}^k \log \frac{1}{1+e^{-t·c}}</script><p>In summary, given a test target word t and its context window of k words c (1:k), assigns a probability based on how similar this context window is to the target word.</p>
<h3 id="Learning-skip-gram-embeddings"><a href="#Learning-skip-gram-embeddings" class="headerlink" title="Learning skip-gram embeddings"></a>Learning skip-gram embeddings</h3><p>Word2vec learns embeddings by starting with an initial set of embedding vectors and then iteratively shifting the embedding of each word w to be more like the embeddings of words that occur nearby in texts, and less like the embeddings of words that don’t occur nearby.</p>
<p>For each of these (t, c) training instances we’ll create k negative samples, each consisting of the target t plus a ‘noise word’. The noise words are chosen according to their weighted unigram frequency pα(w), where α (in practice is 0.75) is a weight, to give rare words slightly higher probability.</p>
<script type="math/tex; mode=display">P_\alpha (w) = \frac{count(w)^\alpha}{\sum_{w'} count (w')^\alpha}</script><script type="math/tex; mode=display">L(\theta) = \sum_{(t,c) \in +} \log P(+|t,c) + \sum_{(t,c) \in -} \log P(-|t,c)</script><script type="math/tex; mode=display">L(\theta) = \log P(+|t,c) +\sum_{i=1}^k \log P(-|t,n_i)</script><script type="math/tex; mode=display">L(\theta) = \log \frac{1}{1+e^{-c·t}} +\sum_{i=1}^k \log \frac {1}{1+e^{n_i·t}}</script><p>The skip-gram learns two embeddings:</p>
<ul>
<li>target embedding t</li>
<li>context embedding c</li>
</ul>
<p>We can:</p>
<ul>
<li>throw away C matrix</li>
<li>add the two embeddings together: ti + ci</li>
</ul>
<p>Window size L often tuned on a dev set.</p>
<h2 id="Visualizing-Embeddings"><a href="#Visualizing-Embeddings" class="headerlink" title="Visualizing Embeddings"></a>Visualizing Embeddings</h2><ul>
<li>list most similar words to w by cosines</li>
<li><p>use a clustering algorithm to show a hierarchical representation of which words are similar to others</p>
</li>
<li><p>project dimensions to 2</p>
</li>
</ul>
<h2 id="Semantic-properties-of-embeddings"><a href="#Semantic-properties-of-embeddings" class="headerlink" title="Semantic properties of embeddings"></a>Semantic properties of embeddings</h2><ul>
<li><p>Shorter context windows tend to lead to representations that are a bit more syntactic (with same parts of speech).</p>
</li>
<li><p>Longer context windows, the highest cosine words to a target word w tend to be words that are topically related but not similar.</p>
</li>
</ul>
<p>Two kinds of similarity or association between words:</p>
<ul>
<li>Two words have <strong>first-order co-occurrence</strong> (sometimes called <strong>syntagmatic association</strong>) if they are typically nearby each other.</li>
<li>Two words have <strong>second-order co-occurrence</strong> (sometimes called <strong>paradigmatic association</strong>) if they have similar neighbors. </li>
</ul>
<p><strong>Analogy</strong> is another semantic property of embeddings to capture relational meanings (king - man + woman close to queen).</p>
<p><strong>Embeddings and Historical Semantics</strong>: studying how meaning changes over time, by computing multiple embedding spaces, each from texts written in a particular time period. </p>
<h2 id="Bias-and-Embeddings"><a href="#Bias-and-Embeddings" class="headerlink" title="Bias and Embeddings"></a>Bias and Embeddings</h2><p>In addition to their ability to learn word meaning from text, embeddings also reproduce the implicit biases and stereotypes that were latent in the text.</p>
<p>Embeddings also encode the implicit associations that are a property of human reasoning.</p>
<h2 id="Evaluating-Vector-Models"><a href="#Evaluating-Vector-Models" class="headerlink" title="Evaluating Vector Models"></a>Evaluating Vector Models</h2><p>The most important evaluation metric for vector models is extrinsic evaluation on tasks; adding them as features into any NLP task and seeing whether this improves performance over some other model.</p>
<p>Nonetheless it is useful to have intrinsic evaluations.</p>
<ul>
<li>The most common metric is to test their performance on <strong>similarity</strong><ul>
<li>WordSim-353</li>
<li>SimLex-999</li>
<li>TOEFL dataset</li>
</ul>
</li>
<li>More realistic are intrinsic similarity tasks that include context: SCWS</li>
<li>Another task is to solve problems like: a is to b as c is to d, given a, b, and c and having to find d.</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>In vector semantics, a word is a vector, also called <strong>embedding</strong></li>
<li>There are two classes: <strong>sparse</strong> and <strong>dense</strong>. Sparse model like <strong>tf-idf</strong> each dimension corresponds to a word in the vocabulary V</li>
<li>Cell in sparse models are functions of <strong>co-occurrence counts</strong>. The <strong>term-document</strong> matrix has rows of each word (<strong>term</strong>) in V and a column for each document</li>
<li>The <strong>word-context</strong> matrix has a row for each (target) word in V and a column for each context term in V</li>
<li>A common sparse weighting is <strong>tf-idf</strong>, which weighs each cell by its <strong>term frequency</strong> and <strong>inverse document frequency</strong></li>
<li>Word and document similarity is computed by <strong>dot product</strong> between vectors. The <strong>cosine similarity</strong> is the most popular metric</li>
<li><strong>PPMI (pointwise positive mutual information)</strong> is an alternative weighting scheme to tf-idf</li>
<li>Dense vector models have dimensionality of 50-300 and the dimensions are harder to interpret</li>
<li>The <strong>word2vec</strong> family including <strong>skip-gram</strong> and <strong>CBOW</strong>, is a popular efficient way to compute dense embeddings</li>
<li>Skip-gram trains a logistic regression classifier to compute the probability that two words are “likely to occur nearby in text”. This probability is computed by dot production between two word embeddings</li>
<li>Other important embedding algorithms include <strong>GloVe</strong>, a method based on ratios of word co-occurrence probabilities, and <strong>fasttext</strong>, an open-source library for computing word embeddings by summing embeddings of the bag of character n-grams that make up a word</li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/05/16/SLP/2019-05-16-Vector-Semantics/">
    <time datetime="2019-05-16T03:11:00.000Z" class="entry-date">
        2019-05-16
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cosine/">Cosine</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Embeddings/">Embeddings</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Lexical-Semantics/">Lexical Semantics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PPMI/">PPMI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/TF-IDF/">TF-IDF</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Vector-Semantics/">Vector Semantics</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Word2vec/">Word2vec</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2019/05/31/SLP/2019-05-31-Neural-Networks-and-Neural-Language-Models/" rel="prev"><span class="meta-nav">←</span> Neural Networks and Neural Language Models Note (SLP Ch07)</a></span>
    
    
        <span class="nav-next"><a href="/2019/05/08/SLP/2019-05-08-Logistic-Regression/" rel="next">Logistic Regression Note (SLP Ch05) <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">15</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">39</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">10</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2019/07/09/2019-07-09-Elasticsearch-Basic/">《Elasticsearch 权威指南》之基础入门 Note（基于 7.x）</a>
          </li>
        
          <li>
            <a href="/2019/07/08/SLP/2019-07-08-Syntactic-Parsing/">Syntactic Parsing Note (SLP Ch11)</a>
          </li>
        
          <li>
            <a href="/2019/06/21/SLP/2019-06-21-Formal-Grammars-of-English/">Formal Grammars of English Note (SLP Ch10)</a>
          </li>
        
          <li>
            <a href="/2019/06/19/2019-06-19-Think-From-Three-Gates/">信息熵与选择：由三门问题想到的</a>
          </li>
        
          <li>
            <a href="/2019/06/17/SLP/2019-06-17-Senquence-Processing-with-Recurrent-Networks/">Sequence Processing with Recurrent Networks Note (SLP Ch09)</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a><span class="tag-list-count">40</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backward/">Backward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Beam-Search/">Beam Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Business/">Business</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CCG/">CCG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CFG/">CFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CKY/">CKY</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CYK/">CYK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calculus/">Calculus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chunking/">Chunking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer-Science/">Computer Science</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context-Free-Grammars/">Context-Free Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cosine/">Cosine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Entropy/">Cross Entropy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DB/">DB</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Clearing/">Data Clearing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Science/">Data Science</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Structure/">Data Structure</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decoding/">Decoding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/">DeepLearning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependence/">Dependence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Diary/">Diary</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Economics/">Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Edit-Distance/">Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Elasticsearch/">Elasticsearch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Embeddings/">Embeddings</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/">Entropy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Evaluation/">Evaluation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/F1/">F1</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FDW/">FDW</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FSM/">FSM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Analysis/">Formal Analysis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Grammars/">Formal Grammars</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forward/">Forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Full-Text-Search/">Full-Text-Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GRU/">GRU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gan/">Gan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradient-Descent/">Gradient Descent</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HMM/">HMM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IE/">IE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Industry/">Industry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Extraction/">Information Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Theory/">Information Theory</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Job/">Job</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LM/">LM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Language-Model/">Language Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexical-Semantics/">Lexical Semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalism/">Lexicalism</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalized-Grammars/">Lexicalized Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Algebra/">Linear Algebra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lucene/">Lucene</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MEMM/">MEMM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine/">Machine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markov/">Markov</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Materialized-Views/">Materialized Views</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">35</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/">Naive Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ngram/">Ngram</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Occupation/">Occupation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Orientation/">Orientation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PPMI/">PPMI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Partial-Parsing/">Partial Parsing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammars/">Phrase-Structure Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PoS/">PoS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Postgres/">Postgres</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regex/">Regex</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regular-Expression/">Regular Expression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reqular-Expressions/">Reqular Expressions</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SGD/">SGD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/">SQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SRN/">SRN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Search/">Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sentiment-Classification/">Sentiment Classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simon/">Simon</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simpson-Paradox/">Simpson Paradox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smoothing/">Smoothing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sort/">Sort</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spell-Check/">Spell Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Style/">Style</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/">System</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TF-IDF/">TF-IDF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tagging/">Tagging</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test/">Test</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Text-Normalization/">Text Normalization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Treebank/">Treebank</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valence/">Valence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vector-Semantics/">Vector Semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VirtualBox/">VirtualBox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Viterbi/">Viterbi</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word2vec/">Word2vec</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Work/">Work</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZhouZhihua/">ZhouZhihua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zipf/">Zipf</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Business/" style="font-size: 10px;">Business</a> <a href="/tags/C/" style="font-size: 12px;">C</a> <a href="/tags/CCG/" style="font-size: 10px;">CCG</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer-Science/" style="font-size: 14px;">Computer Science</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Cosine/" style="font-size: 10px;">Cosine</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/DB/" style="font-size: 12px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Science/" style="font-size: 14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 14px;">Data Structure</a> <a href="/tags/Decoding/" style="font-size: 10px;">Decoding</a> <a href="/tags/DeepLearning/" style="font-size: 14px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 10px;">Diary</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Embeddings/" style="font-size: 12px;">Embeddings</a> <a href="/tags/Entropy/" style="font-size: 12px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 12px;">Evaluation</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Formal-Analysis/" style="font-size: 10px;">Formal Analysis</a> <a href="/tags/Formal-Grammars/" style="font-size: 12px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 12px;">HMM</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/LM/" style="font-size: 12px;">LM</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Language-Model/" style="font-size: 10px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 12px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/NLP/" style="font-size: 18px;">NLP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Ngram/" style="font-size: 12px;">Ngram</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Philosophy/" style="font-size: 12px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase-Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Postgres/" style="font-size: 12px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Python/" style="font-size: 16px;">Python</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SQL/" style="font-size: 12px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10px;">Search</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Smoothing/" style="font-size: 10px;">Smoothing</a> <a href="/tags/Sort/" style="font-size: 10px;">Sort</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/System/" style="font-size: 12px;">System</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 12px;">Viterbi</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2019 Yam
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>