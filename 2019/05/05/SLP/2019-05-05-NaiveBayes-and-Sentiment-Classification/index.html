<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>Naive Bayes and Sentiment Classification Note (SLP Ch04) | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Text categorization, the task of assigning a label or category to an entire text or document.  sentiment analysis spam detection subject category or topic label  Probabilistic classifier additionally">
<meta name="keywords" content="NLP,AI,Naive Bayes,Sentiment Classification,Evaluation,F1,Test">
<meta property="og:type" content="article">
<meta property="og:title" content="Naive Bayes and Sentiment Classification Note (SLP Ch04)">
<meta property="og:url" content="http://www.yam.gift/2019/05/05/SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Text categorization, the task of assigning a label or category to an entire text or document.  sentiment analysis spam detection subject category or topic label  Probabilistic classifier additionally">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch4-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch4-2.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch4-3.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch4-4.jpeg">
<meta property="og:updated_time" content="2019-05-08T03:09:05.559Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Naive Bayes and Sentiment Classification Note (SLP Ch04)">
<meta name="twitter:description" content="Text categorization, the task of assigning a label or category to an entire text or document.  sentiment analysis spam detection subject category or topic label  Probabilistic classifier additionally">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/slp-ch4-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="http://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification" class="post-SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      Naive Bayes and Sentiment Classification Note (SLP Ch04)
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/05/05/SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification/" data-id="cjy1gawt8005wd6cczox7022n" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p><strong>Text categorization</strong>, the task of assigning a label or category to an entire text or document.</p>
<ul>
<li>sentiment analysis</li>
<li>spam detection</li>
<li>subject category or topic label</li>
</ul>
<p><strong>Probabilistic classifier</strong> additionally will tell us the probability of the observation being in the class.</p>
<p><strong>Generative classifiers</strong> like naive Bayes build a model of how a class could generate some input data. </p>
<p><strong>Discriminative classifiers</strong> like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. </p>
<a id="more"></a>
<h2 id="Naive-Bayes-Classifiers"><a href="#Naive-Bayes-Classifiers" class="headerlink" title="Naive Bayes Classifiers"></a>Naive Bayes Classifiers</h2><p><strong>Bayesian inference</strong>: </p>
<script type="math/tex; mode=display">\hat c = \arg \max_{c \in C} P(c|d) = \arg \max_{c \in C} P(d|c) P(c) = \arg \max_{c \in C} P(f_1,f_2,…f_n|c) P(c)</script><ul>
<li>given document d</li>
<li>prior probability of the class P(c)</li>
<li><p>likelihood of the document p(d|c)</p>
</li>
<li><p>f1, f2, … as a set of features</p>
</li>
</ul>
<p>P(f1,f2…|c) is hard to compute directly, estimate the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets.</p>
<p><strong>Naive Bayes classifiers</strong> make two simplifying assumptions:</p>
<ul>
<li>position doesn’t matter</li>
<li>P(fi|c) are independent given the class c: <script type="math/tex">P(f_1,f_2,…,f_n|c) = P(f_1|c)P(f_2|c)…P(f_n|c)</script></li>
</ul>
<script type="math/tex; mode=display">C_{NB} = \arg \max_{c \in C} P(c) \prod_{f \in F} P(f|c)</script><p>To apply the naive Bayes classifier to text, we need to consider word position:</p>
<script type="math/tex; mode=display">C_{NB} = \arg \max_{c \in C} P(c) \prod_{i \in positions} P(w_i|c)</script><p>Calculations are done in log space to avoid underflow and increase speed:</p>
<script type="math/tex; mode=display">C_{NB} = \arg \max_{c \in C} \log P(c)  + \sum_{i \in positions} \log P(w_i|c)</script><h2 id="Training-the-Naive-Bayes-Classifer"><a href="#Training-the-Naive-Bayes-Classifer" class="headerlink" title="Training the Naive Bayes Classifer"></a>Training the Naive Bayes Classifer</h2><script type="math/tex; mode=display">\hat P(c) = \frac {N_c}{N_{doc}}</script><script type="math/tex; mode=display">\hat P(w_i|c) = \frac {count(w_i, c)}{\sum_{w \in V} count(w, c)}</script><p>If a word only occurs in one class, the probability for this feature in the other class will be zero, then the total class is zero, no matter the other evidence. The simplest solution is Add-one smoothing:</p>
<script type="math/tex; mode=display">\hat P(w_i|c) = \frac {count(w_i, c)+1}{\sum_{w \in V} (count(w, c)+1)} =  \frac {count(w_i, c)+1}{(\sum_{w \in V} count(w, c)) + |V|}</script><p>V consists of the union of all the word types in all classes, not just the words in one class c.</p>
<p>The solution for <strong>unknown words</strong> (in test data) is to ignore them, remove them from the test document and not include any probability for them at all.</p>
<p>In most text classification applications, however, using a stop word list doesn’t improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">function Train Naive Bayes(D, C) returns log P(c) <span class="keyword">and</span> log P(w|c)</span><br><span class="line"><span class="keyword">for</span> each <span class="class"><span class="keyword">class</span> <span class="title">c</span> ∈ <span class="title">C</span></span></span><br><span class="line">	Ndoc = number of documents in D</span><br><span class="line">    Nc = number of documents <span class="keyword">from</span> D <span class="keyword">in</span> <span class="class"><span class="keyword">class</span> <span class="title">c</span></span></span><br><span class="line"><span class="class">    <span class="title">logprior</span>[<span class="title">c</span>] ← <span class="title">log</span> <span class="title">Nc</span>/<span class="title">Ndoc</span></span></span><br><span class="line"><span class="class">    <span class="title">V</span> ← <span class="title">vocabulary</span> <span class="title">of</span> <span class="title">D</span></span></span><br><span class="line"><span class="class">    <span class="title">bigdoc</span>[<span class="title">c</span>] ← <span class="title">append</span><span class="params">(d)</span> <span class="title">for</span> <span class="title">d</span> ∈ <span class="title">D</span> <span class="title">with</span> <span class="title">class</span> <span class="title">c</span></span></span><br><span class="line"><span class="class">    <span class="title">for</span> <span class="title">each</span> <span class="title">word</span> <span class="title">w</span> <span class="title">in</span> <span class="title">V</span></span></span><br><span class="line"><span class="class">    	<span class="title">count</span><span class="params">(w, c)</span> ← <span class="title">num</span> <span class="title">of</span> <span class="title">occurrences</span> <span class="title">of</span> <span class="title">w</span> <span class="title">in</span> <span class="title">bigdoc</span>[<span class="title">c</span>]</span></span><br><span class="line">        loglikelihood[w,c] ← log (count(w,c) + 1)/∑(count(w*, c) + 1)</span><br><span class="line">    <span class="keyword">return</span> logprior, loglikelihood</span><br><span class="line"></span><br><span class="line">function Test Naive Bayes(testdoc, logprior, loglikelihood, C, V) returns best c</span><br><span class="line"><span class="keyword">for</span> each <span class="class"><span class="keyword">class</span> <span class="title">c</span> ∈ <span class="title">C</span></span></span><br><span class="line"><span class="class">	<span class="title">sum</span>[<span class="title">c</span>] ← <span class="title">logprior</span>[<span class="title">c</span>]</span></span><br><span class="line"><span class="class">    <span class="title">for</span> <span class="title">each</span> <span class="title">position</span> <span class="title">i</span> <span class="title">in</span> <span class="title">testdoc</span></span></span><br><span class="line"><span class="class">    	<span class="title">word</span> ← <span class="title">testdoc</span>[<span class="title">i</span>]</span></span><br><span class="line"><span class="class">        <span class="title">if</span> <span class="title">word</span> ∈ <span class="title">V</span></span></span><br><span class="line">        	sum[c] ← sum[c] + loglikehood[word, c]</span><br><span class="line"><span class="keyword">return</span> argmax_c sum[c]</span><br></pre></td></tr></table></figure>
<p>The code is here: <a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/Bayes/Naive-Bayes.ipynb" target="_blank" rel="noopener">Naive-Bayes</a></p>
<h2 id="Optimizing-for-Sentiment-Analysis"><a href="#Optimizing-for-Sentiment-Analysis" class="headerlink" title="Optimizing for Sentiment Analysis"></a>Optimizing for Sentiment Analysis</h2><ul>
<li><p>In some tasks like sentiment classification, whether a word occurs or not seems to matter more than its frequency. Thus it often improve performance to clip the word counts in <strong>each document</strong> at 1. This variant is called <strong>binary multinomial naive Bayes</strong> or <strong>binary NB</strong>. DO NOT need to smooth. In Python, just use <code>set(doc_i)</code>.</p>
</li>
<li><p>Deal with <strong>negation</strong>. A simple baseline is to add a prefix <code>NOT_</code> to every word after a token of logical negation (n’t, not, no, never) until the next punctuation mark. It works quite well in practice.</p>
</li>
<li>When training data is insufficient, we can instead derive the positive and negative word features from <strong>sentiment lexicons</strong>. <ul>
<li>A common way to use lexicons is a naive Bayes classifier is to add a feature that is  counted whenever a word from that lexicon occurs. </li>
<li>If training data is sparse or not representative of the test set, using dense lexicon features (whether a word occurs in the lexicon) instead of sparse individual-word features may generalize better.</li>
</ul>
</li>
</ul>
<h2 id="Naive-Bayes-for-other-text-classification-tasks"><a href="#Naive-Bayes-for-other-text-classification-tasks" class="headerlink" title="Naive Bayes for other text classification tasks"></a>Naive Bayes for other text classification tasks</h2><ul>
<li>spam detection: sets of words or phrases as features</li>
<li>language ID: byte n-grams.</li>
</ul>
<h2 id="Naive-Bayes-as-aLanguage-Model"><a href="#Naive-Bayes-as-aLanguage-Model" class="headerlink" title="Naive Bayes as aLanguage Model"></a>Naive Bayes as aLanguage Model</h2><p>If we use all the words in the text, naive Bayes is similar to language model. Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model.</p>
<p>Since the likelihood features from the naive Bayes model assign a probability to each word P(w|c), the model also assigns a probability to each sentence: <script type="math/tex">P(s|c) = \prod_{i \in positions} P(w_i|c)</script></p>
<h2 id="Evaluation-Precision-Recall-F-measure"><a href="#Evaluation-Precision-Recall-F-measure" class="headerlink" title="Evaluation: Precision, Recall, F-measure"></a>Evaluation: Precision, Recall, F-measure</h2><p><img src="http://qnimg.lovevivian.cn/slp-ch4-1.jpeg" alt=""></p>
<p>Accuracy is not a good metric when the goal is to discover something that is rare, or at least not completely balanced in frequency, which is a very common situation in the world.</p>
<p>Precision and recall, emphasize true positives: finding the things that we are supposed to be looking for.</p>
<p><strong>F-measure</strong>: <script type="math/tex">F_\beta = \frac {(\beta^2+1)PR}{\beta^2P + R}</script></p>
<p>The β weights the importance of recall and precision, based on the needs of an application.</p>
<ul>
<li>β &gt; 1 favor recall</li>
<li>β &lt; 1 favor precision</li>
<li>β = 1 equally balanced, called F1</li>
</ul>
<p>F-measure comes from a weighted harmonic mean of precision and recall.</p>
<script type="math/tex; mode=display">HarmonicMean(a_1, a_2,…,a_n) = \frac {n}{\frac{1}{a_1}+\frac{1}{a_2}+…+\frac{1}{a_n}}</script><script type="math/tex; mode=display">F = \frac {1}{\alpha \frac{1}{P} + (1-\alpha) \frac{1}{R}} = F_\beta</script><script type="math/tex; mode=display">\beta^2 = \frac{1-\alpha}{\alpha}</script><h3 id="More-than-two-classes"><a href="#More-than-two-classes" class="headerlink" title="More than two classes"></a>More than two classes</h3><p>Two kinds of multi-class classification tasks:</p>
<ul>
<li>any-of</li>
<li>multi-label</li>
</ul>
<p>We combine the metric values in two ways:</p>
<ul>
<li>macroaveraging: compute the performance for each class, and then average over classes.</li>
<li>microaveraging: collect the decisions for all classes into a single contingency table, and<br>  then compute precision and recall from that table. </li>
</ul>
<p><img src="http://qnimg.lovevivian.cn/slp-ch4-2.jpeg" alt=""></p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch4-3.jpeg" alt=""></p>
<h2 id="Test-sets-and-Cross-validation"><a href="#Test-sets-and-Cross-validation" class="headerlink" title="Test sets and Cross-validation"></a>Test sets and Cross-validation</h2><p>We use the <strong>development test set</strong> (devset) to tune some parameters.</p>
<p>If we could use all our data both for training and test, we do this by <strong>cross-validation</strong>: </p>
<ul>
<li>randomly choose a training and test set division of data, train and compute error rate</li>
<li>repeat with a different randomly selected training and test set</li>
</ul>
<p>Repeat times n, we get an average error rate, it is called <strong>n-fold cross-validation</strong>.</p>
<p>As all the data is used for test, we need the whole corpus to be blind, means we can’t examine any of the data to suggest possible features and see what’s going on. But looking at the corpus is often important.</p>
<p>So it is common to create a fixed training and test set, then do cross-validation inside the training set, but compute error rate the normal way in the test set.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch4-4.jpeg" alt=""></p>
<h2 id="Statistical-Significance-Testing"><a href="#Statistical-Significance-Testing" class="headerlink" title="Statistical Significance Testing"></a>Statistical Significance Testing</h2><p>We have a test set x of n observations x = x1, x2, … xn on which A’s performance is better than B by δ(x). We need to reject the <strong>null hypothesis</strong> that A isn’t really better than B and this difference δ(x) occurred purely by chance.</p>
<p>In language processing we use non-parametric tests like the <strong>bootstrap test</strong>, or a similar test <strong>approximate randomization</strong> because most metrics are not normally distributed. <strong>Bootstrapping</strong> refers to repeatedly drawing large numbers of smaller samples with replacement (bootstrap samples) from an original larger sample.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">function BOOSTRAP(test set x, num of samples b) returns p-value(x)</span><br><span class="line">Calculate δ(x) <span class="comment"># how much better A do than B on x</span></span><br><span class="line"><span class="keyword">for</span> i = <span class="number">1</span> to b do</span><br><span class="line">	<span class="keyword">for</span> j = <span class="number">1</span> to n do</span><br><span class="line">    	Select a member of x at random <span class="keyword">and</span> add it to x*(i)</span><br><span class="line">    Calculate δ(x*(i))</span><br><span class="line"><span class="keyword">for</span> each x*(i)</span><br><span class="line">	s ← s + <span class="number">1</span> <span class="keyword">if</span> δ(x*(i)) &gt; <span class="number">2</span>δ(x)</span><br><span class="line">p-value(x) ≈ s/b</span><br><span class="line"><span class="keyword">return</span> p-value(x)</span><br></pre></td></tr></table></figure>
<p>The code is here: <a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/Test/Statistical-Significance-Testing.ipynb" target="_blank" rel="noopener">Statistical-Significance-Testing</a></p>
<h2 id="Advanced-Feature-Selection"><a href="#Advanced-Feature-Selection" class="headerlink" title="Advanced: Feature Selection"></a>Advanced: Feature Selection</h2><p>Features are generally ranked by how informative they are about the classification decision. A very common metric is <strong>information gain</strong> which tells us how many bits of information the presence of the word gives us for guessing the class.</p>
<script type="math/tex; mode=display">G(w) = -\sum_{i=1}^C P(c_i) \log P(c_i) + P(w) \sum_{i=1}^C P(c_i|w) \log P(c_i|w) + P(\overline{w}) \sum_{i=1}^C P(c_i|\overline{w}) \log P(c_i|\overline{w})</script><p> c_i is ith class, w- means that a document does not contain the word w​.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>Many language processing tasks can be viewed as tasks of <strong>classification</strong> learn to model the class given the observation.</li>
<li>Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.</li>
<li>Naive Bayes is a generative model that make the bag of words assumption (position doesn’t matter) and the conditional independence assumption (words are conditionally independent of each other given the class). Naive Bayes with binarized features seems to work better for many text classification tasks.</li>
<li>Feature selection can be used to automatically remove features that aren’t helpful.</li>
<li>Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.</li>
<li>Classifiers are evaluated based on precision and recall.</li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/05/05/SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification/">
    <time datetime="2019-05-05T03:11:00.000Z" class="entry-date">
        2019-05-05
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Evaluation/">Evaluation</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/F1/">F1</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Naive-Bayes/">Naive Bayes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Sentiment-Classification/">Sentiment Classification</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Test/">Test</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2019/05/08/SLP/2019-05-08-Logistic-Regression/" rel="prev"><span class="meta-nav">←</span> Logistic Regression Note (SLP Ch05)</a></span>
    
    
        <span class="nav-next"><a href="/2019/04/22/SLP/2019-04-22-RegularExpressions-TextNormalization-EditDistance/" rel="next">Regular Expressions, Text Normalization, and Edit Distance Note (SLP Ch02) <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">42</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">11</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2019/08/03/LeetCode/2019-08-03-Longest-Palindromic-Substring/">Longest Palindromic Substring (LeetCode 5)</a>
          </li>
        
          <li>
            <a href="/2019/08/02/Paper/2019-08-02-Baidu-ERNIE-Tutorial/">ERNIE Tutorial（论文笔记 + 实践指南）</a>
          </li>
        
          <li>
            <a href="/2019/07/18/LeetCode/2019-07-15-Median-of-Two-Sorted-Arrays/">Median of Two Sorted Arrays (LeetCode 4)</a>
          </li>
        
          <li>
            <a href="/2019/07/17/SLP/2019-07-17-Statistical-Parsing/">Statistical Parsing Note (SLP Ch12)</a>
          </li>
        
          <li>
            <a href="/2019/07/15/Diary/2019-07-15-diary/">随笔：命运</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AE/">AE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a><span class="tag-list-count">41</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AR/">AR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Array/">Array</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backward/">Backward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Beam-Search/">Beam Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Binary-Search/">Binary Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Business/">Business</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CCG/">CCG</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CFG/">CFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CKY/">CKY</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CYK/">CYK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calculus/">Calculus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chunking/">Chunking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Collins-Parser/">Collins Parser</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer-Science/">Computer Science</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context-Free-Grammars/">Context-Free Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cosine/">Cosine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Entropy/">Cross Entropy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-brackets/">Cross-brackets</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DB/">DB</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DP/">DP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Clearing/">Data Clearing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Science/">Data Science</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Structure/">Data Structure</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decoding/">Decoding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/">DeepLearning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependence/">Dependence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Diary/">Diary</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ERNIE/">ERNIE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Economics/">Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Edit-Distance/">Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Elasticsearch/">Elasticsearch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Embeddings/">Embeddings</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/">Entropy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Evaluation/">Evaluation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/F1/">F1</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FDW/">FDW</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FSM/">FSM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Analysis/">Formal Analysis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Grammars/">Formal Grammars</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forward/">Forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Full-Text-Search/">Full-Text-Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GRU/">GRU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gan/">Gan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Garden-path/">Garden-path</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradient-Descent/">Gradient Descent</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HMM/">HMM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IE/">IE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Industry/">Industry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Extraction/">Information Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Theory/">Information Theory</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Job/">Job</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LM/">LM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Language-Model/">Language Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexical-Semantics/">Lexical Semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalism/">Lexicalism</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalized-CFG/">Lexicalized CFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalized-Grammars/">Lexicalized Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Algebra/">Linear Algebra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Sturcture/">Linear Sturcture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linked-List/">Linked List</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lucene/">Lucene</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MEMM/">MEMM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine/">Machine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Manacher/">Manacher</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markov/">Markov</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Materialized-Views/">Materialized Views</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Median/">Median</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">38</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/">Naive Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ngram/">Ngram</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Occupation/">Occupation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Orientation/">Orientation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCCG/">PCCG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCFG/">PCFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PPMI/">PPMI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Palindromic/">Palindromic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Partial-Parsing/">Partial Parsing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammars/">Phrase-Structure Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PoS/">PoS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Postgres/">Postgres</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pretraining/">Pretraining</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Queue/">Queue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regex/">Regex</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regular-Expression/">Regular Expression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reqular-Expressions/">Reqular Expressions</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SCFG/">SCFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SGD/">SGD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/">SQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SRN/">SRN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Search/">Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sentiment-Classification/">Sentiment Classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simon/">Simon</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simpson-Paradox/">Simpson Paradox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Slide/">Slide</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smoothing/">Smoothing</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sort/">Sort</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spell-Check/">Spell Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stack/">Stack</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/String/">String</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Style/">Style</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Substring/">Substring</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supertagging/">Supertagging</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/">System</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TF-IDF/">TF-IDF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tagging/">Tagging</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test/">Test</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Text-Normalization/">Text Normalization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Treebank/">Treebank</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valence/">Valence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vector-Semantics/">Vector Semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VirtualBox/">VirtualBox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Viterbi/">Viterbi</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word2vec/">Word2vec</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Work/">Work</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XLNet/">XLNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZhouZhihua/">ZhouZhihua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zipf/">Zipf</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Binary-Search/" style="font-size: 10px;">Binary Search</a> <a href="/tags/Business/" style="font-size: 10px;">Business</a> <a href="/tags/C/" style="font-size: 11.67px;">C</a> <a href="/tags/CCG/" style="font-size: 11.67px;">CCG</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer-Science/" style="font-size: 15px;">Computer Science</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Cosine/" style="font-size: 10px;">Cosine</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/DB/" style="font-size: 11.67px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Science/" style="font-size: 13.33px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 15px;">Data Structure</a> <a href="/tags/Decoding/" style="font-size: 10px;">Decoding</a> <a href="/tags/DeepLearning/" style="font-size: 13.33px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 11.67px;">Diary</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Embeddings/" style="font-size: 11.67px;">Embeddings</a> <a href="/tags/Entropy/" style="font-size: 11.67px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 11.67px;">Evaluation</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Formal-Analysis/" style="font-size: 10px;">Formal Analysis</a> <a href="/tags/Formal-Grammars/" style="font-size: 13.33px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 11.67px;">HMM</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/LM/" style="font-size: 11.67px;">LM</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Language-Model/" style="font-size: 10px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 11.67px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/NLP/" style="font-size: 18.33px;">NLP</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Ngram/" style="font-size: 11.67px;">Ngram</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Philosophy/" style="font-size: 11.67px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase-Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Postgres/" style="font-size: 11.67px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pretraining/" style="font-size: 11.67px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Python/" style="font-size: 16.67px;">Python</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SQL/" style="font-size: 11.67px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10px;">Search</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 11.67px;">Smoothing</a> <a href="/tags/Sort/" style="font-size: 10px;">Sort</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/String/" style="font-size: 10px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/System/" style="font-size: 11.67px;">System</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 11.67px;">Viterbi</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2019 Yam
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>