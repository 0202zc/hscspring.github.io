<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>自然语言计算机形式分析的理论与方法笔记(Ch13) | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="第十三章：N 元语法和数据平滑N 元语法N 元语法模型利用前面 N-1 个单词来预测下一个词。一些特殊情况：标点、大小写、屈折变化等。 一个单词的概率只依赖于它前面一个单词的这种假设叫作 Markov 假设，这样的模型叫 Bi-gram，即二元语法模型，也叫一阶 Markov 模型。 N 元语法模型可以使用训练语料库 “归一化” 得到。 p(w_n|w_{n-1}) = \frac {C(w_{n">
<meta name="keywords" content="NLP,AI,Ngram,Smoothing">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言计算机形式分析的理论与方法笔记(Ch13)">
<meta property="og:url" content="http://www.yam.gift/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="第十三章：N 元语法和数据平滑N 元语法N 元语法模型利用前面 N-1 个单词来预测下一个词。一些特殊情况：标点、大小写、屈折变化等。 一个单词的概率只依赖于它前面一个单词的这种假设叫作 Markov 假设，这样的模型叫 Bi-gram，即二元语法模型，也叫一阶 Markov 模型。 N 元语法模型可以使用训练语料库 “归一化” 得到。 p(w_n|w_{n-1}) = \frac {C(w_{n">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-04-13T13:37:51.485Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="自然语言计算机形式分析的理论与方法笔记(Ch13)">
<meta name="twitter:description" content="第十三章：N 元语法和数据平滑N 元语法N 元语法模型利用前面 N-1 个单词来预测下一个词。一些特殊情况：标点、大小写、屈折变化等。 一个单词的概率只依赖于它前面一个单词的这种假设叫作 Markov 假设，这样的模型叫 Bi-gram，即二元语法模型，也叫一阶 Markov 模型。 N 元语法模型可以使用训练语料库 “归一化” 得到。 p(w_n|w_{n-1}) = \frac {C(w_{n">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="http://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing" class="post-NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      自然语言计算机形式分析的理论与方法笔记(Ch13)
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="http://www.yam.gift/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/" data-id="cju9u9za700035occ7pzjn2ga" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <h1 id="第十三章：N-元语法和数据平滑"><a href="#第十三章：N-元语法和数据平滑" class="headerlink" title="第十三章：N 元语法和数据平滑"></a>第十三章：N 元语法和数据平滑</h1><h2 id="N-元语法"><a href="#N-元语法" class="headerlink" title="N 元语法"></a>N 元语法</h2><p>N 元语法模型利用前面 N-1 个单词来预测下一个词。一些特殊情况：标点、大小写、屈折变化等。</p>
<p>一个单词的概率只依赖于它前面一个单词的这种假设叫作 Markov 假设，这样的模型叫 Bi-gram，即二元语法模型，也叫一阶 Markov 模型。</p>
<p>N 元语法模型可以使用训练语料库 “归一化” 得到。</p>
<script type="math/tex; mode=display">p(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n)}{\sum_w C(w_{n-1}w)}​</script><p>以 <script type="math/tex">w_{n-1}</script> 开头的二元语法计数必定等于 <script type="math/tex">w_{n-1}</script> 这个单词的计数，于是：</p>
<script type="math/tex; mode=display">p(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n)}{C(w_{n-1})}</script><p>一般化 N 元语法的参数估计：</p>
<script type="math/tex; mode=display">p(w_n|w_{n-N+1}^{n-1}) = \frac {C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}</script><p>两个重要事实：</p>
<ul>
<li>N 增加时，精确度相应增加，同时生成句子的局限性增加（可选的下个词减少）</li>
<li>严重依赖于语料库</li>
</ul>
<a id="more"></a>
<h2 id="数据平滑"><a href="#数据平滑" class="headerlink" title="数据平滑"></a>数据平滑</h2><p>N 元语法在特定语料中会有大量零概率的情况，但实际上可能并非如此，因此需要给某些零概率和低概率的 N 元语法重新赋值，这就是 “平滑”。</p>
<ul>
<li><p>Additive</p>
<ul>
<li>给取出来的二元语法的计数全部加一，即：(ci + 1)/(N+V)，V 是词表大小，因为每个都加一，所以分母加 V，<script type="math/tex">p*(w_n|w_{n-1}) = \frac {C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V}​</script>。</li>
<li>一个很大的问题是，每个二元语法计数加一，对应的一元语法加 V，如果某个一元语法本来出现的次数很少的话，会使其概率发生急剧变化。比如：“蝴蝶 吃饭”，我们假设它在语料中出现了 1 次，“蝴蝶” 出现了 2 次，本来的概率是 1/2=0.5，如果词表大小是 1000，概率就会变成 2/(1000+2)。当然，如果 “蝴蝶 吃饭” 没有出现在语料中，那这样的概率是可以接受的。问题就在于那些出现概率本身不高的词。</li>
<li>同样的问题也表现在未登录词上，会给较高的概率，尤其词表非常大的时候。这相当于人为稀释了所有在训练数据中词的概率（意味着提高了未登录词的概率）。根本原因还是数据太稀疏。</li>
<li>除了上面介绍的加一，还可以加 α（一个比较小的数），大同小异，可以参考：<a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/Ngram/LM.ipynb" target="_blank" rel="noopener">LM</a></li>
</ul>
</li>
<li><p>Witten-Bell</p>
<ul>
<li>1991年提出，思想：看一个零概率 N 元语法的概率可以用首次看一个 N 元语法的概率模拟，即借助 “第一次看过的事物” 的数量估计 “从没见过的事物” 的概率。</li>
<li>所有零的 N 元语法的全部概率为：T/(N+T)，N 表示所有 Ngram 数，T 表示不重复的 Ngram 数。假设有 Z 个零次 Ngram，每个的概率为：T/((N+T)Z)；非零次的 Ngram，每个的概率为：Ci/(N+T)。</li>
</ul>
</li>
<li><p>Good-Turing</p>
<ul>
<li><p>1953 年提出，思想：用观察计数较高的 N 元语法数的方法，来重新估计概率，并把它指派给零计数或低计数的 N 元语法。新的平滑计数：<script type="math/tex">c^* = (c+1) \frac{N_{c+1}}{N_C}</script></p>
</li>
<li><p>c 表示从未出现过的 N 元语法计数，比如 c=0，Nc=10 万 表示 10 万个 N 元语法计数为 0，c*=(0+1) × N_(c+1)/Nc；依次类推可以求出 c=1、2…… 时的 c*，进而可以使用 c* 来表示原来的计数 c。出现 0 次的自然也被 c=0 对应的 c*（一般是一个比较小的数）替换，也就实现了平滑。</p>
</li>
<li><p>实际上，并不是对于所有的计数 c 都使用打折，较大的计数是可靠的，Katz（首先把 Good-Turing 打折算法用于 N 元语法平滑）建议取某个阈值 k=5：</p>
<script type="math/tex; mode=display">c^* = \frac{(c+1)\frac{N_{c+1}}{N_c} -c \frac{(k+1)N_{k+1}}{N_1}} {1-\frac{(k+1)N_{k+1}}{N_1}}, 1≤ c ≤ k</script></li>
</ul>
</li>
</ul>
<p>上面的方法都是解决零频率 N 元语法问题，此外还可以通过利用对应的 N-1 元或更低元的语法来计算。主要有两种方法：Backoff 和 Interpolation，Backoff 中，只有当阶数较高的 N 元语法中存在零计数时，才把阶数较高的 N  元语法降为阶数较低的 N 元语法。</p>
<ul>
<li><p>Katz Smoothing (Backoff)</p>
<ul>
<li>1987 年提出</li>
<li><script type="math/tex; mode=display">\hat {P}(w_i|w_{i-2}w_{i-1}) = {P(w_i|w_{i-2}w_{i-1}), c(w_{i-2}w_{i-1}w_i) > 0}​</script></li>
<li><script type="math/tex; mode=display">\hat {P}(w_i|w_{i-2}w_{i-1}) = {\alpha_1 P(w_i|w_{i-1}), c(w_{i-2}w_{i-1}w_i) = 0\ and\ c(w_{i-1}w_i) > 0}</script></li>
<li><script type="math/tex; mode=display">\hat {P}(w_i|w_{i-2}w_{i-1}) = {\alpha_2 P(w_i), other}</script></li>
<li>α 的目的是使等式的结果为真正的概率，保证：<script type="math/tex">\sum_{i,j} P(w_n|w_iw_j) = 1</script></li>
</ul>
</li>
<li><p>Jelinek-Mercer Smoothing (Deleted Interpolation)</p>
<ul>
<li>1980 年提出，使用线性插值把不同阶的语法结合起来，不同阶通过权值加权</li>
<li><script type="math/tex; mode=display">\hat {P}(w_n|w_{n-1}w_{n-2}) = {\lambda_1 P(w_n|w_{n-1}w_{n-2}) +\lambda_2 P(w_n|w_{n-1}) + \lambda_3 P(w_n)},\ \sum_i \lambda_i = 1</script></li>
<li>实际不仅仅只为三元语法训练三个 λ，还把每一个 λ 看成上下文的函数。</li>
</ul>
</li>
</ul>
<p>另外还有以下两种表现不错的平滑算法，可以参考：<a href="https://nbviewer.jupyter.org/github/hscspring/Note_NLP/blob/master/CMU-NeuralNetworksforNLP2017/02-lm/02-lmNote.ipynb" target="_blank" rel="noopener">LM</a></p>
<ul>
<li>Absolute Discounting: Discounting of the counts for frequent N-grams is necessary to save some probability mass for the smoothing algorithm to distribute to the unseen N-grams.</li>
<li>Kneser-Ney Smoothing: Augments absolute discounting with a more sophisticated way to handle the lower-order unigram distribution.</li>
</ul>
<p>Example（语料中共有 1616 个不同的 Type），假设咱们需要平滑下面 Bi-gram：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原始统计表</th>
<th>I</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>Chinese</th>
<th>food</th>
<th>lunch</th>
<th>…</th>
<th>Nw</th>
<th>Tw</th>
<th>Zw</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>I</strong></td>
<td>8</td>
<td>1087</td>
<td>0</td>
<td>12</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td></td>
<td>3437</td>
<td>95</td>
<td>1521</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>3</td>
<td>0</td>
<td>786</td>
<td>0</td>
<td>6</td>
<td>8</td>
<td>6</td>
<td></td>
<td>1215</td>
<td>76</td>
<td>1540</td>
</tr>
<tr>
<td><strong>to</strong></td>
<td>3</td>
<td>0</td>
<td>10</td>
<td>860</td>
<td>3</td>
<td>0</td>
<td>12</td>
<td></td>
<td>3256</td>
<td>130</td>
<td>1486</td>
</tr>
<tr>
<td><strong>eat</strong></td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>19</td>
<td>2</td>
<td>52</td>
<td></td>
<td>938</td>
<td>124</td>
<td>1492</td>
</tr>
<tr>
<td><strong>Chinese</strong></td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>120</td>
<td>1</td>
<td></td>
<td>213</td>
<td>20</td>
<td>1592</td>
</tr>
<tr>
<td><strong>food</strong></td>
<td>19</td>
<td>0</td>
<td>17</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td></td>
<td>1506</td>
<td>82</td>
<td>534</td>
</tr>
<tr>
<td><strong>lunch</strong></td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td></td>
<td>459</td>
<td>45</td>
<td>1571</td>
</tr>
<tr>
<td>…</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>原始 prob</th>
<th>I</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>Chinese</th>
<th>food</th>
<th>lunch</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>I</strong></td>
<td>.0023</td>
<td>.32</td>
<td>0</td>
<td>.0038</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>.0025</td>
<td>0</td>
<td>.65</td>
<td>0</td>
<td>.0049</td>
<td>.0066</td>
<td>.0049</td>
</tr>
<tr>
<td><strong>to</strong></td>
<td>.00092</td>
<td>0</td>
<td>.0031</td>
<td>.26</td>
<td>.00092</td>
<td>0</td>
<td>.0037</td>
</tr>
<tr>
<td><strong>eat</strong></td>
<td>0</td>
<td>0</td>
<td>.0021</td>
<td>0</td>
<td>.02</td>
<td>.0021</td>
<td>.055</td>
</tr>
<tr>
<td><strong>Chinese</strong></td>
<td>.0094</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>.56</td>
<td>.0047</td>
</tr>
<tr>
<td><strong>food</strong></td>
<td>.013</td>
<td>0</td>
<td>.011</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td><strong>lunch</strong></td>
<td>.0087</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>.0022</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Add-One</li>
</ul>
<p>计算 Bi-gram 的 probability：<script type="math/tex">P(w_n|w_{n-1}) = (C(w_{n-1} w_n)+1)/(C(w_n)+V)</script></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>加一 prob</th>
<th>I</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>Chinese</th>
<th>food</th>
<th>lunch</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>I</strong></td>
<td>.0018</td>
<td>.22</td>
<td>.0002</td>
<td>.0028</td>
<td>.0002</td>
<td>.0002</td>
<td>.0002</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>.0014</td>
<td>.00035</td>
<td>.28</td>
<td>.00035</td>
<td>.0025</td>
<td>.0032</td>
<td>.0025</td>
</tr>
<tr>
<td><strong>to</strong></td>
<td>.00082</td>
<td>.00021</td>
<td>.0023</td>
<td>.18</td>
<td>.00082</td>
<td>.00021</td>
<td>.0027</td>
</tr>
<tr>
<td><strong>eat</strong></td>
<td>.00039</td>
<td>.00039</td>
<td>.0012</td>
<td>.00039</td>
<td>.0078</td>
<td>.0012</td>
<td>.021</td>
</tr>
<tr>
<td><strong>Chinese</strong></td>
<td>.0016</td>
<td>.00055</td>
<td>.00055</td>
<td>.00055</td>
<td>.00055</td>
<td>.066</td>
<td>.0011</td>
</tr>
<tr>
<td><strong>food</strong></td>
<td>.0064</td>
<td>.00032</td>
<td>.0058</td>
<td>.00032</td>
<td>.00032</td>
<td>.00032</td>
<td>.00032</td>
</tr>
<tr>
<td><strong>lunch</strong></td>
<td>.0024</td>
<td>.00048</td>
<td>.00048</td>
<td>.00048</td>
<td>.00048</td>
<td>.00096</td>
<td>.00048</td>
</tr>
</tbody>
</table>
</div>
<p>加一平滑后的数量：使用上面的 prob × C(Wn)</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>加一 Smooth</th>
<th>I</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>Chinese</th>
<th>food</th>
<th>lunch</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>I</strong></td>
<td>6</td>
<td>740</td>
<td>.68</td>
<td>8.84</td>
<td>.68</td>
<td>.68</td>
<td>.68</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>2</td>
<td>.42</td>
<td>331</td>
<td>.42</td>
<td>3</td>
<td>4</td>
<td>3</td>
</tr>
<tr>
<td><strong>to</strong></td>
<td>3</td>
<td>.69</td>
<td>8</td>
<td>594</td>
<td>3</td>
<td>.69</td>
<td>9</td>
</tr>
<tr>
<td><strong>eat</strong></td>
<td>.37</td>
<td>.37</td>
<td>1</td>
<td>.37</td>
<td>7.4</td>
<td>1</td>
<td>20</td>
</tr>
<tr>
<td><strong>Chinese</strong></td>
<td>.36</td>
<td>.12</td>
<td>.12</td>
<td>.12</td>
<td>.12</td>
<td>15</td>
<td>.24</td>
</tr>
<tr>
<td><strong>food</strong></td>
<td>10</td>
<td>.48</td>
<td>9</td>
<td>.48</td>
<td>.48</td>
<td>.48</td>
<td>.48</td>
</tr>
<tr>
<td><strong>lunch</strong></td>
<td>1.1</td>
<td>.22</td>
<td>.22</td>
<td>.22</td>
<td>.22</td>
<td>.44</td>
<td>.22</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>Witten-Bell</li>
</ul>
<p>根据 T/(N+T)Z 和 Ci/(N+T) 分别计算为 0 和不为 0 概率，如：95/((3437+95) × 1521)，8/(3437+95)。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>WB prob</th>
<th>I</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>Chinese</th>
<th>food</th>
<th>lunch</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>I</strong></td>
<td>.0023</td>
<td>.31</td>
<td>.0000177</td>
<td>.0037</td>
<td>.0000177</td>
<td>.0000177</td>
<td>.0000177</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>.0023</td>
<td>.000038</td>
<td>.61</td>
<td>.000038</td>
<td>.0046</td>
<td>.0062</td>
<td>.0046</td>
</tr>
<tr>
<td><strong>…</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>WB Smooth</th>
<th>I</th>
<th>want</th>
<th>to</th>
<th>eat</th>
<th>Chinese</th>
<th>food</th>
<th>lunch</th>
<th>…</th>
<th>Nw</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>I</strong></td>
<td>7.78</td>
<td>1057.76</td>
<td>.061</td>
<td>12.65</td>
<td>.06</td>
<td>.06</td>
<td>.06</td>
<td></td>
<td>3437</td>
</tr>
<tr>
<td><strong>want</strong></td>
<td>2.82</td>
<td>.046</td>
<td>739.73</td>
<td>.046</td>
<td>5.65</td>
<td>7.53</td>
<td>5.65</td>
<td></td>
<td>1215</td>
</tr>
<tr>
<td><strong>…</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>以上具体计算可以参考代码：<a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/Ngram/Smoothing.ipynb" target="_blank" rel="noopener">Smoothing</a></p>
<p>Ngram 可以用于处理上下文有关的错误，基本思想是：对于句子中的每个单词生成它的一切可能的错误拼写，或者是只包括排版印刷错误而造成的错拼，或者是也包括同音词造成的错拼，然后选出使该句子具有最高先验概率的拼写。此外还有 Bayes 分类法、Bayes 分类法与三元语法结合、判定表方法、基于转换的学习方法、潜在语义分析法、筛选算法（效果最好）。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul>
<li>N 元语法<ul>
<li>一个单词的概率只依赖于它前面一个单词的这种假设叫作 Markov 假设，这样的模型叫 Bi-gram，即二元语法模型，也叫一阶 Markov 模型。</li>
<li>N 增加时，精确度相应增加，同时生成句子的局限性增加（可选的下个词减少）；严重依赖于语料库。</li>
</ul>
</li>
<li>数据平滑<ul>
<li>Add-One (Add-α)：简单，未登录词或低概率词会被给予过高的概率。</li>
<li>Witten-Bell：看一个零概率 N 元语法的概率可以用首次看一个 N 元语法的概率模拟。</li>
<li>Good-Turing：可以复杂，但简单修改也能工作的很好；仍然没有区分不同类型的罕见事件。</li>
<li>Katz Smoothing (Backoff)/Jelinek-Mercer Smoothing (Deleted Interpolation)：利用对应的 N-1 元或更低元的语法（结合不同阶）来计算；前者只有当阶数较高的 N 元语法中存在零计数时，才把阶数较高的 N  元语法降为阶数较低的 N 元语法。。</li>
<li>Absolute Discounting/Kneser-Ney Smoothing：前者对 N 元语法计数进行绝对折扣；后者假设过去在更多情境中出现的词语更有可能出现在某些新的语境中。</li>
</ul>
</li>
<li><p>具体表现（参考自：<a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf" target="_blank" rel="noopener">moothing-tutorialf</a>）</p>
<ul>
<li>Jelinek-Mercer 在小型训练集上表现更好； Katz 在大型训练集上表现更好。</li>
<li>Katz 平滑对大数量的 N-gram 表现良好； Kneser-Ney 最适合小数量。</li>
<li>Absolute Discounting 优于 Linear Discounting。</li>
<li>在低（非零）计数的 Ngram 上，Jelinek-Mercer 优于 Katz。</li>
</ul>
</li>
</ul>
<p>这章虽然是非常简单的 Ngram 语法，但 Smoothing 的各种算法却非常有意思，从中可以深刻地感受到对一个简单问题处理的智慧，我想这可能也是算法的魅力吧。关于 Witten-Bell Smoothing 找了好久才找到一个容易理解的资料（参考文献 2），更多关于 N-gram 和 Smoothing 可以参阅：<a href="https://nbviewer.jupyter.org/github/hscspring/Note_NLP/blob/master/CMU-NeuralNetworksforNLP2017/02-lm/02-lmNote.ipynb" target="_blank" rel="noopener">LM</a></p>
<p><strong>参考文献</strong>：</p>
<ul>
<li><a href="https://gawron.sdsu.edu/compling/course_core/lectures/smoothing.htm" target="_blank" rel="noopener">Computational Linguistics</a> 加一平滑的例子不错。</li>
<li><a href="http://gki.informatik.uni-freiburg.de/teaching/ws0607/advanced/lecture.html" target="_blank" rel="noopener">Foundations of Artificial Intelligence · Advanced AI Techniques - Lectures</a> 参考了 7b 的平滑部分，强烈推荐，2006 年的课件清晰度（Witten-Bell）超过了网上能找到的任何一个教程或博客。</li>
<li><a href="https://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf" target="_blank" rel="noopener">20050421-smoothing-tutorial.pdf</a> 和 <a href="https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe6.pdf" target="_blank" rel="noopener">scribe6.pdf</a> 比较全面介绍了各种平滑算法的思想。</li>
</ul>
<p>几个不错的课件：</p>
<ul>
<li><a href="http://l2r.cs.uiuc.edu/~danr/Teaching/CS546-09/Lectures/Lec5-Stat-09-ext.pdf" target="_blank" rel="noopener">CS546:Learning and NLP Lec 6: Ngrams and Backoff Models</a></li>
<li><a href="https://www.cs.jhu.edu/~jason/665/PDFSlides/lect05-smoothing.pdf" target="_blank" rel="noopener">lect05-smoothing.pdf</a></li>
<li><a href="http://www.cs.virginia.edu/~kc2wc/teaching/NLP16/slides/03-smooth.pdf" target="_blank" rel="noopener">lect03-smooth</a></li>
<li><a href="http://www.cs.brandeis.edu/~cs136a/CS136a_Slides/CS136a_Lect11_PerplexityAndSmoothing.pdf" target="_blank" rel="noopener">CS136a_Lect11_PerplexityAndSmoothing</a></li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/03/15/NLPFA/2019-03-15-Ch13-Ngram-and-Smoothing/">
    <time datetime="2019-03-15T03:32:00.000Z" class="entry-date">
        2019-03-15
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Ngram/">Ngram</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Smoothing/">Smoothing</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2019/03/22/NLPFA/2019-03-22-Ch14-HMM/" rel="prev"><span class="meta-nav">←</span> 自然语言计算机形式分析的理论与方法笔记(Ch14)</a></span>
    
    
        <span class="nav-next"><a href="/2019/03/11/NLPFA/2019-03-11-Ch12-Bayes-and-Dynamic-Programming/" rel="next">自然语言计算机形式分析的理论与方法笔记(Ch12) <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">9</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2019/04/13/NLPFA/2019-04-13-NLP-Formal-Analysis/">自然语言计算机形式分析的理论与方法笔记</a>
          </li>
        
          <li>
            <a href="/2019/04/09/2019-04-09-Information-Extraction-Note/">Information Extraction Note</a>
          </li>
        
          <li>
            <a href="/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/">自然语言计算机形式分析的理论与方法笔记(Ch18)</a>
          </li>
        
          <li>
            <a href="/2019/03/31/2019-03-31-Nabokov-Favorite-Word/">《纳博科夫最喜欢的词》读书笔记与思考</a>
          </li>
        
          <li>
            <a href="/2019/03/29/NLPFA/2019-03-29-Ch15-Formal-Model-of-Automatic-Speech-Processing/">自然语言计算机形式分析的理论与方法笔记(Ch15)</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a><span class="tag-list-count">30</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backward/">Backward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Business/">Business</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calculus/">Calculus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer-Science/">Computer Science</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Clearing/">Data Clearing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Science/">Data Science</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Structure/">Data Structure</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/">DeepLearning</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependence/">Dependence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Diary/">Diary</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Economics/">Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/">Entropy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Evaluation/">Evaluation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FSM/">FSM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Analysis/">Formal Analysis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forward/">Forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gan/">Gan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HMM/">HMM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IE/">IE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Industry/">Industry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Extraction/">Information Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalism/">Lexicalism</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Algebra/">Linear Algebra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine/">Machine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markov/">Markov</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">25</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ngram/">Ngram</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Orientation/">Orientation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simon/">Simon</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simpson-Paradox/">Simpson Paradox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smoothing/">Smoothing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sort/">Sort</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spell-Check/">Spell Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Style/">Style</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/">System</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valence/">Valence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VirtualBox/">VirtualBox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Viterbi/">Viterbi</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZhouZhihua/">ZhouZhihua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zipf/">Zipf</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AI/" style="font-size: 20px;">AI</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Business/" style="font-size: 10px;">Business</a> <a href="/tags/C/" style="font-size: 12px;">C</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer-Science/" style="font-size: 14px;">Computer Science</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Science/" style="font-size: 14px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 14px;">Data Structure</a> <a href="/tags/DeepLearning/" style="font-size: 14px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 10px;">Diary</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Entropy/" style="font-size: 10px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10px;">Evaluation</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Formal-Analysis/" style="font-size: 10px;">Formal Analysis</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 10px;">HMM</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Math/" style="font-size: 12px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/NLP/" style="font-size: 18px;">NLP</a> <a href="/tags/Ngram/" style="font-size: 10px;">Ngram</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Philosophy/" style="font-size: 12px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Python/" style="font-size: 16px;">Python</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Smoothing/" style="font-size: 10px;">Smoothing</a> <a href="/tags/Sort/" style="font-size: 10px;">Sort</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/System/" style="font-size: 12px;">System</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10px;">Viterbi</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2019 Yam
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>