<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>Part-of-Speech Tagging Note (SLP Ch08) | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Parts-of-speech (also known as POS, word classes, or syntactic categories) are useful because they reveal a lot about a word and its neighbors. Useful for:   labeling named entities  coreference resol">
<meta name="keywords" content="AI,NLP,PoS,Tagging,HMM,Viterbi,Beam Search,Decoding,MEMM">
<meta property="og:type" content="article">
<meta property="og:title" content="Part-of-Speech Tagging Note (SLP Ch08)">
<meta property="og:url" content="https://www.yam.gift/2019/06/11/NLP/SLP/2019-06-11-Ch08-Part-of-Speech-Tagging/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Parts-of-speech (also known as POS, word classes, or syntactic categories) are useful because they reveal a lot about a word and its neighbors. Useful for:   labeling named entities  coreference resol">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch8-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch8-2.jpeg">
<meta property="og:updated_time" content="2020-05-20T13:36:46.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Part-of-Speech Tagging Note (SLP Ch08)">
<meta name="twitter:description" content="Parts-of-speech (also known as POS, word classes, or syntactic categories) are useful because they reveal a lot about a word and its neighbors. Useful for:   labeling named entities  coreference resol">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/slp-ch8-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-NLP/SLP/2019-06-11-Ch08-Part-of-Speech-Tagging" class="post-NLP/SLP/2019-06-11-Ch08-Part-of-Speech-Tagging post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      Part-of-Speech Tagging Note (SLP Ch08)
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2019/06/11/NLP/SLP/2019-06-11-Ch08-Part-of-Speech-Tagging/" data-id="cl7yw7chg00yuxybzkqfg1geg" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>Parts-of-speech (also known as <strong>POS</strong>, <strong>word classes</strong>, or <strong>syntactic categories</strong>) are useful because they reveal a lot about a word and its neighbors. Useful for: </p>
<ul>
<li><p>labeling <strong>named entities</strong></p>
</li>
<li><p>coreference resolution</p>
</li>
<li>speech recognition or synthesis</li>
</ul>
<a id="more"></a>
<h2 id="English-Word-Classes"><a href="#English-Word-Classes" class="headerlink" title="English Word Classes"></a>English Word Classes</h2><p>Parts-of-speech can be divided into two broad supercategories: </p>
<ul>
<li><strong>closed class</strong> : with relatively fixed membership, generally <strong>function words</strong>. The closed classes differ more from language to language than do the open classes<ul>
<li><strong>prepositions</strong><ul>
<li>occur before noun phrases</li>
<li>on, under, over, near, by, at, from, to, with</li>
</ul>
</li>
<li><strong>particles</strong><ul>
<li>resemble a preposition or an adverb and is used in combination with a verb</li>
<li>like up, down, on, off, in, out, at, by</li>
<li>A verb and a particle that act as a single syntactic and/or semantic unit are called a <strong>phrasal verb</strong>, the meaning is <strong>non-compositional</strong> (not predictable from the distinct meanings)</li>
</ul>
</li>
<li><strong>determiners</strong><ul>
<li>occurs with nouns, often marking the beginning of a noun phrase</li>
<li>a, an, the, this, that</li>
<li>One small subtype of determiners is the <strong>article</strong>: English has three articles: a, an, and the</li>
</ul>
</li>
<li><strong>conjunctions</strong><ul>
<li>join two phrases, clauses, or sentences</li>
<li>and, but, or, as, if, when</li>
<li>Subordinating conjunctions like <em>that</em> which link a verb to its argument are also called <strong>complementizers</strong></li>
</ul>
</li>
<li><strong>pronouns</strong><ul>
<li>often act as a kind of shorthand for referring to some noun phrase or entity or event</li>
<li>she, who, I, others</li>
<li><strong>Personal pronouns</strong> refer to persons or entities</li>
<li><strong>Possessive pronouns</strong> are forms of personal pronouns that indicate either actual possession or more often just an abstract relation between the person and some object</li>
<li><strong>Wh-pronouns</strong> are used in certain question forms, or may also act as complementizers</li>
</ul>
</li>
<li><strong>auxiliary verbs</strong><ul>
<li>mark semantic features of a main verb</li>
<li>can, may, should, are</li>
<li><strong>copula</strong> verb be</li>
<li><strong>modal</strong> verbs do and have</li>
</ul>
</li>
<li><strong>numerals</strong> one, two, three, first, second, third</li>
<li><strong>interjections</strong> (oh, hey, alas, uh, um), <strong>negatives</strong> (no, not), <strong>politeness markers</strong> (please, thank you), <strong>greetings</strong> (hello, goodbye), and the existential <strong>there</strong> (there are two on the table) among others</li>
</ul>
</li>
<li><strong>open class</strong>: new nouns and verbs are continually being borrowed or created. 4 major in English:<ul>
<li><strong>Noun</strong> have two classes: <strong>Proper nouns</strong> (names of specific persons or entities) and <strong>common nouns</strong> (<strong>count nouns</strong> and <strong>mass nouns</strong>)</li>
<li><strong>Verbs</strong> refer to actions and processes, English verbs have inflections</li>
<li><strong>Adjective</strong> includes many terms for properties or qualities</li>
<li><strong>Adverb</strong> , is rather a hodge-podge in both form and meaning, each can be viewed as modifying something<ul>
<li><strong>Directional adverbs</strong> or <strong>locative adverbs</strong> (home, here, downhill) specify the direction or location of some action;</li>
<li><strong>Degree adverbs</strong> (extremely, very, somewhat) specify the extent of some action, process, or property;</li>
<li><strong>Manner adverbs</strong> (slowly, slinkily, delicately) describe the manner of some action or process</li>
<li><strong>Temporal adverbs</strong> describe the time that some action or event took place (yesterday, Monday)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="The-Penn-Treebank-Part-of-Speech-Tagset"><a href="#The-Penn-Treebank-Part-of-Speech-Tagset" class="headerlink" title="The Penn Treebank Part-of-Speech Tagset"></a>The Penn Treebank Part-of-Speech Tagset</h2><p><img src="http://qnimg.lovevivian.cn/slp-ch8-1.jpeg" alt=""></p>
<p>main tagged corpora of English</p>
<ul>
<li><strong>Brown</strong>: a million words of samples from 500 written texts from different genres published in the United States in 1961</li>
<li><strong>WSJ</strong>: a million words published in the WSJ in 1989</li>
<li><strong>Switchboard</strong>: 2 million words of telephone conversations collected in 1990-1991</li>
</ul>
<h2 id="Part-of-Speech-Tagging"><a href="#Part-of-Speech-Tagging" class="headerlink" title="Part-of-Speech Tagging"></a>Part-of-Speech Tagging</h2><p><strong>Part-of-speech tagging</strong> is a <strong>disambiguation</strong> task, words are <strong>ambiguous</strong>. Some of the most ambiguous frequent words are that, back, down, put and set.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch8-2.jpeg" alt=""></p>
<p><strong>Most Frequent Class Baseline</strong>: Always compare a classifier against a baseline at least as good as the most frequent class baseline (assigning each token to the class it occurred in most often in the training set).</p>
<h2 id="HMM-Part-of-Speech-Tagging"><a href="#HMM-Part-of-Speech-Tagging" class="headerlink" title="HMM Part-of-Speech Tagging"></a>HMM Part-of-Speech Tagging</h2><p>The HMM is a sequence model. </p>
<h3 id="Markov-Chains"><a href="#Markov-Chains" class="headerlink" title="Markov Chains"></a>Markov Chains</h3><p>A <strong>Markov chain</strong> is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. </p>
<p><strong>Markov Assumption</strong>: <script type="math/tex">P(q_i=a|q_1…q_{i-1}) = P(q_i=a|q_{i-1})</script></p>
<p>Formally, a Markov chain is specified by: </p>
<ul>
<li>a set of N <strong>states</strong>: <script type="math/tex">Q=q_1q_2…q_N</script></li>
<li>a <strong>transition probability matrix</strong> A, each aij representing the probability of moving from state i to state j: <script type="math/tex">A = a_{11}a_{12}…a_{n1}…a_{nn}, \ \ s.t.\ \sum_{j=1}^N a_{ij} = 1\ \ \forall i</script></li>
<li>an <strong>initial probability distribution</strong> over states, πi is the probability that the Markov chain will start in i: <script type="math/tex">\pi = \pi_1, \pi_2, …, \pi_N\ \ \ \sum_{i=1}^n \pi_i = 1</script></li>
</ul>
<h3 id="The-Hidden-Markov-Model"><a href="#The-Hidden-Markov-Model" class="headerlink" title="The Hidden Markov Model"></a>The Hidden Markov Model</h3><p>A <strong>hidden Markov model (HMM)</strong> is specified by:</p>
<ul>
<li>a set of N <strong>states</strong>: <script type="math/tex">Q=q_1q_2…q_N</script></li>
<li>a <strong>transition probability matrix</strong> A, each aij representing the probability of moving from state i to state j: <script type="math/tex">A = a_{11}…a_{ij}…a_{nn}, \ \ s.t.\ \sum_{j=1}^N a_{ij} = 1\ \ \forall i</script></li>
<li>a sequence of T <strong>observations</strong>, each one drawn from a vocabulary <script type="math/tex">V = v_1,v_2,…,v_V</script></li>
<li>a sequence of <strong>observation likelihoods</strong>, also called <strong>emission probabilities</strong>, each expressing the probability of an observation ot being generated from a state i: <script type="math/tex">B=b_i(o_t)</script></li>
<li>an <strong>initial probability distribution</strong> over states, πi is the probability that the Markov chain will start in i: <script type="math/tex">\pi = \pi_1, \pi_2, …, \pi_N\ \ \ \sum_{i=1}^n \pi_i = 1</script></li>
</ul>
<p>A first-order HMM instantiates two simplifying assumptions:</p>
<ul>
<li><strong>Markov Assumption</strong>: <script type="math/tex">P(q_i=a|q_1…q_{i-1}) = P(q_i=a|q_{i-1})</script></li>
<li><strong>Output Independence</strong>: <script type="math/tex">P(o_i|q_1…q_i,…,q_T,o_1,…,o_i,…,o_T) = P(o_i|q_i)</script></li>
</ul>
<h3 id="The-components-of-an-HMM-tagger"><a href="#The-components-of-an-HMM-tagger" class="headerlink" title="The components of an HMM tagger"></a>The components of an HMM tagger</h3><p>A matrix represent the probability of a tag occurring given the previous tag: </p>
<script type="math/tex; mode=display">P(t_i|t_{i-1}) = \frac {C(t_{i-1}, t_i)}{C(t_{i-1})}</script><p>B emission probabilities represent the probability given a tag that it will be associated with a given word:</p>
<script type="math/tex; mode=display">P(w_i|t_i) = \frac{C(t_i, w_i)}{C(t_i)}</script><h3 id="HMM-tagging-as-decoding"><a href="#HMM-tagging-as-decoding" class="headerlink" title="HMM tagging as decoding"></a>HMM tagging as decoding</h3><p>The task of determining the hidden variables sequence corresponding to the sequence of observations is called <strong>decoding</strong>: Given as input an HMM λ=(A,B) and a sequence of observations O, find the most probable sequence of states Q.</p>
<p>The goal of HMM decoding is to choose the tag sequence that is most probable given the observation sequence of n words:</p>
<script type="math/tex; mode=display">\hat t_1^n = \arg \max_{t_1^n} P(t_1^n|w_1^n)</script><script type="math/tex; mode=display">\hat t_1^n = \arg \max_{t_1^n} \frac {P(w_1^n|t_1^n) P(t_1^n)}{P(w_1^n)}</script><script type="math/tex; mode=display">\hat t_1^n = \arg \max_{t_1^n}{P(w_1^n|t_1^n) P(t_1^n)}</script><p>HMM taggers make two further simplifying assumptions:</p>
<ul>
<li>the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags: <script type="math/tex">P(w_1^n|t_1^n) ≈ \prod_{i=1}^n P(w_i|t_i)</script></li>
<li>bigram assumption, the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence: <script type="math/tex">P(t_1^n) ≈ \prod_{i=1}^n P(t_i|t_{i-1})</script></li>
</ul>
<script type="math/tex; mode=display">\hat t_1^n = \arg \max_{t_1^n} \prod_{i=1}^n P(w_i|t_i) P(t_i|t_{i-1})</script><ul>
<li>B emission probability: <script type="math/tex">P(w_i|t_i)</script></li>
<li>A transition probability: <script type="math/tex">P(t_i|t_{i-1})</script></li>
</ul>
<h3 id="The-Viterbi-Algorithm"><a href="#The-Viterbi-Algorithm" class="headerlink" title="The Viterbi Algorithm"></a>The Viterbi Algorithm</h3><p>As an instance of <strong>dynamic programming</strong>, Viterbi resembles the <strong>minimum edit distance</strong> algorithm.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">function VITERBI(observations of len T, state-graph of len N) returns best-path, path-prob</span><br><span class="line"></span><br><span class="line">create a path probability matrix viterbi[N, T]</span><br><span class="line"><span class="keyword">for</span> each state s <span class="keyword">from</span> <span class="number">1</span> to N do</span><br><span class="line">	viterbi[s, <span class="number">1</span>] ← πs * bs(o_1)</span><br><span class="line">    backpointer[s, <span class="number">1</span>] ← <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> each time step t <span class="keyword">from</span> <span class="number">2</span> to T do</span><br><span class="line">	<span class="keyword">for</span> each state s <span class="keyword">from</span> <span class="number">1</span> to N do</span><br><span class="line">    	viterbi[s,t] ← max viterbi[s<span class="string">", t-1] * a_&#123;s"</span>,s&#125; * b_s(o_t)</span><br><span class="line">        backpointer[s,t] ← argmax viterbi[s<span class="string">", t-1] * a_&#123;s"</span>,s&#125; * b_s(o_t)</span><br><span class="line">bestpathprob ← max viterbi[s, T]</span><br><span class="line">bestpathpointer ← argmax viterbi[s, T]</span><br><span class="line">bestpath ← the path starting at state bestpathpointer, that follows backpointer[] to states back <span class="keyword">in</span> time</span><br><span class="line"><span class="keyword">return</span> bestpath, bestpathprob</span><br></pre></td></tr></table></figure>
<p>The code is <a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/Decoding/Viterbi.ipynb" target="_blank" rel="noopener">here</a>.</p>
<script type="math/tex; mode=display">v_t(j) = \max_{q_1,…,q_{t-1}} P(q_1,…,q_{t-1},o_1,…o_t, q_t=j|\lambda)</script><script type="math/tex; mode=display">v_t(j) = \max_{i=1}^n v_{t-1}(i) a_{ij} b_j(o_t)</script><ul>
<li>the <strong>previous Viterbi path probability</strong> from the previous time step: <script type="math/tex">v_{t-1}(i)</script></li>
<li>the <strong>transition probability</strong> from previous state <code>qi</code> to current state <code>qj</code>: <script type="math/tex">a_{ij}</script></li>
<li>the <strong>state observation likelihood</strong> of the observation symbol <code>ot</code> given the current state j: <script type="math/tex">b_j(o_t)</script></li>
</ul>
<h3 id="Expanding-the-HMM-Algorithm-to-Trigrams"><a href="#Expanding-the-HMM-Algorithm-to-Trigrams" class="headerlink" title="Expanding the HMM Algorithm to Trigrams"></a>Expanding the HMM Algorithm to Trigrams</h3><p>In practice we use the two previous tags:</p>
<script type="math/tex; mode=display">P(t_1^n) ≈ \prod_{i=1}^n P(t_i|t_{i-1}, t_{i-2})</script><p>That increases a little performance but makes the Viterbi complexity from N hidden states to N×N.</p>
<p>One advanced feature is adding an end-of-sequence marker:</p>
<script type="math/tex; mode=display">\hat t_1^n = \arg \max_{t_1^n} P(t_1^n|w_1^n) = \arg \max_{t_1^n} [\prod_{i=1}^n P(w_i|t_i) P(t_i|t_{i-1}, t_{i=2})] P(t_{n+1}|t_n)</script><p>One problem is sparsity, the standard approach to solving this problem is the same interpolation idea<br>we saw in language modeling smoothing:</p>
<script type="math/tex; mode=display">P(t_i|t_{i-1}t_{i-2}) = \lambda_3 \hat P(t_i|t_{i-1}t_{i-2}) + \lambda_2 \hat P(t_i|t_{i-1}) + \lambda_1 \hat P(t_i)</script><p>λ1 + λ2 + λ3 = 1, λs are set by <strong>deleted interpolation</strong>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">function DELETED-INTERPOLAtiON(corpus) returns λ<span class="number">1</span>, λ<span class="number">2</span>, λ<span class="number">3</span></span><br><span class="line">	λ<span class="number">1</span>, λ<span class="number">2</span>, λ<span class="number">3</span> ← <span class="number">0</span></span><br><span class="line">    foreach trigram t1, t2, t3 <span class="keyword">with</span> C(t1, t2, t3) &gt; <span class="number">0</span></span><br><span class="line">    	depending on the maximum of the following three values</span><br><span class="line">        	case (C(t1,t2,t3)<span class="number">-1</span>)/(C(t1,t2)<span class="number">-1</span>): increment λ<span class="number">3</span> by C(t1,t2,t3)</span><br><span class="line">            case (C(t2,t3)<span class="number">-1</span>)/(C(t2)<span class="number">-1</span>): increment λ<span class="number">2</span> by C(t2,t3)</span><br><span class="line">            case (C(t3)<span class="number">-1</span>)/(N<span class="number">-1</span>): increment λ<span class="number">1</span> by C(t3)</span><br><span class="line">        end</span><br><span class="line">    end</span><br><span class="line">    normalize λ<span class="number">1</span>, λ<span class="number">2</span>, λ<span class="number">3</span></span><br><span class="line">    <span class="keyword">return</span> λ<span class="number">1</span>, λ<span class="number">2</span>, λ<span class="number">3</span></span><br></pre></td></tr></table></figure>
<h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><p>One common solution to the complexity of Viterbi is the use of <strong>beam search</strong> decoding: just keep the best few hypothesis at time point t. <strong>Beam width</strong> β is a fixed number of states.</p>
<p>The code is <a href="https://nbviewer.jupyter.org/github/hscspring/All4NLP/blob/master/Decoding/BeamSearch.ipynb" target="_blank" rel="noopener">here</a>.</p>
<h3 id="Unknown-Words"><a href="#Unknown-Words" class="headerlink" title="Unknown Words"></a>Unknown Words</h3><p><strong>Unknown words</strong> often new common nous and verbs, one useful feature for distinguishing parts of speech if word shape: words starting with capital letters are likely to be proper nouns (NNP). Morphology is the strongest source of information, we are computing for each suffix of length i the probability of the tag ti given the suffix letters <code>(P(t_i|l_{n-i+1}...l_n)</code>, then use Bayesian inversion to compute the likelihood p(wi|ti).</p>
<blockquote>
<p>It is not suitable for Chinese. In Chinese, a name is often starting with a surname, but it’s not helpful for some cases. For example, the surname “王”, there are many common words starting with it: “王子 王位 王八 王八蛋…”, we can not distinguish them without their context. So most time we have use a model or a big dict to solve the problem.</p>
</blockquote>
<h2 id="Maximum-Entropy-Markov-Models"><a href="#Maximum-Entropy-Markov-Models" class="headerlink" title="Maximum Entropy Markov Models"></a>Maximum Entropy Markov Models</h2><p><strong>MEMM</strong> is <strong>maximum entropy Markov model</strong>, compute the posterior P(T|W) directly, training it to discriminate among the possible tag sequences:</p>
<script type="math/tex; mode=display">\hat T = \arg \max_T P(T|W) = \arg \max_T \prod_i P(t_i|w_i, t_{i-1})</script><p>HMMs compute likelihood (observation word conditioned on tags) but MEMMs compute posterior<br>(tags conditioned on observation words).</p>
<h3 id="Features-in-a-MEMM"><a href="#Features-in-a-MEMM" class="headerlink" title="Features in a MEMM"></a>Features in a MEMM</h3><p>Using feature <strong>templates</strong> like: </p>
<script type="math/tex; mode=display"><t_i, w_{i+-k}>, <t_i, t_{i+-k}>, <t_i, t_{i-1}, w_i>, <t_i, w_{i+-k}, w_{i+-k+1}></script><p><code>Janet/NNP will/MD back/VB the/DT bill/NN</code>, when wi is the word back, features are:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ti = VB and wi−2 = Janet</span><br><span class="line">ti = VB and wi−1 = will</span><br><span class="line">ti = VB and wi = back</span><br><span class="line">ti = VB and wi+1 = the</span><br><span class="line">ti = VB and wi+2 = bill</span><br><span class="line">ti = VB and ti−1 = MD</span><br><span class="line">ti = VB and ti−1 = MD and ti−2 = NNP</span><br><span class="line">ti = VB and wi = back and wi+1 = the</span><br></pre></td></tr></table></figure>
<p>Also necessary are features to deal with unknown words:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">wi contains a particular prefix (from all prefixes of length ≤ 4)</span><br><span class="line">wi contains a particular suffix (from all suffixes of length ≤ 4)</span><br><span class="line">wi contains a number</span><br><span class="line">wi contains an upper-case letter</span><br><span class="line">wi contains a hyphen</span><br><span class="line">wi is all upper case</span><br><span class="line">wi’s word shape</span><br><span class="line">wi’s short word shape</span><br><span class="line">wi is upper case and has a digit and a dash (like CFC-12)</span><br><span class="line">wi is upper case and followed within 3 words by Co., Inc., etc.</span><br></pre></td></tr></table></figure>
<p><strong>Word shape</strong> features are used to represent the abstract letter pattern of the unknown words. For example the word well-dressed would generate the following non-zero valued feature values:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">prefix(wi) = w</span><br><span class="line">prefix(wi) = we</span><br><span class="line">prefix(wi) = wel</span><br><span class="line">prefix(wi) = well</span><br><span class="line">suffix(wi) = ssed</span><br><span class="line">suffix(wi) = sed</span><br><span class="line">suffix(wi) = ed</span><br><span class="line">suffix(wi) = d</span><br><span class="line">has-hyphen(wi)</span><br><span class="line">word-shape(wi) = xxxx-xxxxxxx</span><br><span class="line">short-word-shape(wi) = x-x</span><br></pre></td></tr></table></figure>
<p>The result of the known-word templates and word-signature features is a very large set of features.</p>
<p>Generally a feature cutoff is used in which features are thrown out if they have count &lt; 5 in the training set.</p>
<h3 id="Decoding-and-Training-MEMMs"><a href="#Decoding-and-Training-MEMMs" class="headerlink" title="Decoding and Training MEMMs"></a>Decoding and Training MEMMs</h3><p>Input word wi, features will be its neighbors within l words and the previous k tags:</p>
<script type="math/tex; mode=display">\hat T = \arg \max_T P(T|W) =  \arg \max_T \prod_i P(t_i|w_{i-l}^{i+l}, t_{i-k}^{i-1}) = \arg \max_T \prod_i \frac {exp \lgroup \sum_j \theta_j f_j (t_i, w_{i-l}^{i+l}, t_{i-k}^{i-1}) \rgroup} {\sum_{t' \in tagset} exp \lgroup \sum_j \theta_j f_j (t_i, w_{i-l}^{i+l}, t_{i-k}^{i-1}) \rgroup}</script><p><strong>Greedy</strong> decoding builds a local classifier that classifies each word left to right, making a hard classification of the first word in the sentence, then the second, and so on.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">function GREEDY SEQUENCE DECODING (words W, model P) returns tag sequence T</span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span> to length(W)</span><br><span class="line">	\hat &#123;t_i&#125; = argmax_t<span class="string">'∈T P(t'</span>|w,t) <span class="comment"># choose the best tag on each token</span></span><br></pre></td></tr></table></figure>
<p>The problem is the classifier can’t use evidence from future decisions. Instead Viterbi is optimal for the whole sentence.</p>
<ul>
<li>HMM: <script type="math/tex">v_t(j) = \max_{i=1}^N v_{t-1}(i) P(s_j|s_i) P(o_t|s_j)\ \ 1≤j≤N,\ 1≤t≤T</script></li>
<li>MEMM: <script type="math/tex">v_t(j) = \max_{i=1}^N v_{t-1}(i) P(s_j|s_i, o_t)\ \ 1≤j≤N,\ 1≤t≤T</script></li>
</ul>
<h2 id="Bidirectionality"><a href="#Bidirectionality" class="headerlink" title="Bidirectionality"></a>Bidirectionality</h2><p>One problem with the MEMM and HMM models is that they are exclusively run left-to-right. While the Viterbi still allows present decisions to be influenced indirectly by future decisions, it would help even more if a decision about word wi could directly use information about future tags <code>t_{i+k}</code>.</p>
<p>MEMMs also have a theoretical weakness, referred to alternatively as the <strong>label bias</strong> or <strong>observation bias</strong> problem. These are names for situations when one source of information is ignored because it is <strong>explained away</strong> by another source.</p>
<p>The model is <strong>conditional random field</strong> or <strong>CRF</strong>, which is an undirected graphical model, computes log-linear functions over a <strong>clique</strong> (a set of relevant features might include output features of words in future time steps) at each time step. The probability of the best sequence is similarly computed by the Viterbi.</p>
<p>CRF normalizes probabilities over all tag sequences, rather than all the tags at an individual time t, training requires computing the sum over all possible labelings, which makes training quite slow.</p>
<h2 id="Part-of-Speech-Tagging-for-Other-Languages"><a href="#Part-of-Speech-Tagging-for-Other-Languages" class="headerlink" title="Part-of-Speech Tagging for Other Languages"></a>Part-of-Speech Tagging for Other Languages</h2><p>While English unknown words tend to be proper nouns in Chinese the majority of unknown words are common nouns and verbs because of extensive compounding. Tagging models for Chinese use similar unknown word features to English, including character prefix and suffix features, as well as novel features like the <strong>radicals</strong> of each character in a word.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>A small set of <strong>closed class</strong> words that are highly frequent, ambiguous, act as <strong>function words</strong>; <strong>open-class</strong> words like <strong>nouns, verbs, adjectives</strong>. </li>
<li><strong>Part-of-speech tagging</strong> is the process of assigning a part-of-speech label to each word of a sequence.</li>
<li><p>Two common approaches to <strong>sequence modeling</strong> are a <strong>generative</strong> approach, <strong>HMM</strong> tagging, and a <strong>discriminative</strong> approach, <strong>MEMM</strong> tagging.</p>
</li>
<li><p>The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The <strong>Viterbi</strong> algorithm is used for <strong>decoding</strong>, finding the most likely tag sequence. <strong>Beam search</strong> maintains only a fraction of high scoring states rather than all states during decoding.</p>
</li>
<li><p><strong>Maximum entropy Markov model</strong> or <strong>MEMM</strong> taggers train logistic regression models to pick the best tag given an observation word and its context and the previous tags, and then use Viterbi to choose the best sequence of tags.</p>
</li>
<li>Modern taggers are generally run <strong>bidirectionally</strong>.</li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/06/11/NLP/SLP/2019-06-11-Ch08-Part-of-Speech-Tagging/">
    <time datetime="2019-06-11T04:11:00.000Z" class="entry-date">
        2019-06-11
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Beam-Search/">Beam Search</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Decoding/">Decoding</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HMM/">HMM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/MEMM/">MEMM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PoS/">PoS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Tagging/">Tagging</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Viterbi/">Viterbi</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2019/06/15/DB/2019-06-15-Common-DB-Related/" rel="prev"><span class="meta-nav">←</span> 常用 DataBase 相关操作和资源</a></span>
    
    
        <span class="nav-next"><a href="/2019/06/08/ExpSum/2019-06-08-Concept-and-View-of-Work/" rel="next">一些关于工作的观点（From《华为工作法》） <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">70</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">100</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">21</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2022/10/16/Paper/2022-10-16-GlobalPointer/">Global Pointer：Novel Efficient Span-based Approach for NER</a>
          </li>
        
          <li>
            <a href="/2022/10/15/Paper/2022-10-15-DeepGen/">DeepGen：Diverse Search Ad Generation and Real-Time Customization</a>
          </li>
        
          <li>
            <a href="/2022/09/11/Diary/2022-09-11-Passion/">只如初见的不只爱情</a>
          </li>
        
          <li>
            <a href="/2022/08/28/Paper/2022-08-28-FLAN/">FLAN：Fine-tuned Language Models are Zero-Shot Learners</a>
          </li>
        
          <li>
            <a href="/2022/07/17/Paper/2022-07-17-W2NER-Code/">W2NER 代码</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.17px;">AI</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Algorithm/" style="font-size: 13.33px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12.5px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 17.5px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/Binary-Search/" style="font-size: 11.67px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 11.67px;">Business</a> <a href="/tags/C/" style="font-size: 10.83px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.83px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.83px;">Classification</a> <a href="/tags/Cognition/" style="font-size: 10.83px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12.5px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.83px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DB/" style="font-size: 10.83px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 15px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 16.67px;">Data Structure</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12.5px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 12.5px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dropout/" style="font-size: 10.83px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.83px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.67px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.83px;">Embeddings</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 10.83px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.83px;">Evaluation</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 10px;">Few-Shot</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.67px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-2/" style="font-size: 10px;">GPT-2</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.83px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.83px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 10px;">Growth</a> <a href="/tags/HMM/" style="font-size: 10.83px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.83px;">Knowledge Graph</a> <a href="/tags/LM/" style="font-size: 10.83px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Language-Model/" style="font-size: 10.83px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 10px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.83px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MTL/" style="font-size: 11.67px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.67px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.83px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 14.17px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.67px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.83px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 10.83px;">NNW</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.83px;">Ngram</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.83px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.83px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.83px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Postgres/" style="font-size: 10.83px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 10.83px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.83px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.83px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 10.83px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.83px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18.33px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/R-Drop/" style="font-size: 10.83px;">R-Drop</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.83px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 13.33px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.83px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.83px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 15.83px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.83px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.83px;">SVM</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.83px;">Search</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Self-Attention/" style="font-size: 11.67px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.83px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.83px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.83px;">Sort</a> <a href="/tags/Span/" style="font-size: 10.83px;">Span</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.83px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.83px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.83px;">System</a> <a href="/tags/T5/" style="font-size: 10.83px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 10.83px;">THW</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.83px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.5px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.83px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 10.83px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 10px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2022 Yam
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>