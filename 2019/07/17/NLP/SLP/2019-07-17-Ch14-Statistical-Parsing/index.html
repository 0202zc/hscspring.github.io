<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>Statistical Parsing Note (SLP Ch14) | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="One crucial use of probabilistic parsing is to solve the problem of disambiguation. CYK only represent them. Probabilistic context-free grammar (PCFG) is the most commonly used probabilistic grammar f">
<meta name="keywords" content="AI,NLP,Formal Grammars,CCG,PCFG,SCFG,Lexicalized CFG,Collins Parser,PCCG,Supertagging,Cross-brackets,Garden-path">
<meta property="og:type" content="article">
<meta property="og:title" content="Statistical Parsing Note (SLP Ch14)">
<meta property="og:url" content="https://www.yam.gift/2019/07/17/NLP/SLP/2019-07-17-Ch14-Statistical-Parsing/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="One crucial use of probabilistic parsing is to solve the problem of disambiguation. CYK only represent them. Probabilistic context-free grammar (PCFG) is the most commonly used probabilistic grammar f">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch12-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch12-2.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch12-3.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch12-4.jpeg">
<meta property="og:updated_time" content="2020-01-23T00:49:48.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Statistical Parsing Note (SLP Ch14)">
<meta name="twitter:description" content="One crucial use of probabilistic parsing is to solve the problem of disambiguation. CYK only represent them. Probabilistic context-free grammar (PCFG) is the most commonly used probabilistic grammar f">
<meta name="twitter:image" content="http://qnimg.lovevivian.cn/slp-ch12-1.jpeg">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
<link rel="alternate" href="/atom.xml" title="Yam" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-NLP/SLP/2019-07-17-Ch14-Statistical-Parsing" class="post-NLP/SLP/2019-07-17-Ch14-Statistical-Parsing post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      Statistical Parsing Note (SLP Ch14)
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2019/07/17/NLP/SLP/2019-07-17-Ch14-Statistical-Parsing/" data-id="cl7yw7chi00yyxybz2fq1neti" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>One crucial use of probabilistic parsing is to solve the problem of <strong>disambiguation</strong>. CYK only represent them.</p>
<p><strong>Probabilistic context-free grammar</strong> (PCFG) is the most commonly used probabilistic grammar formalism. Ways that improve PCFGs trained on Treebank grammars:</p>
<ul>
<li>change the names of the non-terminals (sometimes more specific and sometimes more general)</li>
<li>adding more sophisticated conditioning factors, extending PCFGs to handle probabilistic <strong>subcategorization</strong> information and probabilistic <strong>lexical dependencies</strong></li>
</ul>
<p>Heavily lexicalized grammar formalisms:</p>
<ul>
<li>Lexical-Functional Grammar (LFG) Bresnan, 1982</li>
<li>Head-Driven Phrase Structure Grammar (HPSG) Pollard and Sag, 1994</li>
<li>Tree-Adjoining Grammar (TAG) Joshi, 1985</li>
<li>Combinatory Categorial Grammar (CCG)</li>
</ul>
<a id="more"></a>
<h2 id="Probabilistic-Context-Free-Grammars"><a href="#Probabilistic-Context-Free-Grammars" class="headerlink" title="Probabilistic Context-Free Grammars"></a>Probabilistic Context-Free Grammars</h2><p>PCFG, also <strong>Stochastic Context-Free Grammar</strong> (SCFG) first proposed by Booth (1969).</p>
<p>A context-free grammar G: (N, Σ, R, S):</p>
<ul>
<li>N a set of <strong>non-terminal symbols</strong> (or <strong>variables</strong>)</li>
<li>Σ a set of <strong>terminal symbols</strong> (disjoint from N)</li>
<li>R a set of <strong>rules</strong> or productions, each of the form <code>A → β[p]</code><ul>
<li>A is a non-terminal</li>
<li>β is a string of symbols from the infinite set of strings <code>(Σ∪N)*</code></li>
<li>p is P(β|A), or P(A→β), or P(A→β|A); Σ_β P(A→β) = 1</li>
</ul>
</li>
<li>S a designated <strong>start symbols</strong></li>
</ul>
<p><img src="http://qnimg.lovevivian.cn/slp-ch12-1.jpeg" alt=""></p>
<h3 id="PCFGs-for-Disambiguation"><a href="#PCFGs-for-Disambiguation" class="headerlink" title="PCFGs for Disambiguation"></a>PCFGs for Disambiguation</h3><p>The probability of a particular parse T is defined as the product of the probabilities of all the n rules:</p>
<script type="math/tex; mode=display">
P(T, S)=\prod_{i=1}^{n} P\left(R H S_{i} | L H S_{i}\right)</script><p>P(T, S) = P(T) P(S|T), a parse tree includes all the words of the sentence, P(S|T) = 1, P(T, S) = P(T).</p>
<p>The string of words S is called the <strong>yield</strong> of any parse tree over S. Thus, out of all parse trees with a yield of S, the disambiguation algorithm picks the parse tree that is most probable given S:</p>
<script type="math/tex; mode=display">
\hat{T}(S)=\underset{T s . . S=\text { yield }(T)}{\operatorname{argmax}} P(T | S)</script><script type="math/tex; mode=display">
\hat{T}(S)=\underset{T s . t . S=\text { yield }(T)}{\operatorname{argmax}} \frac{P(T, S)}{P(S)}</script><script type="math/tex; mode=display">
\hat{T}(S)=\underset{T s . t . S=\text { yield }(T)}{\operatorname{argmax}} P(T)</script><h3 id="PCFGs-for-Language-Modeling"><a href="#PCFGs-for-Language-Modeling" class="headerlink" title="PCFGs for Language Modeling"></a>PCFGs for Language Modeling</h3><p>A second attribute of a PCFG is that it assigns a probability to the string of words constituting a sentence. The probability of an ambiguous sentence is the sum of the probabilities of all the parse trees:</p>
<script type="math/tex; mode=display">
\begin{aligned} P(S) &=\sum_{T s . t . S=\text { yield }(T)} P(T, S) \\ &=\sum_{T s, s=\text { yield }(T)} P(T) \end{aligned}</script><p>An additional feature of PCFGs that is useful for language modeling is their ability to assign a probability to substrings of a sentence.</p>
<h2 id="Probabilistic-CKY-Parsing-of-PCFGs"><a href="#Probabilistic-CKY-Parsing-of-PCFGs" class="headerlink" title="Probabilistic CKY Parsing of PCFGs"></a>Probabilistic CKY Parsing of PCFGs</h2><p>Most modern probabilistic parses are based on the <strong>probabilistic CKY</strong>, first designed by Ney (1991).</p>
<p>We need to convert a probabilistic grammar to CNF.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">function PROBABILISTIC-CKY (words, grammar) returns most probable parse <span class="keyword">and</span> its probability</span><br><span class="line">    <span class="keyword">for</span> j ← <span class="keyword">from</span> <span class="number">1</span> to LENGTH(words) do</span><br><span class="line">        <span class="keyword">for</span> all &#123; A | A → words[j] ∈ grammar&#125;</span><br><span class="line">            table[j<span class="number">-1</span>, j, A] ← P(A → words[j])</span><br><span class="line">        <span class="keyword">for</span> i ← <span class="keyword">from</span> j<span class="number">-2</span> downto <span class="number">0</span> do</span><br><span class="line">            <span class="keyword">for</span> k ← i+<span class="number">1</span> to j<span class="number">-1</span> do</span><br><span class="line">                <span class="keyword">for</span> all &#123;A | A → BC ∈ grammar, <span class="keyword">and</span> table[i,k,B] &gt; <span class="number">0</span> <span class="keyword">and</span> table[k,j,C] &gt; <span class="number">0</span> &#125;</span><br><span class="line">                    <span class="keyword">if</span> (table[i,j,A] &lt; P(A → BC) × table[i,k,B] × table[k,j,C]) then</span><br><span class="line">                        table[i,j,A] ← P(A → BC)× table[i,k,B] × table[k,j,C]</span><br><span class="line">                        back[i,j,A] ← &#123;k, B, C&#125;</span><br><span class="line">    <span class="keyword">return</span> BUILD_TREE(back[<span class="number">1</span>, LENGTH(words), S], table[<span class="number">1</span>, LENGth(words), S]</span><br></pre></td></tr></table></figure>
<p>Code TBD.</p>
<h2 id="Ways-to-Learn-PCFG-Rule-Probabilities"><a href="#Ways-to-Learn-PCFG-Rule-Probabilities" class="headerlink" title="Ways to Learn PCFG Rule Probabilities"></a>Ways to Learn PCFG Rule Probabilities</h2><p>Two ways to learn probabilities for the rules of a grammar:</p>
<ul>
<li>use a treebank</li>
<li>generate the counts by parsing a corpus of sentences with the parser. <ul>
<li>Beginning with a parser with equal rule probabilities, then parse the sentence, compute a probability for each parse, use these probabilities to weight the counts, re-estimate the rule probabilities, and so on, until our probabilities converge.</li>
<li>Standard algorithm is called the <strong>inside-outside</strong> algorithm, which was proposed by Baker as a generalization of the forward-backward algorithm for HMMs. Also is a special case of the EM.</li>
</ul>
</li>
</ul>
<h2 id="Problems-with-PCFGs"><a href="#Problems-with-PCFGs" class="headerlink" title="Problems with PCFGs"></a>Problems with PCFGs</h2><p>Two main problems as probability estimators:</p>
<ul>
<li><strong>Poor independence assumptions</strong><ul>
<li>CFG rules impose an independence assumption on probabilities</li>
<li>This results in poor probability estimates, because in English the choice of how a node expands can after all depend on the location of the node in the parse tree.</li>
</ul>
</li>
<li><strong>Lack of lexical conditioning</strong><ul>
<li>Don’t model syntactic facts about special words</li>
<li>We need a model that augments the PCFG probabilities to deal with <strong>lexical dependency</strong> statistics for different verbs and prepositions.</li>
<li>Coordination ambiguities are another case in which lexical dependencies are the key to choosing the proper parse.</li>
</ul>
</li>
</ul>
<p>PCFGs are incapable of modeling important <strong>structural</strong> and <strong>lexical</strong> dependencies.</p>
<h2 id="Improving-PCFGs-by-Splitting-Non-Terminals"><a href="#Improving-PCFGs-by-Splitting-Non-Terminals" class="headerlink" title="Improving PCFGs by Splitting Non-Terminals"></a>Improving PCFGs by Splitting Non-Terminals</h2><p>One idea is to <strong>split</strong> the non-terminal into different versions. One way is to do <strong>parent annotation</strong>, in which we annotate each node with its parent in the parse tree. </p>
<p>To deal with cases in which parent annotation is insufficient: </p>
<ul>
<li>specifically splitting the pre-terminal nodes</li>
<li>handwrite rules that specify a particular node split based on other features of the tree</li>
</ul>
<p>Node-splitting increases the size of the grammar, reduces the amount of training data available for each grammar rule, leading to overfitting. It is important to split to just the correct level of granularity for a particular training set.</p>
<p>The <strong>split and merge</strong> algorithm (Petrov et al. 2006) automatically search for optimal splits.</p>
<h2 id="Probabilistic-Lexicalized-CFGs"><a href="#Probabilistic-Lexicalized-CFGs" class="headerlink" title="Probabilistic Lexicalized CFGs"></a>Probabilistic Lexicalized CFGs</h2><p>Lexicalized parsers includes:</p>
<ul>
<li><strong>Collins parser</strong> (Collins, 1999)</li>
<li><strong>Charniak parser</strong> (Charniak, 1997)</li>
</ul>
<p><strong>Lexicalized grammar</strong>: each non-terminal in the tree is annotated with its lexical <strong>head</strong>, like:</p>
<p><code>VP(dumped) → VBD(dumped) NP(sacks) PP(into)</code></p>
<p>In the standard type of lexicalized grammar, <strong>head tag</strong> is added, too:</p>
<p><code>VP(dumped,VBD) → VBD(dumped,VBD) NP(sacks,NNS) PP(into,P)</code></p>
<p>Probabilistic lexicalized CFG would lead to two kinds of rules:</p>
<ul>
<li><strong>lexical rules</strong>: express the expansion of a pre-terminal to a word</li>
<li><strong>internal rules</strong>: express the other rule expansions</li>
</ul>
<p><img src="http://qnimg.lovevivian.cn/slp-ch12-2.jpeg" alt=""></p>
<p>The MLE estimate for the probability for the rule above would be:</p>
<p><code>Count(VP(dumped,VBD) → VBD(dumped, VBD) NP(sacks,NNS) PP(into,P))/Count(VP(dumped,VBD))</code></p>
<p>But we can’t get good estimates because they are too specific, most rule probabilities will be 0.</p>
<h3 id="The-Collins-Parser"><a href="#The-Collins-Parser" class="headerlink" title="The Collins Parser"></a>The Collins Parser</h3><p>The first intuition is to think of the right-hand side of every (internal) CFG rule as consisting of a head non-terminal, together with the nonterminals to the left of the head and the non-terminals to the right of the head:</p>
<script type="math/tex; mode=display">
L H S \rightarrow L_{n} L_{n-1} \ldots L_{1} H R_{1} \ldots R_{n-1} R_{n}</script><p>Each of the symbols like L1 or R3 or H or LHS is actually a complex symbol representing the category and its head and head tag, like VP(dumped, VP).</p>
<p>Now, instead of computing a single MLE probability for this rule, we are going to break down this rule via a neat generative story, a slight simplification of what is called Collins Model 1. </p>
<p>This new generative story is that given the left-hand side, we first generate the head of the rule and then generate the dependents of the head, one by one, <strong>from the inside out</strong>. Each of these generation steps will have its own probability. A special STOP non-terminal is added at the left and right edges of the rule: </p>
<p><code>P(VP(dumped,VBD) → STOP VBD(dumped, VBD) NP(sacks,NNS) PP(into,P) STOP)</code></p>
<ul>
<li>Generate the head VBD(dumped,VBD) with probability <code>P(H|LHS) = P(VBD(dumped,VBD) | VP(dumped,VBD))</code></li>
<li>Generate the left dependent (which is STOP, since there isn’t one) with probability <code>P(STOP| VP(dumped,VBD) VBD(dumped,VBD))</code></li>
<li>Generate right dependent NP(sacks,NNS) with probability <code>Pr(NP(sacks,NNS| VP(dumped,VBD), VBD(dumped,VBD))</code></li>
<li>Generate the right dependent PP(into,P) with probability <code>Pr(PP(into,P) | VP(dumped,VBD), VBD(dumped,VBD))</code></li>
<li>Generate the right dependent STOP with probability <code>Pr(STOP | VP(dumped,VBD), VBD(dumped,VBD))</code></li>
</ul>
<p>So the previous rule is estimated as:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">PH(VBD|VP, dumped) × PL(STOP|VP,VBD,dumped) </span><br><span class="line">                   × PR(NP(sacks,NNS)|VP,VBD,dumped)</span><br><span class="line">                   × PR(PP(into,P)|VP,VBD,dumped)</span><br><span class="line">                   × PR(STOP|VP,VBD,dumped)</span><br></pre></td></tr></table></figure>
<p>Each of these probabilities can be estimated from much smaller amounts of data than the full probability.</p>
<p>More generally, if H is a head word hw and head tag ht, <code>lw/lt</code> and <code>rw/rt</code> are the <code>word/tag</code> on the left and right, and P is the parent, then the probability of an entire rule can be expressed as:</p>
<ul>
<li>Generate the head of the phrase H(hw, ht) with: <script type="math/tex">P_{H}(H(h w, h t) | P, h w, h t)</script></li>
<li><p>Generate modifiers to the left of the head with: <script type="math/tex">\prod_{i=1}^{n+1} P_{L}\left(L_{i}\left(l w_{i}, l t_{i}\right) | P, H, h w, h t\right)</script></p>
</li>
<li><p>Generate modifiers to the right of the head with: <script type="math/tex">\prod_{i=1}^{n+1} P_{P}\left(R_{i}\left(r w_{i}, r t_{i}\right) | P, H, h w, h t\right)</script></p>
</li>
</ul>
<script type="math/tex; mode=display">L_{n+1}\left(l w_{n+1}, l t_{n+1}\right)=\mathrm{STOP}, R_{n+1}\left(r w_{n+1}, r t_{n+1}\right)=\mathrm{STOP}</script><p>We stop generating once we’ve generated a STOP token.</p>
<h3 id="Advanced-Further-Details-of-the-Collins-Parser"><a href="#Advanced-Further-Details-of-the-Collins-Parser" class="headerlink" title="Advanced: Further Details of the Collins Parser"></a>Advanced: Further Details of the Collins Parser</h3><p>The actual Collins parser models are more complex. Collins Model 1 includes a <strong>distance</strong> feature:</p>
<script type="math/tex; mode=display">
\begin{array}{c}{P_{L}\left(L_{i}\left(l w_{i}, l_{i}\right) | P, H, h w, h t, d i s t a n c e_{L}(i-1)\right)} \\ {P_{R}\left(R_{i}\left(r w_{i}, r t_{i}\right) | P, H, h w, h t, d i s t a n c e_{R}(i-1)\right)}\end{array}</script><p>The distance measure is a function of the sequence of words below the previous modifiers (i.e., the words that are the yield of each modifier non-terminal we have already generated on the left).</p>
<p>The simplest version of this distance measure is just a tuple of two binary features based on the surface string below these previous dependencies: </p>
<ul>
<li>(1) Is the string of length zero? (i.e., were no previous words generated?) </li>
<li>(2) Does the string contain a verb?</li>
</ul>
<p>Collins Model 2 adds more sophisticated features, conditioning on subcategorization frames for each verb and distinguishing arguments from adjuncts.</p>
<p>Finally, smoothing is as important for statistical parsers as it was for N-gram models. The Collins model addresses this problem by interpolating three backed-off models: fully lexicalized (conditioning on<br>the headword), backing off to just the head tag, and altogether unlexicalized.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch12-3.jpeg" alt=""></p>
<script type="math/tex; mode=display">
P_{R}(\ldots)=\lambda_{1} e_{1}+\left(1-\lambda_{1}\right)\left(\lambda_{2} e_{2}+\left(1-\lambda_{2}\right) e_{3}\right)</script><p>In fact there are no “backoff” but interpolated models, e1, e2, e3 are the maximum likelihood estimates above. λ1, λ2 are set to implement Witten-Bell discounting.</p>
<p>Unknown words in the test set and any word less than 6 times in the training set with token <code>&lt;UNKNOWN&gt;</code>. Unknown words in the test set are assigned a pos tag in a preprocessing step by the Ratnaparkhi (1996) tagger.</p>
<h2 id="Probabilistic-CCG-Parsing"><a href="#Probabilistic-CCG-Parsing" class="headerlink" title="Probabilistic CCG Parsing"></a>Probabilistic CCG Parsing</h2><p>Lexicalized grammar frameworks such as CCG pose problems for which the phrase based methods we’ve been discussing are not particularly well-suited.</p>
<p>CCG consists of three parts:</p>
<ul>
<li>a set of categories, can be atomic elements (like S, NP, or function <code>(S\NP)/NP</code>)</li>
<li>a lexicon that associates words with categories</li>
<li>a set of rules that govern how categories combine in context. Specify how functions, their arguments, and other functions combine.</li>
</ul>
<p><code>X/Y Y ⇒ X</code> applies a function to its argument on the right, <strong>forward function application</strong></p>
<p><code>Y X\Y ⇒ X</code> looks to the left for its argument, <strong>backward function application</strong></p>
<h3 id="Ambiguity-in-CCG"><a href="#Ambiguity-in-CCG" class="headerlink" title="Ambiguity in CCG"></a>Ambiguity in CCG</h3><p>The difficulties with CCG parsing arise from the ambiguity caused by the large number of complex lexical categories combined with the very general nature of the grammatical rules. The choice of lexical categories is the primary problem to be addressed in CCG parsing.</p>
<h3 id="CCG-Parsing-Frameworks"><a href="#CCG-Parsing-Frameworks" class="headerlink" title="CCG Parsing Frameworks"></a>CCG Parsing Frameworks</h3><p>The large number of lexical categories available for each word, combined with the promiscuity of CCG’s combinatoric rules, leads to an explosion in the number of (mostly useless) constituents added to the parsing table. The key to managing this explosion is to accurately assess and exploit the most likely lexical categories possible for each word — a process called supertagging.</p>
<p>There are two approaches to CCG parsing that make use of supertags below.</p>
<h3 id="Supertagging"><a href="#Supertagging" class="headerlink" title="Supertagging"></a>Supertagging</h3><p>Supertagging is the corresponding task for highly lexicalized grammar frameworks, where the assigned tags often dictate much of the derivation for a sentence.</p>
<p>It relies on treebanks such as CCGbank:</p>
<ul>
<li>provide overall set of lexical categories</li>
<li>provide the allowable category assignments for each word in the lexicon</li>
</ul>
<p>The standard approach is to use supervised machine learning to build a sequence classifier. A common approach is to use the maximum entropy Markov model (MEMM).</p>
<script type="math/tex; mode=display">
\begin{aligned} \hat{T} &=\underset{T}{\operatorname{argmax}} P(T | W) \\ &=\underset{T}{\operatorname{argmax}} \prod_{i} P\left(t_{i} | w_{i-l}^{i+l}, t_{i-k}^{i-1}\right) \end{aligned}</script><script type="math/tex; mode=display">
=\underset{T}{\operatorname{argmax}} \prod_{i} \frac{\exp \left(\sum_{i} w_{i} f_{i}\left(t_{i}, w_{i-l}^{i+l}, t_{i-k}^{i-1}\right)\right)}{\sum_{t^{\prime} \in \operatorname{tagset}} \exp \left(\sum_{i} w_{i} f_{i}\left(t^{\prime}, w_{i-l}^{i+l}, t_{i-k}^{i-1}\right)\right)}</script><p>k, l both set to 2, POS tags and short character suffixes are also used.</p>
<p>Commonly, <strong>a probability distribution over the possible supertags</strong> for each word will return, instead of the most probability.</p>
<ul>
<li>Viterbi only finds the single best tag sequence, so here we need to use the forward-backward algorithm.</li>
<li>RNN is also OK, and differ from traditional classifier-based methods in :<ul>
<li>Use vector-based word representations rather than word-based feature functions</li>
<li>Input representations span the entire sentence, as opposed to size-limited sliding windows</li>
<li>Avoiding the use of high-level features, such as POS</li>
</ul>
</li>
</ul>
<blockquote>
<p>In fact, DNN model is much easier in practice. Besides this, we can easily add many more mechanisms, for instance, attention.</p>
</blockquote>
<h3 id="CCG-Parsing-using-the-A-Algorithm"><a href="#CCG-Parsing-using-the-A-Algorithm" class="headerlink" title="CCG Parsing using the A* Algorithm"></a>CCG Parsing using the <code>A*</code> Algorithm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">function CCG-ASTAR_PARSE(words) returns table <span class="keyword">or</span> failure</span><br><span class="line">    supertags ← SUPERTAGGER(words)</span><br><span class="line">    <span class="keyword">for</span> i ← <span class="keyword">from</span> <span class="number">1</span> to LENGTH(words) do</span><br><span class="line">        <span class="keyword">for</span> all &#123;A | (words[i], A, score) ∈ supertags&#125;</span><br><span class="line">            edge ← MAKEEDGE(i<span class="number">-1</span>, i, A, score)</span><br><span class="line">            table ← INSERTEDGE(table, edge)</span><br><span class="line">            agenda ← INSERTEDGE(agenda, edge)</span><br><span class="line">    loop do</span><br><span class="line">        if EMPTY?(agenda) return failure</span><br><span class="line">        current ← POP(agenda)</span><br><span class="line">        if COMPLETEDPARSE?(current) return table</span><br><span class="line">        table ← INSERTEDGE(chart, edge)</span><br><span class="line">        <span class="keyword">for</span> each rule <span class="keyword">in</span> APPLICABLERULES(edge) do</span><br><span class="line">            successor ← APPLY(rule, edge)</span><br><span class="line">            <span class="keyword">if</span> successor <span class="keyword">not</span> ∈ <span class="keyword">in</span> agenda <span class="keyword">or</span> chart</span><br><span class="line">                agenda ← INSERTEDGE(agenda, successor)</span><br><span class="line">            <span class="keyword">elif</span> successor ∈ agenda <span class="keyword">with</span> higher cost</span><br><span class="line">                agenda ← REPLACEEDGE(agenda, successor)</span><br></pre></td></tr></table></figure>
<p>TBD</p>
<h2 id="Evaluating-Parsers"><a href="#Evaluating-Parsers" class="headerlink" title="Evaluating Parsers"></a>Evaluating Parsers</h2><p>Use PARSEVAL measures which were proposed by Black et al. (1991).</p>
<p>The intuition is to measure how much the <strong>constituents</strong> in the hypothesis parse tree look like the constituents in a hand-labeled, gold-reference parse.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch12-4.jpeg" alt=""></p>
<p><strong>F-measure</strong> is often used, can be seen <a href="https://yam.gift/2019/05/05/SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification/" target="_blank" rel="noopener">here</a>. An additional metric is used for each sentence: <strong>cross-brackets</strong>.</p>
<p>Cross-brackets: the number of constituents for which the reference parse has a bracketing such as ((A B) C) but the hypothesis parse has a bracketing such as (A (B C)).</p>
<p>For comparing parsers that use different grammars, the canonical implementation of the PARSEVAL metrics is called <strong>evalb</strong> (Sekine and Collins, 1997).</p>
<p>In lexically-oriented grammars, such as CCG and LFG, use alternative evaluation metrics based on measuring the precision and recall of labeled dependencies, where the labels indicate the grammatical relations (Lin 1995, Carroll et al. 1998, Collins et al. 1999).<br>The reason we use components is that it gives us a more fine-grained metric. This is especially true for long sentences, where most parsers don’t get a perfect parse. </p>
<h2 id="Human-Parsing"><a href="#Human-Parsing" class="headerlink" title="Human Parsing"></a>Human Parsing</h2><p>Recent studies suggest that there are at least two ways in which humans apply probabilistic parsing algorithms:</p>
<ul>
<li>One family of studies has shown that when humans read, the predictability of a word seems to influence the reading time; more predictable words are read more quickly. One way of defining predictability is from simple bigram measures.</li>
<li>The second family of studies has examined how humans disambiguate sentences that have multiple possible parses, suggesting that humans prefer whichever parse is more probable. These studies often rely on a specific class of temporarily ambiguous sentences called <strong>garden-path</strong> sentences (first described by Bever, 1970) which are cleverly constructed to have three properties that very difficult to parse:<ul>
<li><strong>temporarily ambiguous</strong>: The sentence is unambiguous, but its initial portion is ambiguous.</li>
<li>One of the two or more parses in the initial portion is somehow preferable to the human parsing mechanism.</li>
<li>But the dispreferred parse is the correct one for the sentence.</li>
</ul>
</li>
</ul>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li>Probabilistic grammars assign a probability to a sentence or string of words while attempting to capture more sophisticated syntactic information than the N-gram grammars.</li>
<li>A <strong>probabilistic context-free grammar (PCFG)</strong> is a context-free grammar in which every rule is annotated with the probability of that rule being chosen. Each PCFG rule is treated as if it were <strong>conditionally independent</strong>. The probabilistic <strong>CKY (Cocke-Kasami-Younger)</strong> algorithm is a probabilistic version of the CKY parsing algorithm. PCFG probabilities can be learned by counting in a <strong>parsed corpus</strong> or by parsing a corpus. The <strong>inside-outside</strong> algorithm is a way of dealing with the fact that the sentences being parsed are ambiguous.</li>
<li>Raw PCFGs suffer from poor independence assumptions among rules and lack of sensitivity to lexical dependencies. One way to deal with this problem is to split and merge non-terminals (automatically or by hand). <strong>Probabilistic lexicalized CFGs</strong> are another solution to this problem in which the basic PCFG model is augmented with a <strong>lexical head</strong> for each rule. Parsers for lexicalized PCFGs (like the Charniak and Collins parsers) are based on extensions to probabilistic CKY parsing.</li>
<li>Parsers are evaluated with three metrics: <strong>labeled recall, labeled precision,</strong> and <strong>cross-brackets</strong>.</li>
<li>Evidence from <strong>garden-path sentences</strong> and other on-line sentence-processing experiments suggest that the human parser uses some kinds of probabilistic information about grammar.</li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/07/17/NLP/SLP/2019-07-17-Ch14-Statistical-Parsing/">
    <time datetime="2019-07-17T03:00:00.000Z" class="entry-date">
        2019-07-17
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CCG/">CCG</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Collins-Parser/">Collins Parser</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cross-brackets/">Cross-brackets</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Formal-Grammars/">Formal Grammars</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Garden-path/">Garden-path</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Lexicalized-CFG/">Lexicalized CFG</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PCCG/">PCCG</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PCFG/">PCFG</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SCFG/">SCFG</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Supertagging/">Supertagging</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2019/07/18/DSA/LeetCode/2019-07-15-Median-of-Two-Sorted-Arrays/" rel="prev"><span class="meta-nav">←</span> Median of Two Sorted Arrays (LeetCode 4)</a></span>
    
    
        <span class="nav-next"><a href="/2019/07/15/Diary/2019-07-15-diary/" rel="next">随笔：命运 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">70</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">100</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">21</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=541131&auto=0&height=66"></iframe>
      <!-- 评论代码 -->
      <!-- <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio> -->
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2022/10/16/Paper/2022-10-16-GlobalPointer/">Global Pointer：Novel Efficient Span-based Approach for NER</a>
          </li>
        
          <li>
            <a href="/2022/10/15/Paper/2022-10-15-DeepGen/">DeepGen：Diverse Search Ad Generation and Real-Time Customization</a>
          </li>
        
          <li>
            <a href="/2022/09/11/Diary/2022-09-11-Passion/">只如初见的不只爱情</a>
          </li>
        
          <li>
            <a href="/2022/08/28/Paper/2022-08-28-FLAN/">FLAN：Fine-tuned Language Models are Zero-Shot Learners</a>
          </li>
        
          <li>
            <a href="/2022/07/17/Paper/2022-07-17-W2NER-Code/">W2NER 代码</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 19.17px;">AI</a> <a href="/tags/ALBERT/" style="font-size: 10px;">ALBERT</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/AUC/" style="font-size: 10px;">AUC</a> <a href="/tags/Accuracy/" style="font-size: 10px;">Accuracy</a> <a href="/tags/Activation/" style="font-size: 10px;">Activation</a> <a href="/tags/Algorithm/" style="font-size: 13.33px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Arrow/" style="font-size: 10px;">Arrow</a> <a href="/tags/Attention/" style="font-size: 12.5px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Automation/" style="font-size: 10px;">Automation</a> <a href="/tags/BERT/" style="font-size: 17.5px;">BERT</a> <a href="/tags/BIO/" style="font-size: 10px;">BIO</a> <a href="/tags/BIOHD/" style="font-size: 10px;">BIOHD</a> <a href="/tags/BM25/" style="font-size: 10px;">BM25</a> <a href="/tags/BPE/" style="font-size: 10px;">BPE</a> <a href="/tags/Backtracking/" style="font-size: 10px;">Backtracking</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bahdanau-Attention/" style="font-size: 10px;">Bahdanau Attention</a> <a href="/tags/Bart/" style="font-size: 10px;">Bart</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert-Flow/" style="font-size: 10px;">Bert-Flow</a> <a href="/tags/Bi-LSTM/" style="font-size: 10px;">Bi-LSTM</a> <a href="/tags/Biasing/" style="font-size: 10px;">Biasing</a> <a href="/tags/Binary-Search/" style="font-size: 11.67px;">Binary Search</a> <a href="/tags/Blending/" style="font-size: 10px;">Blending</a> <a href="/tags/Brain/" style="font-size: 10px;">Brain</a> <a href="/tags/Brain-Decoding/" style="font-size: 10px;">Brain Decoding</a> <a href="/tags/Bridge/" style="font-size: 10px;">Bridge</a> <a href="/tags/Business/" style="font-size: 11.67px;">Business</a> <a href="/tags/C/" style="font-size: 10.83px;">C</a> <a href="/tags/C4/" style="font-size: 10px;">C4</a> <a href="/tags/CCG/" style="font-size: 10.83px;">CCG</a> <a href="/tags/CE-BERT/" style="font-size: 10px;">CE BERT</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CRF/" style="font-size: 10px;">CRF</a> <a href="/tags/CS/" style="font-size: 10px;">CS</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/Camera/" style="font-size: 10px;">Camera</a> <a href="/tags/Catalan/" style="font-size: 10px;">Catalan</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/Chi2/" style="font-size: 10px;">Chi2</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Class-Imbalance-Loss/" style="font-size: 10px;">Class Imbalance Loss</a> <a href="/tags/Classification/" style="font-size: 10.83px;">Classification</a> <a href="/tags/Cognition/" style="font-size: 10.83px;">Cognition</a> <a href="/tags/Collaborative-Filtering/" style="font-size: 10px;">Collaborative Filtering</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 12.5px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Contrastive-Learning/" style="font-size: 10px;">Contrastive-Learning</a> <a href="/tags/Coordinate-Ascent/" style="font-size: 10px;">Coordinate Ascent</a> <a href="/tags/Cosine/" style="font-size: 10.83px;">Cosine</a> <a href="/tags/Cosine-Similarity/" style="font-size: 10px;">Cosine Similarity</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Cross-view/" style="font-size: 10px;">Cross-view</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/Culture/" style="font-size: 10px;">Culture</a> <a href="/tags/DA/" style="font-size: 10px;">DA</a> <a href="/tags/DB/" style="font-size: 10.83px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Augmentation/" style="font-size: 10px;">Data Augmentation</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Enhancement/" style="font-size: 10px;">Data Enhancement</a> <a href="/tags/Data-Preprocess/" style="font-size: 10px;">Data Preprocess</a> <a href="/tags/Data-Science/" style="font-size: 15px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 16.67px;">Data Structure</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/DeBERTa/" style="font-size: 10px;">DeBERTa</a> <a href="/tags/Debiasing/" style="font-size: 10px;">Debiasing</a> <a href="/tags/Decoder/" style="font-size: 10px;">Decoder</a> <a href="/tags/Decoding/" style="font-size: 10px;">Decoding</a> <a href="/tags/Deep/" style="font-size: 10px;">Deep</a> <a href="/tags/DeepGen/" style="font-size: 10px;">DeepGen</a> <a href="/tags/DeepGraph/" style="font-size: 10px;">DeepGraph</a> <a href="/tags/DeepLearning/" style="font-size: 12.5px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 12.5px;">Diary</a> <a href="/tags/Disentangled-Attention/" style="font-size: 10px;">Disentangled Attention</a> <a href="/tags/DistilBERT/" style="font-size: 10px;">DistilBERT</a> <a href="/tags/Distillation/" style="font-size: 10px;">Distillation</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Docker-Compose/" style="font-size: 10px;">Docker-Compose</a> <a href="/tags/Dockerfile/" style="font-size: 10px;">Dockerfile</a> <a href="/tags/Dropout/" style="font-size: 10.83px;">Dropout</a> <a href="/tags/Dynamic-Mask/" style="font-size: 10px;">Dynamic-Mask</a> <a href="/tags/EDA/" style="font-size: 10px;">EDA</a> <a href="/tags/EMD/" style="font-size: 10px;">EMD</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Efficient-DeepLearning/" style="font-size: 10px;">Efficient-DeepLearning</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 10.83px;">Elixir</a> <a href="/tags/Ellipsis/" style="font-size: 10px;">Ellipsis</a> <a href="/tags/Embedding/" style="font-size: 11.67px;">Embedding</a> <a href="/tags/Embeddings/" style="font-size: 10.83px;">Embeddings</a> <a href="/tags/Encoder/" style="font-size: 10px;">Encoder</a> <a href="/tags/Entropy/" style="font-size: 10.83px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 10.83px;">Evaluation</a> <a href="/tags/ExT5/" style="font-size: 10px;">ExT5</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FLAN/" style="font-size: 10px;">FLAN</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Faith/" style="font-size: 10px;">Faith</a> <a href="/tags/Feature-Engineering/" style="font-size: 10px;">Feature Engineering</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 10px;">Few-Shot</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Grammars/" style="font-size: 11.67px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/Funk-MF/" style="font-size: 10px;">Funk MF</a> <a href="/tags/Funnel-Transformer/" style="font-size: 10px;">Funnel Transformer</a> <a href="/tags/GBTD/" style="font-size: 10px;">GBTD</a> <a href="/tags/GELU/" style="font-size: 10px;">GELU</a> <a href="/tags/GP/" style="font-size: 10px;">GP</a> <a href="/tags/GPT-2/" style="font-size: 10px;">GPT-2</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/GSG/" style="font-size: 10px;">GSG</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Global-Pointer/" style="font-size: 10px;">Global Pointer</a> <a href="/tags/Glow/" style="font-size: 10px;">Glow</a> <a href="/tags/Graceful-Shutdown/" style="font-size: 10px;">Graceful Shutdown</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/Graph/" style="font-size: 10.83px;">Graph</a> <a href="/tags/GraphQL/" style="font-size: 10.83px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/Growth/" style="font-size: 10px;">Growth</a> <a href="/tags/HMM/" style="font-size: 10.83px;">HMM</a> <a href="/tags/Hard-SVM/" style="font-size: 10px;">Hard-SVM</a> <a href="/tags/Hinge-Loss/" style="font-size: 10px;">Hinge Loss</a> <a href="/tags/Hope/" style="font-size: 10px;">Hope</a> <a href="/tags/Host-only/" style="font-size: 10px;">Host-only</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/IQR/" style="font-size: 10px;">IQR</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Impossible-Triangle/" style="font-size: 10px;">Impossible-Triangle</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Isolation-Forest/" style="font-size: 10px;">Isolation Forest</a> <a href="/tags/ItemCF/" style="font-size: 10px;">ItemCF</a> <a href="/tags/Jaccard/" style="font-size: 10px;">Jaccard</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Jax/" style="font-size: 10px;">Jax</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Jupyter/" style="font-size: 10px;">Jupyter</a> <a href="/tags/KKT/" style="font-size: 10px;">KKT</a> <a href="/tags/KS/" style="font-size: 10px;">KS</a> <a href="/tags/Kernel/" style="font-size: 10px;">Kernel</a> <a href="/tags/Kernel-Function/" style="font-size: 10px;">Kernel Function</a> <a href="/tags/Kernel-Method/" style="font-size: 10px;">Kernel Method</a> <a href="/tags/Keyword/" style="font-size: 10px;">Keyword</a> <a href="/tags/Knowledge-Graph/" style="font-size: 10.83px;">Knowledge Graph</a> <a href="/tags/LM/" style="font-size: 10.83px;">LM</a> <a href="/tags/LOF/" style="font-size: 10px;">LOF</a> <a href="/tags/LR/" style="font-size: 10px;">LR</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Language-Model/" style="font-size: 10.83px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Life/" style="font-size: 10px;">Life</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/LinkedList/" style="font-size: 10.83px;">LinkedList</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/Luong-Attention/" style="font-size: 10px;">Luong Attention</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/MF/" style="font-size: 10px;">MF</a> <a href="/tags/MTL/" style="font-size: 11.67px;">MTL</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 15px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Managemnt/" style="font-size: 11.67px;">Managemnt</a> <a href="/tags/MarkBERT/" style="font-size: 10px;">MarkBERT</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 10.83px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Matrix-Factorization/" style="font-size: 10px;">Matrix Factorization</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Meta-Learning/" style="font-size: 10px;">Meta Learning</a> <a href="/tags/Metric/" style="font-size: 10px;">Metric</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Minkowski/" style="font-size: 10px;">Minkowski</a> <a href="/tags/Model-Evaluation/" style="font-size: 10px;">Model Evaluation</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multi-Head-Attention/" style="font-size: 10px;">Multi-Head Attention</a> <a href="/tags/Multitask/" style="font-size: 10px;">Multitask</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NAT/" style="font-size: 10px;">NAT</a> <a href="/tags/NER/" style="font-size: 14.17px;">NER</a> <a href="/tags/NLG/" style="font-size: 11.67px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10.83px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/NMT/" style="font-size: 10px;">NMT</a> <a href="/tags/NNW/" style="font-size: 10.83px;">NNW</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Ngram/" style="font-size: 10.83px;">Ngram</a> <a href="/tags/Normalizing-Flow/" style="font-size: 10px;">Normalizing Flow</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Numba/" style="font-size: 10px;">Numba</a> <a href="/tags/Numpy/" style="font-size: 10px;">Numpy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/P-R/" style="font-size: 10px;">P-R</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PEGASUS/" style="font-size: 10px;">PEGASUS</a> <a href="/tags/PLM/" style="font-size: 10.83px;">PLM</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/PTM/" style="font-size: 10px;">PTM</a> <a href="/tags/PageRank/" style="font-size: 10px;">PageRank</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandarallel/" style="font-size: 10px;">Pandarallel</a> <a href="/tags/Pandas/" style="font-size: 10.83px;">Pandas</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Passion/" style="font-size: 10px;">Passion</a> <a href="/tags/Pearson/" style="font-size: 10px;">Pearson</a> <a href="/tags/Philosophy/" style="font-size: 10.83px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Polars/" style="font-size: 10px;">Polars</a> <a href="/tags/Pooling/" style="font-size: 10px;">Pooling</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Postgres/" style="font-size: 10.83px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-Trained/" style="font-size: 10px;">Pre-Trained</a> <a href="/tags/Pre-Training/" style="font-size: 10px;">Pre-Training</a> <a href="/tags/Pre-training/" style="font-size: 10.83px;">Pre-training</a> <a href="/tags/Precision/" style="font-size: 10px;">Precision</a> <a href="/tags/Pretrain/" style="font-size: 10.83px;">Pretrain</a> <a href="/tags/Pretrained/" style="font-size: 10px;">Pretrained</a> <a href="/tags/Pretraining/" style="font-size: 10.83px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/Promote/" style="font-size: 10px;">Promote</a> <a href="/tags/Prompt/" style="font-size: 10.83px;">Prompt</a> <a href="/tags/ProtoBERT/" style="font-size: 10px;">ProtoBERT</a> <a href="/tags/Pruning/" style="font-size: 10px;">Pruning</a> <a href="/tags/Psychology/" style="font-size: 10.83px;">Psychology</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 18.33px;">Python</a> <a href="/tags/QA/" style="font-size: 10px;">QA</a> <a href="/tags/Quant/" style="font-size: 10px;">Quant</a> <a href="/tags/Quantization/" style="font-size: 10px;">Quantization</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/R-Drop/" style="font-size: 10.83px;">R-Drop</a> <a href="/tags/RELU/" style="font-size: 10px;">RELU</a> <a href="/tags/RFE/" style="font-size: 10px;">RFE</a> <a href="/tags/RMSE/" style="font-size: 10px;">RMSE</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/ROC/" style="font-size: 10px;">ROC</a> <a href="/tags/RWD/" style="font-size: 10px;">RWD</a> <a href="/tags/Rank/" style="font-size: 10px;">Rank</a> <a href="/tags/RaspberryPi/" style="font-size: 10.83px;">RaspberryPi</a> <a href="/tags/Raspberrypi/" style="font-size: 10px;">Raspberrypi</a> <a href="/tags/Recall/" style="font-size: 10px;">Recall</a> <a href="/tags/Recommendation/" style="font-size: 13.33px;">Recommendation</a> <a href="/tags/Recursion/" style="font-size: 10.83px;">Recursion</a> <a href="/tags/Reformer/" style="font-size: 10px;">Reformer</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Representation/" style="font-size: 10.83px;">Representation</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Retrieving/" style="font-size: 10px;">Retrieving</a> <a href="/tags/RoBERTa/" style="font-size: 10px;">RoBERTa</a> <a href="/tags/Rotated-Sorted-Array/" style="font-size: 10px;">Rotated Sorted Array</a> <a href="/tags/Rust/" style="font-size: 15.83px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SMO/" style="font-size: 10px;">SMO</a> <a href="/tags/SQL/" style="font-size: 10.83px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD++</a> <a href="/tags/SVM/" style="font-size: 10.83px;">SVM</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10.83px;">Search</a> <a href="/tags/Segmentation/" style="font-size: 10px;">Segmentation</a> <a href="/tags/Self-Attention/" style="font-size: 11.67px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Semantic-Similarity/" style="font-size: 10px;">Semantic Similarity</a> <a href="/tags/Senta/" style="font-size: 10px;">Senta</a> <a href="/tags/Sentence-Representation/" style="font-size: 10px;">Sentence Representation</a> <a href="/tags/Sentence-Similarity/" style="font-size: 10px;">Sentence Similarity</a> <a href="/tags/Sentence-BERT/" style="font-size: 10px;">Sentence-BERT</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/SentimentAnalysis/" style="font-size: 10px;">SentimentAnalysis</a> <a href="/tags/Siamese/" style="font-size: 10px;">Siamese</a> <a href="/tags/Sigmoid/" style="font-size: 10px;">Sigmoid</a> <a href="/tags/SimCSE/" style="font-size: 10.83px;">SimCSE</a> <a href="/tags/Similarity/" style="font-size: 10px;">Similarity</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Skill/" style="font-size: 10px;">Skill</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 10.83px;">Smoothing</a> <a href="/tags/Soft-SVM/" style="font-size: 10px;">Soft-SVM</a> <a href="/tags/Softmax/" style="font-size: 10px;">Softmax</a> <a href="/tags/Sort/" style="font-size: 10.83px;">Sort</a> <a href="/tags/Span/" style="font-size: 10.83px;">Span</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/SqueezeBERT/" style="font-size: 10px;">SqueezeBERT</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Stacking/" style="font-size: 10px;">Stacking</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/Stirling/" style="font-size: 10px;">Stirling</a> <a href="/tags/StratifiedKFold/" style="font-size: 10px;">StratifiedKFold</a> <a href="/tags/String/" style="font-size: 10.83px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Summarization/" style="font-size: 10.83px;">Summarization</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/Swap/" style="font-size: 10px;">Swap</a> <a href="/tags/System/" style="font-size: 10.83px;">System</a> <a href="/tags/T5/" style="font-size: 10.83px;">T5</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/THW/" style="font-size: 10.83px;">THW</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/TanH/" style="font-size: 10px;">TanH</a> <a href="/tags/TensorBay/" style="font-size: 10px;">TensorBay</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Classification/" style="font-size: 10px;">Text Classification</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/TextCNN/" style="font-size: 10.83px;">TextCNN</a> <a href="/tags/TextRank/" style="font-size: 10px;">TextRank</a> <a href="/tags/Thought/" style="font-size: 10px;">Thought</a> <a href="/tags/Transformer/" style="font-size: 17.5px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Tuning/" style="font-size: 10px;">Tuning</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/UniLM/" style="font-size: 10px;">UniLM</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Unix/" style="font-size: 10px;">Unix</a> <a href="/tags/UserCF/" style="font-size: 10px;">UserCF</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/Virtual-Network/" style="font-size: 10px;">Virtual Network</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 10.83px;">Viterbi</a> <a href="/tags/Vocabulary-Learning/" style="font-size: 10px;">Vocabulary Learning</a> <a href="/tags/Voting/" style="font-size: 10px;">Voting</a> <a href="/tags/W2NER/" style="font-size: 10.83px;">W2NER</a> <a href="/tags/WOE/" style="font-size: 10px;">WOE</a> <a href="/tags/Web-Server-Multithreaded-Server/" style="font-size: 10px;">Web Server Multithreaded Server</a> <a href="/tags/Wide/" style="font-size: 10px;">Wide</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/Z-Score/" style="font-size: 10px;">Z-Score</a> <a href="/tags/Zero-Short/" style="font-size: 10px;">Zero-Short</a> <a href="/tags/Zero-Shot/" style="font-size: 10px;">Zero-Shot</a> <a href="/tags/Zero-shot/" style="font-size: 10px;">Zero-shot</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/binning/" style="font-size: 10px;">binning</a> <a href="/tags/few-shot/" style="font-size: 10px;">few-shot</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/jpype/" style="font-size: 10px;">jpype</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/motion/" style="font-size: 10px;">motion</a> <a href="/tags/node2vec/" style="font-size: 10px;">node2vec</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a> <a href="/tags/str/" style="font-size: 10px;">str</a> <a href="/tags/vlc/" style="font-size: 10px;">vlc</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2022 Yam
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>