<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  
  <meta name="description" content="Yam | AI | NLP | 人工智能 | 哲学 | 自然语言处理 | 机器学习" />
  

  
  
  
  
  
  <title>Information Extraction Note (SLP Ch17) | Yam</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Recently, I wanted to build an information extraction system, so I searched for Google. However there were little Chinese articles, the quality was not so good as well. Fortunately, I found several En">
<meta name="keywords" content="NLP,AI,Information Extraction,IE">
<meta property="og:type" content="article">
<meta property="og:title" content="Information Extraction Note (SLP Ch17)">
<meta property="og:url" content="https://www.yam.gift/2019/04/09/SLP/2019-04-09-Information-Extraction/index.html">
<meta property="og:site_name" content="Yam">
<meta property="og:description" content="Recently, I wanted to build an information extraction system, so I searched for Google. However there were little Chinese articles, the quality was not so good as well. Fortunately, I found several En">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://www.nltk.org/images/ie-architecture.png">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch17-1.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch17-2.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch17-3.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch17-4.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch17-5.jpeg">
<meta property="og:image" content="http://qnimg.lovevivian.cn/slp-ch17-6.jpeg">
<meta property="og:updated_time" content="2019-05-08T03:08:36.805Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Information Extraction Note (SLP Ch17)">
<meta name="twitter:description" content="Recently, I wanted to build an information extraction system, so I searched for Google. However there were little Chinese articles, the quality was not so good as well. Fortunately, I found several En">
<meta name="twitter:image" content="https://www.nltk.org/images/ie-architecture.png">
  
    <link rel="alternative" href="/atom.xml" title="Yam" type="application/atom+xml">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="Yam" rel="home">Yam</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">Feeling, Coding, Thinking</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">Home</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/series/">Series</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives/">Archives</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/about/">About</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="https://github.com/hscspring">Projects</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-SLP/2019-04-09-Information-Extraction" class="post-SLP/2019-04-09-Information-Extraction post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      Information Extraction Note (SLP Ch17)
    </h1>
  

        
        <!-- <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://www.yam.gift/2019/04/09/SLP/2019-04-09-Information-Extraction/" data-id="ck40rxa6000f0r8ccl5rby3z7" class="leave-reply bdsharebuttonbox" data-cmd="more">undefined</a>
        </div> --><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
            <!-- Table of Contents -->
              
        <p>Recently, I wanted to build an information extraction system, so I searched for Google. However there were little Chinese articles, the quality was not so good as well. Fortunately, I found several English ones seemed well, and then the summary is here. The whole structure is based on my favorite NLP book <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">Speech and Language Processing</a> (use SLP instead below), also with some other materials in the reference.</p>
<p>Information extraction (IE), turns the unstructured information extraction information embedded in texts into structured data, for example for populating a relational database to enable further processing. Here is a figure of: Simple Pipeline Architecture for an Information Extraction System.</p>
<blockquote>
<p>From: <a href="https://www.nltk.org/book/ch07.html" target="_blank" rel="noopener">https://www.nltk.org/book/ch07.html</a></p>
<p>By the way, this book provides actionable steps, focusing on specific actions.</p>
</blockquote>
<p><img src="https://www.nltk.org/images/ie-architecture.png" alt="architecture"></p>
<a id="more"></a>
<h1 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h1><p>This part is mainly the basic knowledge, which comes from ch17 of <a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">Speech and Language Processing</a>.</p>
<h2 id="Named-Entity-Recognition"><a href="#Named-Entity-Recognition" class="headerlink" title="Named Entity Recognition"></a>Named Entity Recognition</h2><p>The first step in information extraction is to detect the <strong>entities</strong> in the text. A <strong>named entity</strong> is, roughly speaking, anything that can be referred to with a proper name: <strong>a person, a location, an organization</strong>.</p>
<p>The term is commonly extended to include things that aren’t entities, such as <strong>dates, times, and other kinds of temporal expressions, and even numerical expressions like prices</strong>.</p>
<p><strong>Named entity recognition</strong> means finding spans of text taht constitute proper names and then classifying the type of the entity. The difficulties are:</p>
<ul>
<li>ambiguity of segmentation, that’s even serious in Chinese.</li>
<li>type ambiguity</li>
</ul>
<h3 id="NER-as-Sequence-Labeling"><a href="#NER-as-Sequence-Labeling" class="headerlink" title="NER as Sequence Labeling"></a>NER as Sequence Labeling</h3><p>A sequence classifier like an MEMM/CRF  or  a bi-LSTM is trained to label the tokens in a text with tags that indicate the presence of particular kinds of entities. Some details of data and training can be found here: <a href="https://ltp.readthedocs.io/zh_CN/latest/appendix.html" target="_blank" rel="noopener">附录 — LTP 3.3 文档</a></p>
<h3 id="A-feature-based-algorithm-for-NER"><a href="#A-feature-based-algorithm-for-NER" class="headerlink" title="A feature-based algorithm for NER"></a>A feature-based algorithm for NER</h3><p>Typical features for a feature-based NER system:</p>
<ul>
<li>identity of wi, identity of neighboring words</li>
<li>embeddings for wi, embeddings for neighboring words</li>
<li>part of speech of wi, part of speech of neighboring words</li>
<li>base-phrase syntactic chunk label of wi and neighboring words</li>
<li>presence of wi in a <strong>gazetteer</strong></li>
<li>wi contains a particular prefix (from all prefixes of length ≤ 4)</li>
<li>wi contains a particular suffix (from all suffixes of length ≤ 4)</li>
<li>wi is all upper case</li>
<li>word shape of wi, word shape of neighboring words</li>
<li>short word shape of wi, short word shape of neighboring words</li>
<li>presence of hyphen</li>
</ul>
<p>In Chinese, things become a little different, there aren’t many clear features, and Chinese text are always connected together. </p>
<p>For example the named entity token L’Occitane would generate the following noon-zero valued feature values:</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">prefix(w_i) = L                	suffix(w_i) = tane</span><br><span class="line">prefix(w_i) = L'               	suffix(w_i) = ane</span><br><span class="line">prefix(w_i) = L'O				suffix(w_i) = ne</span><br><span class="line">prefix(w_i) = L'Oc				suffix(w_i) = e</span><br><span class="line">word-shape(w_i) = X'Xxxxxxxx	short-word-shape(w_i) = X'Xx</span><br></pre></td></tr></table></figure>
<p>A <strong>gazetteer</strong> is a list of place names, often providing millions of entriesfor locations with detailed geographical and political information. A related resource is <strong>name-lists</strong>, similar lists also contain <strong>corporations, commercial products, and all manner of things biological and mineral</strong>. While gazetteers can be quite effective, list of persons and organizations are not always helpful. </p>
<p>A sequence classifier like an MEMM can be trained to label new sentences. If we assume a context window that includes the two preceding and following words, then the features available to the classifier are those shown in the boxed area.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch17-1.jpeg" alt=""></p>
<h3 id="A-neural-algorithm-for-NER"><a href="#A-neural-algorithm-for-NER" class="headerlink" title="A neural algorithm for NER"></a>A neural algorithm for NER</h3><p>A bi-LSTM output a softmax over all NER tags, but it’s insufficient, since it doesn’t allow us to impose the strong constraintsneighboring tokens have on each other. Instead a CRF layer is normally used on top of the bi-LSTM output, and the Viterbi decoding algorithm is used to decode.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch17-2.jpeg" alt=""></p>
<h3 id="Rule-based-NER"><a href="#Rule-based-NER" class="headerlink" title="Rule-based NER"></a>Rule-based NER</h3><p>While sequence models are the norm in academic research, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with smaller amount of supervised machine learning. For example <a href="https://www.aclweb.org/anthology/N18-3010" target="_blank" rel="noopener">SystemT: Declarative Text Understanding for Enterprise</a>.</p>
<p>One common approach:</p>
<ul>
<li>First, use high-precision rules to tag unambiguous entity mentions</li>
<li>Then, search for substring matches of the previously detected names</li>
<li>Consult application-specific name lists to identify likely name entity mentions from the given domain</li>
<li>Finally, apply probabilistic sequence labeling techniques that make use of the tags from previous stages as additional features</li>
</ul>
<h3 id="Evaluation-of-Named-Entity-Recognition"><a href="#Evaluation-of-Named-Entity-Recognition" class="headerlink" title="Evaluation of Named Entity Recognition"></a>Evaluation of Named Entity Recognition</h3><p>For named entities, the entity rather than the word is the unit of response. The fact that named entity tagging has a segmentation component which is not present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled American but not American Airlines as an organization would cause two errors, a false positive for O and a false<br>negative for I-ORG.</p>
<h2 id="Relation-Extraction"><a href="#Relation-Extraction" class="headerlink" title="Relation Extraction"></a>Relation Extraction</h2><p>An <strong>RDF triple</strong> is a tuple of entity-relationentity, called a subject-predicate-object expression.</p>
<ul>
<li><a href="https://wiki.dbpedia.org/" target="_blank" rel="noopener">DBpedia</a>  is an ontology derived from Wikipedia containing over 2 billion RDF triples.</li>
<li><a href="https://developers.google.com/freebase/" target="_blank" rel="noopener">Data Dumps  |  Freebase API (Deprecated)  |  Google Developers</a></li>
<li><a href="https://wordnet.princeton.edu/" target="_blank" rel="noopener">WordNet | A Lexical Database for English</a> or other ontologies offer useful ontological relations that express hieris-a archical relations between words or concepts.</li>
</ul>
<h3 id="Using-Patterns-to-Extract-Relations"><a href="#Using-Patterns-to-Extract-Relations" class="headerlink" title="Using Patterns to Extract Relations"></a>Using Patterns to Extract Relations</h3><p><img src="http://qnimg.lovevivian.cn/slp-ch17-3.jpeg" alt=""></p>
<ul>
<li><p>Advantage: high-precision and can be tailored to specific domains</p>
</li>
<li><p>Disadvantage : low-recall and a lot of work to create them for all possible patterns</p>
</li>
</ul>
<h3 id="Relation-Extraction-via-Supervised-Learning"><a href="#Relation-Extraction-via-Supervised-Learning" class="headerlink" title="Relation Extraction via Supervised Learning"></a>Relation Extraction via Supervised Learning</h3><p>The most straightforward approach has three steps:</p>
<ul>
<li>Find pairs of named entities (usually in the same sentence)</li>
<li>A filtering classifier is trained to make a binary decision as to whether a given pair of named entities are related</li>
<li>A classifier is trained to assign a label to the relations that were found by step 2</li>
</ul>
<p>For the feature-based classifiers like LR, RF, the most important step is to identify useful features. Consider features for classifying the relationship  between <em>American Airlines (M1)</em> and <em>Tim Wagner (M2)</em> from the sentence: “American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said”.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch17-4.jpeg" alt=""></p>
<p>Labeling a large training set is extremely expensive and supervised models are brittle: they don’t generalize well to different text genres. So much research has focused on the semi-supervised and unsupervised approaches.</p>
<h3 id="Semisupervised-Relation-Extraction-via-Bootstrapping"><a href="#Semisupervised-Relation-Extraction-via-Bootstrapping" class="headerlink" title="Semisupervised Relation Extraction via Bootstrapping"></a>Semisupervised Relation Extraction via Bootstrapping</h3><p>Suppose we just have a few high-precision <strong>seed patterns</strong> or a few <strong>seed tuples</strong>, that’s enough to bootstrap a classifier. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">function BOOTSTRAP(Relation R) returns new relation tuples</span><br><span class="line">	tuples←Gather a set of seed tuples that have relation R</span><br><span class="line">	iterate</span><br><span class="line">		sentences←find sentences that contain entities <span class="keyword">in</span> tuples</span><br><span class="line">		patterns←generalize the context between <span class="keyword">and</span> around entities <span class="keyword">in</span> sentences</span><br><span class="line">		newpairs←use patterns to grep <span class="keyword">for</span> more tuples</span><br><span class="line">		newpairs←newpairs <span class="keyword">with</span> high confidence</span><br><span class="line">		tuples←tuples + newpairs</span><br><span class="line">	<span class="keyword">return</span> tuples</span><br></pre></td></tr></table></figure>
<p>Bootstrapping systems assign <strong>confidence values</strong> to new tuples to avoid <strong>semantic drift</strong>. </p>
<p>Given a document collection D, a current set of tuples T, and a proposed pattern p, we need to track two factors:</p>
<ul>
<li>hits: the set of tuples in T that p matches while looking in D</li>
<li>finds: the total set of tuples that p finds in D</li>
</ul>
<script type="math/tex; mode=display">Conf_{RlogF}(p) = \frac{hits_p}{finds_p} \times log(finds_p)</script><p>We can assess the confidence in a proposed new tuple by combining the evidence supporting it from all the patterns P’ that match that tuple in D. One way to combine such evidence is the <strong>noisy-or</strong> technique. </p>
<p>Assume that a given tuple is supported by a subset of the patterns in P, each with its own confidence assessed as above.</p>
<p>In the noisy-or model, we make two basic assumptions. First, that for a proposed tuple to be false, all of its supporting patterns must have been in error, and second, that the sources of their individual failures are<br>all independent.</p>
<p>If we loosely treat our confidence measures as probabilities, then the probability of any individual pattern p failing is 1−Conf(p); the probability of all of the supporting patterns for a tuple being wrong is the product of their individual failure probabilities, leaving us with the following equation for our confidence in a<br>new tuple.</p>
<script type="math/tex; mode=display">Conf(t) = 1 - \prod_{p \in P'} (1-Conf(p))</script><p>Setting conservative confidence thresholds for the acceptance of new patterns and tuples during the bootstrapping process helps prevent the system from drifting away from the targeted relation.</p>
<h3 id="Distant-Supervision-for-Relation-Extration"><a href="#Distant-Supervision-for-Relation-Extration" class="headerlink" title="Distant Supervision for Relation Extration"></a>Distant Supervision for Relation Extration</h3><p>It combines the advantages of bootstrapping and supervised learning, uses a large dataset to acquire a huge number of seed examples, creates lots of noisy pattern features from all these examples and then combines them in a supervised classifier.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">function DISTANT SUPERVISION(Database D, Text T) returns relation classifier C</span><br><span class="line">	foreach relation R</span><br><span class="line">		foreach tuple (e1,e2) of entities <span class="keyword">with</span> relation R <span class="keyword">in</span> D</span><br><span class="line">			sentences←Sentences <span class="keyword">in</span> T that contain e1 <span class="keyword">and</span> e2</span><br><span class="line">			f←Frequent features <span class="keyword">in</span> sentences</span><br><span class="line">			observations←observations + new training tuple (e1, e2, f, R)</span><br><span class="line">	C←Train supervised classifier on observations</span><br><span class="line">	<span class="keyword">return</span> C</span><br></pre></td></tr></table></figure>
<p>Distant supervision can only help in extracting relations for which a large enough database already exists. To extract new relations without datasets, or relations for new domains, purely unsupervised methods must be used.</p>
<h3 id="Unsupervised-Relation-Extraction"><a href="#Unsupervised-Relation-Extraction" class="headerlink" title="Unsupervised Relation Extraction"></a>Unsupervised Relation Extraction</h3><p>The goal of unsupervised relation extraction is to extract relations from the web when we have no labeled training data, and not even any list of relations. This task is often called <strong>open information extraction or Open IE</strong>. </p>
<p>The <a href="http://reverb.cs.washington.edu/" target="_blank" rel="noopener">ReVerb - Open Information Extraction Software</a> system extracts a relation from a sentence s in 4 steps:</p>
<ul>
<li>Run a part-of-speech tagger and entity chunker over s</li>
<li>For each verb in s, find the longest sequence of words w that start with a verb and satisfy syntactic and lexical constraints, merging adjacent matches.</li>
<li>For each phrase w, find the nearest noun phrase x to the left which is not a relative pronoun, wh-word or existential “there”. Find the nearest noun phrase y to the right.</li>
<li>Assign confidence c to the relation r=(x, w, y) using a confidence classfier and return it.</li>
</ul>
<p>A confidence classifier is then trained on this hand-labeled data, using features of the relation and the surrounding words. </p>
<p>The disadvantage is the need to map these large sets of strings into some canonical form for adding to databases or other knowledge sources. Current methods focus heavily on relations expressed with verbs, and so will miss many relations that are expressed nominally.</p>
<h3 id="Evaluation-of-Relation-Extraction"><a href="#Evaluation-of-Relation-Extraction" class="headerlink" title="Evaluation of Relation Extraction"></a>Evaluation of Relation Extraction</h3><p>Semi-supervised and unsupervised methods are much more difficult to evaluate, since they extract totally new relations from the web or a large text. It’s possible to approximate (only) precision by drawing a<br>random sample of relations from the output, and having a human check the accuracy of each of these relations. </p>
<ul>
<li><p>Only want to know if the system can discover and don’t care how many times it discovers </p>
<script type="math/tex; mode=display">\hat p = \frac {\# of\ correctly\ extracted\ relation\ tuples\ in\ the\ sample}{total \# of\ extracted\ relation\ tuples\ in\ the\ sample}</script></li>
<li><p>Another approach that gives us a little bit of information about recall is to compute precision at different levels of recall. Assuming that our system is able to rank the relations it produces (by probability, or confidence) we can separately compute precision for the top 1000 new relations, the top 10,000 new relations, the top 100,000, and so on.</p>
</li>
</ul>
<h2 id="Extracting-Times"><a href="#Extracting-Times" class="headerlink" title="Extracting Times"></a>Extracting Times</h2><p>Times and dates are those temporal expressions, must be normalized —— converted to a standard format.</p>
<h3 id="Temporal-Expression-Extraction"><a href="#Temporal-Expression-Extraction" class="headerlink" title="Temporal Expression Extraction"></a>Temporal Expression Extraction</h3><p>Temporal expressions are those that refer to absolute points in <strong>time, relative times, durations</strong>, and sets of these.</p>
<p>Temporal expressions are grammatical constructions that have temporal lexical triggersas their heads. Lexical triggers might be nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist of their phrasalprojections: noun phrases, adjective pharases, and adverbial phrases.</p>
<blockquote>
<p>Noun: morning, noon, night, winter, dusk, dawn<br>Proper Noun: January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet<br>Adjective: recent, past, annual, former<br>Adverb: hourly, daily, monthly, yearly</p>
</blockquote>
<p><strong>Rule-based approaches</strong> to temporal expression recognition use cascades of automata to recognize patterns at increasing levels of complexity.</p>
<p><strong>Sequence-labeling approaches</strong> follow the same IOB schema used for named-entity tags. Features are extracted from the token and its context. Tipical features used to train IOB-style temporal expression taggers:</p>
<ul>
<li>Token: The target token to be labeled</li>
<li>Tokens in window: Bag of tokens in the windown around a target</li>
<li>Shape: Character shape features</li>
<li>POS: Parts of speech of target and window words</li>
<li>Chunk tags: Base-phrase phrase tag for target and words in a window </li>
<li>Lexical triggers</li>
</ul>
<p>A major difficulty is avoiding expressions that trigger false positives:</p>
<ul>
<li><em>1984</em> tells the story of Winston Smith</li>
<li>U2’s classic <em>Sunday</em> Bloody <em>Sunday</em></li>
</ul>
<p>All these above are suitable for Chinese.</p>
<h3 id="Temporal-Normalization"><a href="#Temporal-Normalization" class="headerlink" title="Temporal Normalization"></a>Temporal Normalization</h3><p><strong>Temporal normalization</strong> is the process of mapping a temporal expression to either temporal normalization a specific point in time or to a duration.</p>
<p>Normalized times are represented with the VALUE attribute from the ISO 8601 standard for encoding temporal values.</p>
<p>Most temporal expressions in news articles are incomplete and are only implicitly anchored, often with respect to the dateline of the article, which we refer to as the document’s <strong>temporal anchor</strong>. Relative temporal expressions are handled with temporal arithmetic similar to that used for <em>today and yesterday</em>. Such ambiguities are handled by encoding language and domain-specific heuristics into the temporal attachments.</p>
<h2 id="Extracting-Events-and-their-Times"><a href="#Extracting-Events-and-their-Times" class="headerlink" title="Extracting Events and their Times"></a>Extracting Events and their Times</h2><p><strong>Event extraction</strong> is to identify mentions of events in texts. Events are to be classified as actions, states, reporting events (say, report, tell, explain), perception events, and so on. Supervised learning for sequence models with IOB tagging. Common features like:</p>
<ul>
<li>Character affixes: Character-level prefixes and suffixes of target word</li>
<li>Nominalization suffix: Character level suffixes for nominalizations (e.g., -tion)</li>
<li>Part of speech: Part of speech of the target word</li>
<li>Light verb: Binary feature indicating that the target is governed by a light verb</li>
<li>Subject syntactic category: Syntactic category of the subject of the sentence</li>
<li>Morphological stem: Stemmed version of the target word</li>
<li>Verb root: Root form of the verb basis for a nominalization</li>
<li>WordNet hypernyms: Hypernym set for the target</li>
</ul>
<h3 id="Temporal-Ordering-of-Events"><a href="#Temporal-Ordering-of-Events" class="headerlink" title="Temporal Ordering of Events"></a>Temporal Ordering of Events</h3><p>The temporal relation between events is classified into one of the <strong>Allen relations</strong> standard set.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch17-5.jpeg" alt=""></p>
<h2 id="Template-Filling"><a href="#Template-Filling" class="headerlink" title="Template Filling"></a>Template Filling</h2><p><strong>Template filling</strong> is to find documents that invoke particular scripts and then fill the slots in the associated templates with fillers extracted from the text. These slot-fillers may consist of text segments extracted directly from the text, or they may consist of concepts that have been inferred from text elements through some additional processing.</p>
<h3 id="Machine-Learning-Approaches-to-Template-Filling"><a href="#Machine-Learning-Approaches-to-Template-Filling" class="headerlink" title="Machine Learning Approaches to Template Filling"></a>Machine Learning Approaches to Template Filling</h3><p>Two supervised systems:</p>
<ul>
<li><strong>Template recognition</strong>: decides whether the template is present in a particular sentence<ul>
<li>Features extracted from every sequence of words that was labeled in training documents as filling any slot from the template being detected.</li>
<li>The usual set of features can be used: tokens, embeddings, word shapes, part-of-speech tags,<br>  syntactic chunk tags, and named entity tags.</li>
</ul>
</li>
<li><strong>Role-filler extraction</strong>: detects each role (DATE, AMOUNT, and so on)<ul>
<li>Run on every noun-phrase in the parsed input sentence, or a sequence model run over sequences of words.</li>
<li>The usual set of features can be used.</li>
</ul>
</li>
</ul>
<p>Recent work focuses on extracting templates in cases where there is no training data or even predefined templates, by inducing templates as sets of linked events: <a href="http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/chambers2011.pdf" target="_blank" rel="noopener">Template-Based Information Extraction without the Templates</a>. </p>
<h3 id="Earlier-Finite-State-Template-Filling-Systems"><a href="#Earlier-Finite-State-Template-Filling-Systems" class="headerlink" title="Earlier Finite-State Template-Filling Systems"></a>Earlier Finite-State Template-Filling Systems</h3><p>Early systems for dealing with complex templates were based on cascades of transducers based on hand-written rules. </p>
<p>The first four stages use hand-written regular expression and grammar rules to do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and events with a FST-based recognizer and inserts the recognized objects into the appropriate slots in templates. The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events.</p>
<p><img src="http://qnimg.lovevivian.cn/slp-ch17-6.jpeg" alt=""></p>
<p>This is an example of <a href="https://www.isi.edu/~hobbs/fastus-schabes-jul95.pdf" target="_blank" rel="noopener">FASTUS: A Cascaded Finite-State Transducer for Extracting Information from Natural-Language Text</a>.</p>
<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><ul>
<li><strong>Named entities</strong> can be recognized and classified by featured-based or neural sequence labeling techniques.</li>
<li><strong>Relations among entities</strong> can be extracted by <strong>pattern-based approaches</strong>, <strong>supervised learning methods</strong> when annotated training data is available, <strong>lightly supervised bootstrapping methods</strong> when small numbers of seed tuples or seed patterns are available, <strong>distant supervision</strong> when a database of relations is available, and <strong>unsupervised or Open IE methods</strong>.</li>
<li>Reasoning about time can be facilitated by detection and normalization of <strong>temporal expressions</strong> through a combination of statistical learning and rulebased methods.</li>
<li><p><strong>Events</strong> can be detected and ordered in time using sequence models and classifiers trained on temporally- and event-labeled data like the TimeBank corpus.</p>
</li>
<li><p><strong>Template-filling</strong> applications can recognize stereotypical situations in texts and assign elements from the text to roles represented as fixed sets of slots.</p>
</li>
</ul>
<h1 id="Practice"><a href="#Practice" class="headerlink" title="Practice"></a>Practice</h1><p><a href="https://www.searchtechnologies.com/blog/natural-language-processing-techniques" target="_blank" rel="noopener">This post</a> provides industrial level content and also an overall framework.</p>
<p><a href="https://www.nltk.org/book/ch07.html" target="_blank" rel="noopener">The NLTK Book</a> provides a practical tutorial.</p>
<p><a href="https://towardsdatascience.com/deep-learning-for-specific-information-extraction-from-unstructured-texts-12c5b9dceada" target="_blank" rel="noopener">This post</a> provides an example of specific information extraction from unstructured texts (CVs). The steps are below:</p>
<ul>
<li>Parts of speech tagging<ul>
<li>using NLTK library</li>
<li>use a regular expression or model to extract noun phrases examples</li>
<li>results with a number of entities among which some are the target skills and some are not</li>
</ul>
</li>
<li>DeepLearning for candidates classification<ul>
<li>train a model with a labeled training set</li>
<li>The set of features used for training is composed regarding the structure of the candidate phrase and the context.<ul>
<li>Each word’s vector is comprised of such binary features as occurrence of numbers or other special characters, capitalization of the first letter or of the whole word.</li>
<li>Also check if a word appears in the English language vocabulary and in some thematic lists such as names, geographical names, etc.</li>
<li>Usage of another binary feature describing presence of the popular English prefixes and suffixes in a candidate.</li>
</ul>
</li>
<li>The classification has three input layers each designed to take special class of data<ul>
<li>features of candidate phrases</li>
<li>context structure information: 3 neighbouring words to the left and right of candidate phrase respectively</li>
<li>coordinate wise maximum and minimum values of word vectors in the phrase and its context which, among the other information, represent the presence or absence of many binary features in the whole phrase.</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>I was a little confused about the input layers, and I’ve posted a question.</p>
</blockquote>
<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">Speech and Language Processing</a></li>
<li><a href="https://www.nltk.org/book/ch07.html" target="_blank" rel="noopener">7. Extracting Information from Text</a></li>
<li><a href="https://www.searchtechnologies.com/blog/natural-language-processing-techniques" target="_blank" rel="noopener">Natural Language Processing (NLP) Techniques for Extracting Information | Search Technologies</a></li>
<li><a href="https://towardsdatascience.com/deep-learning-for-specific-information-extraction-from-unstructured-texts-12c5b9dceada" target="_blank" rel="noopener">Deep learning for specific information extraction from unstructured texts</a></li>
<li><a href="https://arxiv.org/abs/1807.02383" target="_blank" rel="noopener">[1807.02383] Natural Language Processing for Information Extraction</a></li>
</ul>
<h2 id="Repos"><a href="#Repos" class="headerlink" title="Repos"></a>Repos</h2><p>Application level:</p>
<ul>
<li><a href="https://github.com/snipsco/snips-nlu" target="_blank" rel="noopener">snipsco/snips-nlu: Snips Python library to extract meaning from text</a></li>
<li><a href="https://github.com/machinalis/iepy" target="_blank" rel="noopener">machinalis/iepy: Information Extraction in Python Focused on Relation Extraction</a></li>
</ul>
<p>Non-DeepLearning:</p>
<ul>
<li><a href="https://github.com/marcolagi/quantulum" target="_blank" rel="noopener">marcolagi/quantulum: Python library for information extraction of quantities from unstructured text</a></li>
<li><a href="https://github.com/gabrielStanovsky/template-oie#prerequisites" target="_blank" rel="noopener">gabrielStanovsky/template-oie: Extract templated Open Information Extraction</a></li>
<li><a href="https://github.com/philipperemy/information-extraction-with-dominating-rules" target="_blank" rel="noopener">information-extraction: based on Stanford open IE Library and domination decision rules</a></li>
</ul>
<p>DeepLearning (Suitable for study): </p>
<ul>
<li><a href="https://github.com/crownpku/Information-Extraction-Chinese/tree/master/RE_BGRU_2ATT" target="_blank" rel="noopener">Information-Extraction-Chinese/RE_BGRU_2ATT at master · crownpku/Information-Extraction-Chinese</a></li>
<li><a href="https://github.com/thunlp/OpenNRE" target="_blank" rel="noopener">An Open-Source Package for Neural Relation Extraction (NRE) implemented in TensorFlow</a></li>
<li><a href="https://github.com/yuhaozhang/tacred-relation" target="_blank" rel="noopener">PyTorch implementation of the position-aware attention model for relation extraction</a></li>
</ul>
<p>Dataset:</p>
<ul>
<li><a href="https://github.com/davidsbatista/Annotated-Semantic-Relationships-Datasets" target="_blank" rel="noopener">davidsbatista/Annotated-Semantic-Relationships-Datasets: Public and free annotated datasets of relationships between entities/nominals</a></li>
<li><a href="http://www.timeml.org/timebank/timebank.html" target="_blank" rel="noopener">TimeBank</a></li>
</ul>
<h2 id="CHANGELOG"><a href="#CHANGELOG" class="headerlink" title="CHANGELOG"></a>CHANGELOG</h2><ul>
<li>20190402 Created</li>
</ul>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2019/04/09/SLP/2019-04-09-Information-Extraction/">
    <time datetime="2019-04-09T14:00:00.000Z" class="entry-date">
        2019-04-09
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/Feeling/">Feeling</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/IE/">IE</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Information-Extraction/">Information Extraction</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/">NLP</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2019/04/10/Py4F/2019-04-10-Python-for-Freshman-Ch00/" rel="prev"><span class="meta-nav">←</span> Python 小白快速从入门到放弃：在开始前</a></span>
    
    
        <span class="nav-next"><a href="/2019/04/08/NLPFA/2019-04-08-Ch18-Rationalism-and-Empiricism-in-NLP/" rel="next">自然语言计算机形式分析的理论与方法笔记(Ch18) <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







<div class="bdsharebuttonbox"><a href="#" class="bds_more" data-cmd="more"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a></div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{},"image":{"viewList":["fbook","twi","linkedin","qzone","tsina","douban","weixin","evernotecn"],"viewText":"分享到：","viewSize":"16"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/static/api/js/share.js?v=89860593.js?'];</script>

<section id="comment">
  <!-- 评论代码 -->
  <div id="gitalk-container"></div>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
  <script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
  <script>
  const gitalk = new Gitalk({
    clientID: '0eb512031083d6e7edfb',
    clientSecret: 'e830808995dd813ca26fed50573760963457da37',
    repo: 'hscspring.github.io',
    owner: 'hscspring',
    admin: ['hscspring'],
    id: md5(location.pathname),
    distractionFreeMode: false
  })
  gitalk.render('gitalk-container')
  </script>
  <!-- 评论代码已完成 -->
</section>

</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  <aside class="widget">
    <h3 class="widget-title">Music</h3>
    <div class="widget-content">
      <audio src="http://qnimg.lovevivian.cn/miss.mp3" controls="controls"
             style="width:100%">
        您的浏览器不支持 audio 标签。
      </audio>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Coding/">Coding</a><span class="category-list-count">34</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Feeling/">Feeling</a><span class="category-list-count">50</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Thinking/">Thinking</a><span class="category-list-count">13</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2019/12/15/DS/2019-12-15-Coding-Review2-Explanation-Vol1/">剑指 Offer2（Python 版）解析（上）</a>
          </li>
        
          <li>
            <a href="/2019/12/15/Paper/2019-12-15-Few-Shot-Charge-Prediction-with-Discriminative-Legal-Attributes/">Few-Shot Charge Prediction with Discriminative Legal Attributes Note</a>
          </li>
        
          <li>
            <a href="/2019/12/11/KG/2019-12-11-Relationship-Extraction/">关系提取简述</a>
          </li>
        
          <li>
            <a href="/2019/12/10/Rust/2019-12-10-Rust-Programming-Language-Brief-Note-Vol2/">The Rust Programming Language Brief Note (Vol2)</a>
          </li>
        
          <li>
            <a href="/2019/12/09/2019-12-09-AINLP-GPU-Guide/">AINLP GPU 使用体验指南</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AE/">AE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI/">AI</a><span class="tag-list-count">43</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AR/">AR</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Algorithm/">Algorithm</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Array/">Array</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Attention/">Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Automatic-Speech-Processing/">Automatic Speech Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Backward/">Backward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bayes/">Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Beam-Search/">Beam Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bert/">Bert</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Binary-Search/">Binary Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Business/">Business</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/">C</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CCG/">CCG</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CFG/">CFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CKY/">CKY</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/CYK/">CYK</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Calculus/">Calculus</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChatBot/">ChatBot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Chunking/">Chunking</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Code/">Code</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Collins-Parser/">Collins Parser</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computational-Linguistics/">Computational Linguistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer/">Computer</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Computer-Science/">Computer Science</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Confusing-Labels/">Confusing Labels</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Context-Free-Grammars/">Context-Free Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cosine/">Cosine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Entropy/">Cross Entropy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-brackets/">Cross-brackets</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ctrl/">Ctrl</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DB/">DB</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DNN/">DNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DP/">DP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Clearing/">Data Clearing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Science/">Data Science</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Structure/">Data Structure</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Database/">Database</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Decoding/">Decoding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/">DeepLearning</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dependence/">Dependence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Diary/">Diary</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Django/">Django</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ERNIE/">ERNIE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Economics/">Economics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Edit-Distance/">Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Elasticsearch/">Elasticsearch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Electra/">Electra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Elixir/">Elixir</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Embeddings/">Embeddings</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Entropy/">Entropy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Evaluation/">Evaluation</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/F1/">F1</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FDW/">FDW</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FSM/">FSM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feature-based/">Feature-based</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Few-Shot/">Few-Shot</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Fine-tuning/">Fine-tuning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Analysis/">Formal Analysis</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Formal-Grammars/">Formal Grammars</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Forward/">Forward</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Full-Text-Search/">Full-Text-Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Function-Syntax/">Function Syntax</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GPU/">GPU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GRU/">GRU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gan/">Gan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Garden-path/">Garden-path</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Gradient-Descent/">Gradient Descent</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GraphQL/">GraphQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Grid-Grammar/">Grid Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HMM/">HMM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/IE/">IE</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Imbalance-Data/">Imbalance Data</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Industry/">Industry</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Extraction/">Information Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Information-Theory/">Information Theory</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Job/">Job</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Knowledge-Graph/">Knowledge Graph</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LM/">LM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LSTM/">LSTM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Language-Model/">Language Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexical-Semantics/">Lexical Semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalism/">Lexicalism</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalized-CFG/">Lexicalized CFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lexicalized-Grammars/">Lexicalized Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Algebra/">Linear Algebra</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Sturcture/">Linear Sturcture</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linked-List/">Linked List</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logistic-Regression/">Logistic Regression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Lucene/">Lucene</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MEMM/">MEMM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine/">Machine</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Translation/">Machine Translation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Manacher/">Manacher</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Markov/">Markov</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Materialized-Views/">Materialized Views</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math/">Math</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Median/">Median</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Minimum-Edit-Distance/">Minimum Edit Distance</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Module/">Module</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Multiway-Tree/">Multiway Tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLG/">NLG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLM/">NLM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLP/">NLP</a><span class="tag-list-count">48</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NLU/">NLU</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Naive-Bayes/">Naive Bayes</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neo4j/">Neo4j</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ngram/">Ngram</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NumPy/">NumPy</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Occupation/">Occupation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Orientation/">Orientation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCCG/">PCCG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCFG/">PCFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PPMI/">PPMI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Palindromic/">Palindromic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper/">Paper</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Partial-Parsing/">Partial Parsing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Philosophy/">Philosophy</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammar/">Phrase Structure Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Phrase-Structure-Grammars/">Phrase Structure Grammars</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PoS/">PoS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Position-Encoding/">Position-Encoding</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Postgres/">Postgres</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pragmatic-Automatic-Processing/">Pragmatic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pre-training/">Pre-training</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pretraining/">Pretraining</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Grammar/">Probabilistic Grammar</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Model/">Probabilistic Model</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PyPI/">PyPI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a><span class="tag-list-count">16</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Query/">Query</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Queue/">Queue</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RNN/">RNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regex/">Regex</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Regular-Expression/">Regular Expression</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Relationship-Extraction/">Relationship Extraction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Reqular-Expressions/">Reqular Expressions</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Rust/">Rust</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SCFG/">SCFG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SGD/">SGD</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SQL/">SQL</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SRN/">SRN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Search/">Search</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Self-Attention/">Self-Attention</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Semantic-Automatic-Processing/">Semantic Automatic Processing</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sentiment-Classification/">Sentiment Classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simon/">Simon</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Simpson-Paradox/">Simpson Paradox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Slide/">Slide</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Smoothing/">Smoothing</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sort/">Sort</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spell-Check/">Spell Check</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stack/">Stack</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Statistics/">Statistics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/String/">String</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Style/">Style</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Substring/">Substring</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Supertagging/">Supertagging</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/">System</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TF-IDF/">TF-IDF</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tagging/">Tagging</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Test/">Test</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Text-Generation/">Text Generation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Text-Normalization/">Text Normalization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer/">Transformer</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Transformer-XL/">Transformer-XL</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tree/">Tree</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Treebank/">Treebank</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Unity-Operation/">Unity Operation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vagrant/">Vagrant</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Valence/">Valence</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vector-Semantics/">Vector Semantics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/VirtualBox/">VirtualBox</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visualization/">Visualization</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Viterbi/">Viterbi</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Word2vec/">Word2vec</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Work/">Work</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/XLNet/">XLNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZhouZhihua/">ZhouZhihua</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Zipf/">Zipf</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/knowledge-Graph/">knowledge Graph</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ssh/">ssh</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-content tagcloud">
      <a href="/tags/AE/" style="font-size: 10px;">AE</a> <a href="/tags/AI/" style="font-size: 18.57px;">AI</a> <a href="/tags/AR/" style="font-size: 10px;">AR</a> <a href="/tags/Algorithm/" style="font-size: 10px;">Algorithm</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/Attention/" style="font-size: 10px;">Attention</a> <a href="/tags/Automatic-Speech-Processing/" style="font-size: 10px;">Automatic Speech Processing</a> <a href="/tags/Backward/" style="font-size: 10px;">Backward</a> <a href="/tags/Bayes/" style="font-size: 10px;">Bayes</a> <a href="/tags/Beam-Search/" style="font-size: 10px;">Beam Search</a> <a href="/tags/Bert/" style="font-size: 10px;">Bert</a> <a href="/tags/Binary-Search/" style="font-size: 10px;">Binary Search</a> <a href="/tags/Business/" style="font-size: 10px;">Business</a> <a href="/tags/C/" style="font-size: 11.43px;">C</a> <a href="/tags/CCG/" style="font-size: 11.43px;">CCG</a> <a href="/tags/CFG/" style="font-size: 10px;">CFG</a> <a href="/tags/CKY/" style="font-size: 10px;">CKY</a> <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/CYK/" style="font-size: 10px;">CYK</a> <a href="/tags/Calculus/" style="font-size: 10px;">Calculus</a> <a href="/tags/ChatBot/" style="font-size: 10px;">ChatBot</a> <a href="/tags/Chunking/" style="font-size: 10px;">Chunking</a> <a href="/tags/Code/" style="font-size: 10px;">Code</a> <a href="/tags/Collins-Parser/" style="font-size: 10px;">Collins Parser</a> <a href="/tags/Computational-Linguistics/" style="font-size: 10px;">Computational Linguistics</a> <a href="/tags/Computer/" style="font-size: 10px;">Computer</a> <a href="/tags/Computer-Science/" style="font-size: 14.29px;">Computer Science</a> <a href="/tags/Confusing-Labels/" style="font-size: 10px;">Confusing Labels</a> <a href="/tags/Context-Free-Grammars/" style="font-size: 10px;">Context-Free Grammars</a> <a href="/tags/Cosine/" style="font-size: 10px;">Cosine</a> <a href="/tags/Cross-Entropy/" style="font-size: 10px;">Cross Entropy</a> <a href="/tags/Cross-brackets/" style="font-size: 10px;">Cross-brackets</a> <a href="/tags/Ctrl/" style="font-size: 10px;">Ctrl</a> <a href="/tags/DB/" style="font-size: 11.43px;">DB</a> <a href="/tags/DNN/" style="font-size: 10px;">DNN</a> <a href="/tags/DP/" style="font-size: 10px;">DP</a> <a href="/tags/Data-Clearing/" style="font-size: 10px;">Data Clearing</a> <a href="/tags/Data-Science/" style="font-size: 12.86px;">Data Science</a> <a href="/tags/Data-Structure/" style="font-size: 15.71px;">Data Structure</a> <a href="/tags/Database/" style="font-size: 10px;">Database</a> <a href="/tags/Decoding/" style="font-size: 10px;">Decoding</a> <a href="/tags/DeepLearning/" style="font-size: 14.29px;">DeepLearning</a> <a href="/tags/Dependence/" style="font-size: 10px;">Dependence</a> <a href="/tags/Diary/" style="font-size: 11.43px;">Diary</a> <a href="/tags/Django/" style="font-size: 10px;">Django</a> <a href="/tags/ERNIE/" style="font-size: 10px;">ERNIE</a> <a href="/tags/Economics/" style="font-size: 10px;">Economics</a> <a href="/tags/Edit-Distance/" style="font-size: 10px;">Edit Distance</a> <a href="/tags/Elasticsearch/" style="font-size: 10px;">Elasticsearch</a> <a href="/tags/Electra/" style="font-size: 10px;">Electra</a> <a href="/tags/Elixir/" style="font-size: 11.43px;">Elixir</a> <a href="/tags/Embeddings/" style="font-size: 11.43px;">Embeddings</a> <a href="/tags/Entropy/" style="font-size: 11.43px;">Entropy</a> <a href="/tags/Evaluation/" style="font-size: 11.43px;">Evaluation</a> <a href="/tags/F1/" style="font-size: 10px;">F1</a> <a href="/tags/FDW/" style="font-size: 10px;">FDW</a> <a href="/tags/FSM/" style="font-size: 10px;">FSM</a> <a href="/tags/Feature-based/" style="font-size: 10px;">Feature-based</a> <a href="/tags/Few-Shot/" style="font-size: 10px;">Few-Shot</a> <a href="/tags/Fine-tuning/" style="font-size: 10px;">Fine-tuning</a> <a href="/tags/Formal-Analysis/" style="font-size: 10px;">Formal Analysis</a> <a href="/tags/Formal-Grammars/" style="font-size: 12.86px;">Formal Grammars</a> <a href="/tags/Forward/" style="font-size: 10px;">Forward</a> <a href="/tags/Full-Text-Search/" style="font-size: 10px;">Full-Text-Search</a> <a href="/tags/Function-Syntax/" style="font-size: 10px;">Function Syntax</a> <a href="/tags/GPU/" style="font-size: 10px;">GPU</a> <a href="/tags/GRU/" style="font-size: 10px;">GRU</a> <a href="/tags/Gan/" style="font-size: 10px;">Gan</a> <a href="/tags/Garden-path/" style="font-size: 10px;">Garden-path</a> <a href="/tags/Gradient-Descent/" style="font-size: 10px;">Gradient Descent</a> <a href="/tags/GraphQL/" style="font-size: 11.43px;">GraphQL</a> <a href="/tags/Grid-Grammar/" style="font-size: 10px;">Grid Grammar</a> <a href="/tags/HMM/" style="font-size: 11.43px;">HMM</a> <a href="/tags/IE/" style="font-size: 10px;">IE</a> <a href="/tags/Imbalance-Data/" style="font-size: 10px;">Imbalance Data</a> <a href="/tags/Industry/" style="font-size: 10px;">Industry</a> <a href="/tags/Information-Extraction/" style="font-size: 10px;">Information Extraction</a> <a href="/tags/Information-Theory/" style="font-size: 10px;">Information Theory</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/Knowledge-Graph/" style="font-size: 11.43px;">Knowledge Graph</a> <a href="/tags/LM/" style="font-size: 11.43px;">LM</a> <a href="/tags/LSTM/" style="font-size: 10px;">LSTM</a> <a href="/tags/Language-Model/" style="font-size: 10px;">Language Model</a> <a href="/tags/Lexical-Semantics/" style="font-size: 10px;">Lexical Semantics</a> <a href="/tags/Lexicalism/" style="font-size: 10px;">Lexicalism</a> <a href="/tags/Lexicalized-CFG/" style="font-size: 10px;">Lexicalized CFG</a> <a href="/tags/Lexicalized-Grammars/" style="font-size: 10px;">Lexicalized Grammars</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Linear-Sturcture/" style="font-size: 10px;">Linear Sturcture</a> <a href="/tags/Linked-List/" style="font-size: 10px;">Linked List</a> <a href="/tags/Logistic-Regression/" style="font-size: 10px;">Logistic Regression</a> <a href="/tags/Lucene/" style="font-size: 10px;">Lucene</a> <a href="/tags/MEMM/" style="font-size: 10px;">MEMM</a> <a href="/tags/Machine/" style="font-size: 10px;">Machine</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/Machine-Translation/" style="font-size: 10px;">Machine Translation</a> <a href="/tags/Manacher/" style="font-size: 10px;">Manacher</a> <a href="/tags/Markov/" style="font-size: 10px;">Markov</a> <a href="/tags/Materialized-Views/" style="font-size: 10px;">Materialized Views</a> <a href="/tags/Math/" style="font-size: 11.43px;">Math</a> <a href="/tags/Matplotlib/" style="font-size: 10px;">Matplotlib</a> <a href="/tags/Median/" style="font-size: 10px;">Median</a> <a href="/tags/Minimum-Edit-Distance/" style="font-size: 10px;">Minimum Edit Distance</a> <a href="/tags/Module/" style="font-size: 10px;">Module</a> <a href="/tags/Multiway-Tree/" style="font-size: 10px;">Multiway Tree</a> <a href="/tags/NLG/" style="font-size: 10px;">NLG</a> <a href="/tags/NLM/" style="font-size: 10px;">NLM</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/NLU/" style="font-size: 10px;">NLU</a> <a href="/tags/Naive-Bayes/" style="font-size: 10px;">Naive Bayes</a> <a href="/tags/Neo4j/" style="font-size: 10px;">Neo4j</a> <a href="/tags/Ngram/" style="font-size: 11.43px;">Ngram</a> <a href="/tags/NumPy/" style="font-size: 10px;">NumPy</a> <a href="/tags/Occupation/" style="font-size: 10px;">Occupation</a> <a href="/tags/Orientation/" style="font-size: 10px;">Orientation</a> <a href="/tags/PCCG/" style="font-size: 10px;">PCCG</a> <a href="/tags/PCFG/" style="font-size: 10px;">PCFG</a> <a href="/tags/PPMI/" style="font-size: 10px;">PPMI</a> <a href="/tags/Palindromic/" style="font-size: 10px;">Palindromic</a> <a href="/tags/Pandas/" style="font-size: 10px;">Pandas</a> <a href="/tags/Paper/" style="font-size: 10px;">Paper</a> <a href="/tags/Partial-Parsing/" style="font-size: 10px;">Partial Parsing</a> <a href="/tags/Philosophy/" style="font-size: 11.43px;">Philosophy</a> <a href="/tags/Phrase-Structure-Grammar/" style="font-size: 10px;">Phrase Structure Grammar</a> <a href="/tags/Phrase-Structure-Grammars/" style="font-size: 10px;">Phrase Structure Grammars</a> <a href="/tags/PoS/" style="font-size: 10px;">PoS</a> <a href="/tags/Position-Encoding/" style="font-size: 10px;">Position-Encoding</a> <a href="/tags/Postgres/" style="font-size: 11.43px;">Postgres</a> <a href="/tags/Pragmatic-Automatic-Processing/" style="font-size: 10px;">Pragmatic Automatic Processing</a> <a href="/tags/Pre-training/" style="font-size: 11.43px;">Pre-training</a> <a href="/tags/Pretraining/" style="font-size: 11.43px;">Pretraining</a> <a href="/tags/Probabilistic-Grammar/" style="font-size: 10px;">Probabilistic Grammar</a> <a href="/tags/Probabilistic-Model/" style="font-size: 10px;">Probabilistic Model</a> <a href="/tags/PyPI/" style="font-size: 10px;">PyPI</a> <a href="/tags/Python/" style="font-size: 17.14px;">Python</a> <a href="/tags/Query/" style="font-size: 10px;">Query</a> <a href="/tags/Queue/" style="font-size: 10px;">Queue</a> <a href="/tags/RNN/" style="font-size: 10px;">RNN</a> <a href="/tags/Regex/" style="font-size: 10px;">Regex</a> <a href="/tags/Regular-Expression/" style="font-size: 10px;">Regular Expression</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 10px;">Reinforcement Learning</a> <a href="/tags/Relationship-Extraction/" style="font-size: 10px;">Relationship Extraction</a> <a href="/tags/Reqular-Expressions/" style="font-size: 10px;">Reqular Expressions</a> <a href="/tags/Rust/" style="font-size: 11.43px;">Rust</a> <a href="/tags/SCFG/" style="font-size: 10px;">SCFG</a> <a href="/tags/SGD/" style="font-size: 10px;">SGD</a> <a href="/tags/SQL/" style="font-size: 11.43px;">SQL</a> <a href="/tags/SRN/" style="font-size: 10px;">SRN</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Search/" style="font-size: 10px;">Search</a> <a href="/tags/Self-Attention/" style="font-size: 10px;">Self-Attention</a> <a href="/tags/Semantic-Automatic-Processing/" style="font-size: 10px;">Semantic Automatic Processing</a> <a href="/tags/Sentiment-Classification/" style="font-size: 10px;">Sentiment Classification</a> <a href="/tags/Simon/" style="font-size: 10px;">Simon</a> <a href="/tags/Simpson-Paradox/" style="font-size: 10px;">Simpson Paradox</a> <a href="/tags/Slide/" style="font-size: 10px;">Slide</a> <a href="/tags/Smoothing/" style="font-size: 11.43px;">Smoothing</a> <a href="/tags/Sort/" style="font-size: 11.43px;">Sort</a> <a href="/tags/Spell-Check/" style="font-size: 10px;">Spell Check</a> <a href="/tags/Stack/" style="font-size: 10px;">Stack</a> <a href="/tags/Statistics/" style="font-size: 10px;">Statistics</a> <a href="/tags/String/" style="font-size: 10px;">String</a> <a href="/tags/Style/" style="font-size: 10px;">Style</a> <a href="/tags/Substring/" style="font-size: 10px;">Substring</a> <a href="/tags/Supertagging/" style="font-size: 10px;">Supertagging</a> <a href="/tags/System/" style="font-size: 11.43px;">System</a> <a href="/tags/TF-IDF/" style="font-size: 10px;">TF-IDF</a> <a href="/tags/Tagging/" style="font-size: 10px;">Tagging</a> <a href="/tags/Test/" style="font-size: 10px;">Test</a> <a href="/tags/Text-Generation/" style="font-size: 10px;">Text Generation</a> <a href="/tags/Text-Normalization/" style="font-size: 10px;">Text Normalization</a> <a href="/tags/Transformer/" style="font-size: 14.29px;">Transformer</a> <a href="/tags/Transformer-XL/" style="font-size: 10px;">Transformer-XL</a> <a href="/tags/Tree/" style="font-size: 10px;">Tree</a> <a href="/tags/Treebank/" style="font-size: 10px;">Treebank</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Unity-Operation/" style="font-size: 10px;">Unity Operation</a> <a href="/tags/Vagrant/" style="font-size: 10px;">Vagrant</a> <a href="/tags/Valence/" style="font-size: 10px;">Valence</a> <a href="/tags/Vector-Semantics/" style="font-size: 10px;">Vector Semantics</a> <a href="/tags/VirtualBox/" style="font-size: 10px;">VirtualBox</a> <a href="/tags/Visualization/" style="font-size: 10px;">Visualization</a> <a href="/tags/Viterbi/" style="font-size: 11.43px;">Viterbi</a> <a href="/tags/Word2vec/" style="font-size: 10px;">Word2vec</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/XLNet/" style="font-size: 10px;">XLNet</a> <a href="/tags/ZhouZhihua/" style="font-size: 10px;">ZhouZhihua</a> <a href="/tags/Zipf/" style="font-size: 10px;">Zipf</a> <a href="/tags/knowledge-Graph/" style="font-size: 10px;">knowledge Graph</a> <a href="/tags/ssh/" style="font-size: 10px;">ssh</a>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2019 Yam
    All rights reserved.</p>
    <p>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></p>
    
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <!-- <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_page_pv">本文总阅读量<span id="busuanzi_value_page_pv"></span>次</span> -->
    <!-- <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span> -->

</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>