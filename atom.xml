<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2024-12-01T15:01:09.609Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>hscspring</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【Paper速览】2024年11月</title>
    <link href="https://www.yam.gift/2024/12/01/Paper/2024-12-01-Paper/"/>
    <id>https://www.yam.gift/2024/12/01/Paper/2024-12-01-Paper/</id>
    <published>2024-12-01T15:00:00.000Z</published>
    <updated>2024-12-01T15:01:09.609Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【Paper速览】项目是日常Paper阅读的笔记，每月一篇，内容限于Up感兴趣方向的Paper，记录关键信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLM：上下文压缩、超长上下文、指令跟随、模型架构、继续训练、可控生成、各类应用&lt;/li&gt;
&lt;li&gt;多模态：对齐、融合、语音、视频&lt;/li&gt;
&lt;li&gt;强化学习：在LLM、Embody上的应用&lt;/li&gt;
&lt;li&gt;推理部署：蒸馏、量化、解码、异构、端侧&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Instruction Following" scheme="https://www.yam.gift/tags/Instruction-Following/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Paper" scheme="https://www.yam.gift/tags/Paper/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="Full-Duplex" scheme="https://www.yam.gift/tags/Full-Duplex/"/>
    
      <category term="MM Align" scheme="https://www.yam.gift/tags/MM-Align/"/>
    
      <category term="OMNI" scheme="https://www.yam.gift/tags/OMNI/"/>
    
      <category term="DST" scheme="https://www.yam.gift/tags/DST/"/>
    
      <category term="MM Representation" scheme="https://www.yam.gift/tags/MM-Representation/"/>
    
  </entry>
  
  <entry>
    <title>【Paper速览】2024年10月</title>
    <link href="https://www.yam.gift/2024/11/01/Paper/2024-11-01-Paper/"/>
    <id>https://www.yam.gift/2024/11/01/Paper/2024-11-01-Paper/</id>
    <published>2024-11-01T15:00:00.000Z</published>
    <updated>2024-12-01T15:01:05.358Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;【Paper速览】项目是日常Paper阅读的笔记，每月一篇，内容限于Up感兴趣方向的Paper，记录关键信息。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLM：上下文压缩、超长上下文、指令跟随、模型架构、继续训练、可控生成、各类应用&lt;/li&gt;
&lt;li&gt;多模态：对齐、融合、语音、视频&lt;/li&gt;
&lt;li&gt;强化学习：在LLM、Embody上的应用&lt;/li&gt;
&lt;li&gt;推理部署：蒸馏、量化、解码、异构、端侧&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Prompt" scheme="https://www.yam.gift/tags/Prompt/"/>
    
      <category term="Instruction Following" scheme="https://www.yam.gift/tags/Instruction-Following/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Paper" scheme="https://www.yam.gift/tags/Paper/"/>
    
      <category term="Conflicting Context" scheme="https://www.yam.gift/tags/Conflicting-Context/"/>
    
      <category term="Meta Prompt" scheme="https://www.yam.gift/tags/Meta-Prompt/"/>
    
      <category term="Speech Foundation Model" scheme="https://www.yam.gift/tags/Speech-Foundation-Model/"/>
    
      <category term="Speech Understanding" scheme="https://www.yam.gift/tags/Speech-Understanding/"/>
    
      <category term="RAG" scheme="https://www.yam.gift/tags/RAG/"/>
    
      <category term="System Prompt" scheme="https://www.yam.gift/tags/System-Prompt/"/>
    
      <category term="SLM" scheme="https://www.yam.gift/tags/SLM/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="Continuous Speech Tokenizer" scheme="https://www.yam.gift/tags/Continuous-Speech-Tokenizer/"/>
    
      <category term="Speech Discrete Representation" scheme="https://www.yam.gift/tags/Speech-Discrete-Representation/"/>
    
      <category term="SSL" scheme="https://www.yam.gift/tags/SSL/"/>
    
      <category term="MM Fusion" scheme="https://www.yam.gift/tags/MM-Fusion/"/>
    
      <category term="TTS-LLaMA" scheme="https://www.yam.gift/tags/TTS-LLaMA/"/>
    
      <category term="MoLE-Llama" scheme="https://www.yam.gift/tags/MoLE-Llama/"/>
    
      <category term="NeuGPT" scheme="https://www.yam.gift/tags/NeuGPT/"/>
    
  </entry>
  
  <entry>
    <title>《真希望父母读过这本书》读书笔记</title>
    <link href="https://www.yam.gift/2024/09/30/BabyGrow/2024-09-30-Hope-Parents-Read-the-Book/"/>
    <id>https://www.yam.gift/2024/09/30/BabyGrow/2024-09-30-Hope-Parents-Read-the-Book/</id>
    <published>2024-09-30T14:00:00.000Z</published>
    <updated>2024-10-08T05:01:50.599Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;周六下午和爱人一起去了天目里，散步、聊天、看风景、喝茉酸奶、吃烧鸟、去鸟屋书店读书、去饸饹面馆吃面。这是我们第二次去这里了，主要是那家面馆的面很符合我们口味，但也不能打车过去就吃个面，所以每次都要看一两个小时书。这本书就是这天我读的两本书之一，还剩最后一章没读完，我觉得问题不大，得赶紧把读过的记下来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Life" scheme="https://www.yam.gift/tags/Life/"/>
    
      <category term="BabyGrow" scheme="https://www.yam.gift/tags/BabyGrow/"/>
    
  </entry>
  
  <entry>
    <title>基础和取舍</title>
    <link href="https://www.yam.gift/2024/09/30/Diary/2024-09-30-Work-Design/"/>
    <id>https://www.yam.gift/2024/09/30/Diary/2024-09-30-Work-Design/</id>
    <published>2024-09-30T00:00:00.000Z</published>
    <updated>2024-09-30T11:28:04.257Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;没想到居然一年多没写这样思考性的文字了，ChatGPT后遗症有点大。去年底换工作再加上孩子出生，生活一下子变得异常充实了起来。家庭和育儿方面成长很多，从一开始的没耐心，到逐步理解包容、感同身受（无论对爱人还是孩子），一年不到时间改变了非常多。工作方面也取得了一些成果，强度和深度比此前所有工作都高了一个级别，虽然很忙，但非常开心。比较不满意的是过于忙碌导致没时间夯实基础，总感觉自己比较浮。正好国庆假期，重新整理一下思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Life" scheme="https://www.yam.gift/tags/Life/"/>
    
      <category term="Growth" scheme="https://www.yam.gift/tags/Growth/"/>
    
      <category term="Dream" scheme="https://www.yam.gift/tags/Dream/"/>
    
  </entry>
  
  <entry>
    <title>MIO</title>
    <link href="https://www.yam.gift/2024/09/28/Paper/2024-09-28-MIO/"/>
    <id>https://www.yam.gift/2024/09/28/Paper/2024-09-28-MIO/</id>
    <published>2024-09-28T15:00:00.000Z</published>
    <updated>2024-10-09T15:39:23.866Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/2409.17692&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/2409.17692&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心：多模态输入输出，这里的多模态包括了文本、图像、语音和视频，相比AnyGPT多了视频。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://qnimg.lovevivian.cn/paper-mio-1.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="MIO" scheme="https://www.yam.gift/tags/MIO/"/>
    
      <category term="MultiModal" scheme="https://www.yam.gift/tags/MultiModal/"/>
    
  </entry>
  
  <entry>
    <title>Tiny LLM Continual Pre-training：RHO-1</title>
    <link href="https://www.yam.gift/2024/04/13/NLP/LLM-Training/2024-04-13-LLM-Tiny-Continual-Training-RHO-1/"/>
    <id>https://www.yam.gift/2024/04/13/NLP/LLM-Training/2024-04-13-LLM-Tiny-Continual-Training-RHO-1/</id>
    <published>2024-04-13T15:00:00.000Z</published>
    <updated>2024-06-12T08:30:34.563Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;并不是所有的Next Token都有用，&lt;a href=&quot;http://arxiv.org/abs/2404.07965&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RHO-1&lt;/a&gt;选择那些最有用的Next Token，提升了小模型的继续训练效率。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="RHO-1" scheme="https://www.yam.gift/tags/RHO-1/"/>
    
      <category term="RHO" scheme="https://www.yam.gift/tags/RHO/"/>
    
  </entry>
  
  <entry>
    <title>LLM打街霸</title>
    <link href="https://www.yam.gift/2024/04/08/NLP/2024-04-08-LLM-Colosseum/"/>
    <id>https://www.yam.gift/2024/04/08/NLP/2024-04-08-LLM-Colosseum/</id>
    <published>2024-04-08T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;国外的一个项目，看了一下比较简单，于是也拿过来玩儿一下。由于原项目没支持中文，就简单支持了一下，顺便简单地重构了一下代码。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码地址（Fork）：&lt;a href=&quot;https://github.com/hscspring/llm-colosseum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/hscspring/llm-colosseum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;项目原地址：&lt;a href=&quot;https://github.com/OpenGenerativeAI/llm-colosseum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/OpenGenerativeAI/llm-colosseum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://qnimg.lovevivian.cn/blog-llm-colosseum-1.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="LLM-Colosseum" scheme="https://www.yam.gift/tags/LLM-Colosseum/"/>
    
  </entry>
  
  <entry>
    <title>LLM中的演绎推理、归纳推理和溯因推理</title>
    <link href="https://www.yam.gift/2024/04/06/Paper/2024-04-06-Deductive-Inductive-Abductive/"/>
    <id>https://www.yam.gift/2024/04/06/Paper/2024-04-06-Deductive-Inductive-Abductive/</id>
    <published>2024-04-06T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;一篇简单探索少样本上下文学习和指令推理的文章：&lt;a href=&quot;https://arxiv.org/abs/2404.03028&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2404.03028] An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Instruction Following" scheme="https://www.yam.gift/tags/Instruction-Following/"/>
    
      <category term="Few-shot Prompting" scheme="https://www.yam.gift/tags/Few-shot-Prompting/"/>
    
      <category term="Instruction Inference" scheme="https://www.yam.gift/tags/Instruction-Inference/"/>
    
  </entry>
  
  <entry>
    <title>LLM极简科普</title>
    <link href="https://www.yam.gift/2024/03/16/AI/2024-03-16-LLM-Basic/"/>
    <id>https://www.yam.gift/2024/03/16/AI/2024-03-16-LLM-Basic/</id>
    <published>2024-03-16T15:30:00.000Z</published>
    <updated>2024-06-12T02:06:43.527Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;编者按：本文内容来自Datawhale《AI第一课》，目标是向普通大众传播AI相关知识。本文是第一稿，太过于偏技术，因此需要重新修改打磨。不过从有编程背景读者的角度看我觉得内容尚可，特记录在此。同时也是便于后面对比最终内容和最初内容的差别，提升自己科普内容创作方面的技巧。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;本节主要介绍LLM（Large Language Model，大语言模型）的基础科普。大纲如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算机如何识别文字：Token化+词嵌入（Tokenize+Embedding）&lt;/li&gt;
&lt;li&gt;大模型如何学习（训练）：下个词预测（Next Token Prediction，NTP）&lt;/li&gt;
&lt;li&gt;大模型如何理解文本：多层多头注意力（Multi-Layer+Multi-Head Self-Attention，MHA）&lt;/li&gt;
&lt;li&gt;大模型如何处理任务：上下文学习或情境学习（In-Context Learning）&lt;/li&gt;
&lt;li&gt;大模型如何生成回复：推理（Inference）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文涉及到上面提到的重要概念时，会以中文表述，括号内的是对应的英文表达。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="AIGC" scheme="https://www.yam.gift/tags/AIGC/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Programming Language Environment Cheat Sheet</title>
    <link href="https://www.yam.gift/2024/03/06/Unix/2024-03-06-LanguageEnvCheatSheet/"/>
    <id>https://www.yam.gift/2024/03/06/Unix/2024-03-06-LanguageEnvCheatSheet/</id>
    <published>2024-03-06T15:00:00.000Z</published>
    <updated>2024-12-05T09:20:05.021Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;编程语言环境相关备忘（我只想复制粘贴）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Rust" scheme="https://www.yam.gift/tags/Rust/"/>
    
      <category term="NodeJS" scheme="https://www.yam.gift/tags/NodeJS/"/>
    
  </entry>
  
  <entry>
    <title>LLM Tiny Pretrain：H2O-Danube and Stable LM</title>
    <link href="https://www.yam.gift/2024/02/03/NLP/LLM-Training/2024-02-03-LLM-Tiny-Pretrain/"/>
    <id>https://www.yam.gift/2024/02/03/NLP/LLM-Training/2024-02-03-LLM-Tiny-Pretrain/</id>
    <published>2024-02-03T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.539Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://huggingface.co/collections/stabilityai/stable-lm-650852cfd55dd4e15cdcb30a&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;StableLM&lt;/a&gt; 一直致力于小模型（从7B、3B 到 1.6B），不过 License 商用有些限制，&lt;a href=&quot;http://arxiv.org/abs/2401.16818&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;H2O-Danube&lt;/a&gt; 是 Apache2.0 的小模型（1.8B），整体指标略逊于 StableLM。本文通过这两篇 Paper，记录小模型的预训练。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="H2O-Danube" scheme="https://www.yam.gift/tags/H2O-Danube/"/>
    
      <category term="Stable LM" scheme="https://www.yam.gift/tags/Stable-LM/"/>
    
  </entry>
  
  <entry>
    <title>LLM DataManagement：Weaver</title>
    <link href="https://www.yam.gift/2024/02/01/NLP/LLM-DM/2024-02-01-LLM-DataManagement-Weaver/"/>
    <id>https://www.yam.gift/2024/02/01/NLP/LLM-DM/2024-02-01-LLM-DataManagement-Weaver/</id>
    <published>2024-02-01T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文记录 &lt;a href=&quot;http://arxiv.org/abs/2401.17268&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Weaver&lt;/a&gt; 的数据处理。&lt;/p&gt;
&lt;p&gt;Weaver是一个垂直领域（文字创作）的LLM，做的是继续训练，训练上循规蹈矩，没有什么好说的。稍微有一点点特色的是数据这块，对垂直领域可能有一定借鉴意义。&lt;/p&gt;
&lt;p&gt;另外有提出一个Constitutional DPO的东西，其实就是利用专家写的规则（原则）合成违反这些规则的负样本。相较而言，遵循这些规则的就是正样本。这其实和数据有点关系，垂直领域往往有不少正样本（比如文字创作领域大家的小说、散文等），但负样本却不好找，所以就违反”好“的规则生成负样本。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="DataManagement" scheme="https://www.yam.gift/tags/DataManagement/"/>
    
      <category term="Continual Pretraining" scheme="https://www.yam.gift/tags/Continual-Pretraining/"/>
    
  </entry>
  
  <entry>
    <title>LLM DataManagement：Ziya2</title>
    <link href="https://www.yam.gift/2024/01/29/NLP/LLM-DM/2024-01-29-LLM-DataManagement-Ziya2/"/>
    <id>https://www.yam.gift/2024/01/29/NLP/LLM-DM/2024-01-29-LLM-DataManagement-Ziya2/</id>
    <published>2024-01-29T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;高质量的数据对LLM的重要性无需赘述，本文记录&lt;a href=&quot;http://arxiv.org/abs/2311.03301&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ziya2&lt;/a&gt;的数据管理。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="DataManagement" scheme="https://www.yam.gift/tags/DataManagement/"/>
    
      <category term="Ziya" scheme="https://www.yam.gift/tags/Ziya/"/>
    
      <category term="Continual Pretraining" scheme="https://www.yam.gift/tags/Continual-Pretraining/"/>
    
  </entry>
  
  <entry>
    <title>LLM Continual Pre-training：Ziya2</title>
    <link href="https://www.yam.gift/2024/01/23/NLP/LLM-Training/2024-01-23-LLM-Continual-Training-Ziya2/"/>
    <id>https://www.yam.gift/2024/01/23/NLP/LLM-Training/2024-01-23-LLM-Continual-Training-Ziya2/</id>
    <published>2024-01-23T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.539Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;LLM的训练策略非常讲究，本文主要记录&lt;a href=&quot;http://arxiv.org/abs/2311.03301&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ziya2&lt;/a&gt;的继续训练策略。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Ziya" scheme="https://www.yam.gift/tags/Ziya/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
  </entry>
  
  <entry>
    <title>【Rust与AI】LLM模型基本架构</title>
    <link href="https://www.yam.gift/2023/12/24/Rust/RustAI/2023-12-24-Rust-and-AI-LLM/"/>
    <id>https://www.yam.gift/2023/12/24/Rust/RustAI/2023-12-24-Rust-and-AI-LLM/</id>
    <published>2023-12-24T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.552Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本篇是《Rust与AI》系列的第二篇，上一篇我们主要介绍了本系列的概览和方向，定下了一个基调。本篇我们将介绍LLM的基本架构，我们会以迄今为止使用最广泛的开源模型LLaMA为例展开介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Rust" scheme="https://www.yam.gift/tags/Rust/"/>
    
      <category term="Decoding" scheme="https://www.yam.gift/tags/Decoding/"/>
    
      <category term="Llama" scheme="https://www.yam.gift/tags/Llama/"/>
    
  </entry>
  
  <entry>
    <title>【Rust与AI】概览和方向</title>
    <link href="https://www.yam.gift/2023/12/03/Rust/RustAI/2023-12-03-Rust-and-AI-Introduction/"/>
    <id>https://www.yam.gift/2023/12/03/Rust/RustAI/2023-12-03-Rust-and-AI-Introduction/</id>
    <published>2023-12-03T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.552Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本系列主要介绍Rust与AI的那些天作之合（开源项目），我们会以开源项目代码阅读的方式推进，以Rust为主，同时科普AI相关知识，目的是让更多非算法、非Rust的程序员进一步学习Rust和AI相关知识。当然，很显然地，我们也希望Rust程序员和AI算法工程师能从中有所收获。前者可以关注AI算法的设计和优化，后者可以关注Rust如何助力AI算法。&lt;/p&gt;
&lt;p&gt;本篇是系列第一篇，主要介绍Rust和AI各自的特点与发展近况，以及它俩的遇见会碰撞出怎样的火花。我们热爱AI，我们喜欢Rust语言，仅此而已。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Rust" scheme="https://www.yam.gift/tags/Rust/"/>
    
  </entry>
  
  <entry>
    <title>OpenAIGC大赛小结</title>
    <link href="https://www.yam.gift/2023/11/04/AI/2023-11-04-OpenAIGC/"/>
    <id>https://www.yam.gift/2023/11/04/AI/2023-11-04-OpenAIGC/</id>
    <published>2023-11-04T15:30:00.000Z</published>
    <updated>2024-06-12T02:06:43.527Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;很荣幸周末参加了第一届OpenAIGC开发者大赛，并担任大赛评委之一。期间收获良多，感慨万千，特记录如下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="AIGC" scheme="https://www.yam.gift/tags/AIGC/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>关于大语言模型的思考</title>
    <link href="https://www.yam.gift/2023/10/15/NLP/2023-10-15-Think-About-LLM/"/>
    <id>https://www.yam.gift/2023/10/15/NLP/2023-10-15-Think-About-LLM/</id>
    <published>2023-10-15T03:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;从ChatGPT去年11月底发布到现在差不多一年时间了，短短的一年，整个NLP行业发生了翻天覆地的变化。应用方面，整个AI行业甚至其他行业都受到很大冲击，感觉所有人都在+大模型，都在试图重构产品和服务；研究方面，LLM现在几乎成为所有从业人员研究的热点，各种各样的研究成果层出不穷，让人眼花缭乱，直呼看不过来。&lt;/p&gt;
&lt;p&gt;本人作为一名NLP工程师，自然深度参与。从一开始的Prompt技巧，到InstructGPT三阶段训练研究，再到千奇百怪的高效微调、知识编辑，再到各种量化推理、剪枝、小模型实践，再到目前重新思考预训练。这是一个不断深入的过程，也是一个不断学习的过程。从一开始的“我草牛逼”，到“看起来好像不复杂”，再到“咋回事，咋做的，咋这么多坑，咋办”。&lt;/p&gt;
&lt;p&gt;本文主要记录一点当下最新的思考，包括算法和行业两个方面。我会尽量让自己的观点鲜明，不模棱两可。另外，我们也不是搞预测，只是纯粹的分析和感悟，甚至有一些个人偏好。总的来说，都是个人观点，限于能力，不一定准确（很有可能有错误），希望能借此和同好一起讨论。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 开发指南：Hugging LLM Hugging Future</title>
    <link href="https://www.yam.gift/2023/04/22/NLP/2023-04-22-ChatGPT-Development/"/>
    <id>https://www.yam.gift/2023/04/22/NLP/2023-04-22-ChatGPT-Development/</id>
    <published>2023-04-22T04:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文属于DataWhale &lt;a href=&quot;https://github.com/datawhalechina/hugging-llm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hugging-LLM&lt;/a&gt;开源教程介绍内容，详细教程请跳转链接。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;随着ChatGPT的爆火，我们相信未来会有越来越多的大模型及类似OpenAI提供的服务出现，AI 正在逐渐平民化，将来每个人都可以利用大模型轻松地做出自己的AI产品。&lt;/p&gt;
&lt;p&gt;HuggingLLM是一个面向非算法、有一定编程基础、对AI和ChatGPT（或类似模型）感兴趣的，基于ChatGPT API开发相关应用的开源项目。当然部分内容不需要任何编程经验也可以学习，算法工程师也可能从中受益。项目主要包括ChatGPT基础科普、ChatGPT实现各种NLP常见任务（相似匹配、句词分类、编辑生成、推理等大类）、ChatGPT局限和商业应用等内容。&lt;/p&gt;
&lt;p&gt;项目名为 HuggingLLM，因为我们相信正在经历一个伟大的时代，我们相信这是一个值得每个人全身心拥抱的时代，我们更加相信这个世界必将会因此而变得更加美好。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="LM" scheme="https://www.yam.gift/tags/LM/"/>
    
      <category term="HuggingLLM" scheme="https://www.yam.gift/tags/HuggingLLM/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 基础科普：知其一点所以然</title>
    <link href="https://www.yam.gift/2023/04/15/NLP/2023-04-15-ChatGPT-Introduction/"/>
    <id>https://www.yam.gift/2023/04/15/NLP/2023-04-15-ChatGPT-Introduction/</id>
    <published>2023-04-15T10:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.537Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文属于&lt;a href=&quot;https://datawhale.club/#/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DataWhale&lt;/a&gt;开源组织&lt;a href=&quot;https://github.com/datawhalechina/hugging-llm.git&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HuggingLLM&lt;/a&gt;开源项目内容，更多内容请移步开源项目。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;2022年底，ChatGPT突然间在AI圈开始流行，准确来说是搞自然语言处理（Natural Language Processing，NLP）圈子里先火起来了。遥想当时，本以为就会在圈内火一阵，结果现在……没想到居然成了AI的救命稻草，当然对AI工程师尤其是NLP工程师是什么就不好说了。海内外沸腾之后就是第一时间的跟进，结果自然是努力对齐而无功，连牛逼的Google和FaceBook都翻车了。不过总归是折腾出来一些东西，大伙儿也都有了目标，圈子又有了新活力。希望OpenAI继续发力，我等再多苟些日子。&lt;/p&gt;
&lt;p&gt;无论是ChatGPT还是后来的模仿者，它们其实都是语言模型，准确来说——大语言模型。使用时，无论是调用API还是开源项目，总有一些参数可能需要调整。对大部分内行人士来说应该都不成问题，但对外行就有点玄乎了。基于此，本文将简要介绍ChatGPT相关技术基本原理，行文将站在外行人角度，尝试将内容尽量平民化。虽然不能深入细节，但知晓原理足以很好使用了。&lt;/p&gt;
&lt;p&gt;本文共分为四个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LM：这是ChatGPT的基石的基石，是一个最基本的概念，绕不开，逃不过，没办法。&lt;/li&gt;
&lt;li&gt;Transformer：这是ChatGPT的基石，准确来说它的一部分是基石。&lt;/li&gt;
&lt;li&gt;GPT：本体，从GPT-1，一直到现在的GPT-4，按openai自己的说法，那模型都是那个模型，只是它长大了，变胖了，不过更好看了。关于这点，大家基本都没想到。现在好了，攀不上了。&lt;/li&gt;
&lt;li&gt;RLHF：ChatGPT神兵利器，有此利刃，ChatGPT才是那个ChatGPT，不然就只能是GPT-3。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="InstructGPT" scheme="https://www.yam.gift/tags/InstructGPT/"/>
    
      <category term="LM" scheme="https://www.yam.gift/tags/LM/"/>
    
      <category term="GPT-3" scheme="https://www.yam.gift/tags/GPT-3/"/>
    
      <category term="GPT-2" scheme="https://www.yam.gift/tags/GPT-2/"/>
    
      <category term="GPT-1" scheme="https://www.yam.gift/tags/GPT-1/"/>
    
      <category term="RLHF" scheme="https://www.yam.gift/tags/RLHF/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
  </entry>
  
</feed>
