<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2025-01-01T00:10:50.566Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>hscspring</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LLM应用论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Application/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Application/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:10:50.566Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;LLM应用相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="RAG" scheme="https://www.yam.gift/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>LLM指令跟随论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Instruction-Following/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Instruction-Following/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:12:08.663Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;LLM指令跟随相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Instruction Following" scheme="https://www.yam.gift/tags/Instruction-Following/"/>
    
  </entry>
  
  <entry>
    <title>LLM解码论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Decoding/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Decoding/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:21:36.676Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;LLM解码相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Decoding" scheme="https://www.yam.gift/tags/Decoding/"/>
    
  </entry>
  
  <entry>
    <title>OMNI论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-OMNI/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-OMNI/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:15:48.535Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;OMNI相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="OMNI" scheme="https://www.yam.gift/tags/OMNI/"/>
    
  </entry>
  
  <entry>
    <title>SLM论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-SLM/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-SLM/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:21:25.869Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;SLM相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="SLM" scheme="https://www.yam.gift/tags/SLM/"/>
    
      <category term="MM Fusion" scheme="https://www.yam.gift/tags/MM-Fusion/"/>
    
  </entry>
  
  <entry>
    <title>LLM提示词、上下文论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Prompt/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Prompt/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:12:15.118Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;LLM提示词、上下文相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Prompt" scheme="https://www.yam.gift/tags/Prompt/"/>
    
      <category term="Context" scheme="https://www.yam.gift/tags/Context/"/>
    
  </entry>
  
  <entry>
    <title>音频Codec论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-Codec/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-Codec/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:21:57.758Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Codec相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
  </entry>
  
  <entry>
    <title>XTTS</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-XTTS/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-XTTS/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:27:01.808Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2406.04904&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model&lt;/a&gt;&lt;br&gt;
代码：&lt;a href=&quot;https://github.com/coqui-ai/TTS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;coqui-ai/TTS: 🐸💬 - a deep learning toolkit for Text-to-Speech, battle-tested in research and production&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;基于Tortoise的改进，自回归。本文主要关心架构。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="XTTS" scheme="https://www.yam.gift/tags/XTTS/"/>
    
  </entry>
  
  <entry>
    <title>VITS</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-VITS/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-VITS/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:26:03.099Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2106.06103&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/a&gt;&lt;br&gt;
代码：&lt;a href=&quot;https://github.com/jaywalnut310/vits&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;jaywalnut310/vits: VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一个并行的端到端TTS模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="VITS" scheme="https://www.yam.gift/tags/VITS/"/>
    
  </entry>
  
  <entry>
    <title>DAC</title>
    <link href="https://www.yam.gift/2024/12/30/Paper/TTS/2024-12-30-DAC/"/>
    <id>https://www.yam.gift/2024/12/30/Paper/TTS/2024-12-30-DAC/</id>
    <published>2024-12-30T15:00:00.000Z</published>
    <updated>2025-01-01T00:25:58.731Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2306.06546&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;High-Fidelity Audio Compression with Improved RVQGAN&lt;/a&gt;&lt;br&gt;
代码：&lt;a href=&quot;https://github.com/descriptinc/descript-audio-codec&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;descriptinc/descript-audio-codec: State-of-the-art audio codec with 90x compression factor. Supports 44.1kHz, 24kHz, and 16kHz mono/stereo audio.&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="DAC" scheme="https://www.yam.gift/tags/DAC/"/>
    
  </entry>
  
  <entry>
    <title>TS3-Codec</title>
    <link href="https://www.yam.gift/2024/12/27/Paper/TTS/2024-12-27-TS3-Codec/"/>
    <id>https://www.yam.gift/2024/12/27/Paper/TTS/2024-12-27-TS3-Codec/</id>
    <published>2024-12-27T15:00:00.000Z</published>
    <updated>2025-01-01T00:24:54.810Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2411.18803&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TS3-Codec: Transformer-Based Simple Streaming Single Codec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;没开源代码。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="TS3-Codec" scheme="https://www.yam.gift/tags/TS3-Codec/"/>
    
  </entry>
  
  <entry>
    <title>BigCodec</title>
    <link href="https://www.yam.gift/2024/12/26/Paper/TTS/2024-12-26-BigCodec/"/>
    <id>https://www.yam.gift/2024/12/26/Paper/TTS/2024-12-26-BigCodec/</id>
    <published>2024-12-26T15:00:00.000Z</published>
    <updated>2025-01-01T00:25:03.980Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2409.05377&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码：&lt;a href=&quot;https://github.com/Aria-K-Alethia/BigCodec&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Aria-K-Alethia/BigCodec&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="BigCodec" scheme="https://www.yam.gift/tags/BigCodec/"/>
    
  </entry>
  
  <entry>
    <title>关于AI前沿的思考</title>
    <link href="https://www.yam.gift/2024/12/20/NLP/2024-12-20-Think-About-AI-and-Related/"/>
    <id>https://www.yam.gift/2024/12/20/NLP/2024-12-20-Think-About-AI-and-Related/</id>
    <published>2024-12-19T17:30:00.000Z</published>
    <updated>2024-12-19T18:01:53.100Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;上一次写关于类似（大语言模型）的&lt;a href=&quot;https://yam.gift/2023/10/15/NLP/2023-10-15-Think-About-LLM/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;思考&lt;/a&gt;是在去年10月份了，主要是关于LLM机理、预训练、微调等算法层面的思考。不过后面也提到了未来的方向，以及行业的思考。到今天看那些内容依然实用，而且有种“预判”逐渐成真的感觉。其实我个人很不喜欢预言、预测或诸如此类的事物，但当我们对一个行业了解的足够多、足够深时，很多时候对一些方向性问题的判断就会比较准确。&lt;/p&gt;
&lt;p&gt;言归正传，今天正好看到了OpenAI前首席研究官Bob McGrew采访（&lt;a href=&quot;https://mp.weixin.qq.com/s/t42LuAP_O8WnjkXOwHLhXQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;中文版&lt;/a&gt;），又有了新的想法，正好也谈一谈最近的一些思考。主要围绕着采访中的主题，谈谈自己的看法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Multi-Modal" scheme="https://www.yam.gift/tags/Multi-Modal/"/>
    
      <category term="Embodied AI" scheme="https://www.yam.gift/tags/Embodied-AI/"/>
    
  </entry>
  
  <entry>
    <title>《真希望父母读过这本书》读书笔记</title>
    <link href="https://www.yam.gift/2024/09/30/BabyGrow/2024-09-30-Hope-Parents-Read-the-Book/"/>
    <id>https://www.yam.gift/2024/09/30/BabyGrow/2024-09-30-Hope-Parents-Read-the-Book/</id>
    <published>2024-09-30T14:00:00.000Z</published>
    <updated>2024-10-08T05:01:50.599Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;周六下午和爱人一起去了天目里，散步、聊天、看风景、喝茉酸奶、吃烧鸟、去鸟屋书店读书、去饸饹面馆吃面。这是我们第二次去这里了，主要是那家面馆的面很符合我们口味，但也不能打车过去就吃个面，所以每次都要看一两个小时书。这本书就是这天我读的两本书之一，还剩最后一章没读完，我觉得问题不大，得赶紧把读过的记下来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Life" scheme="https://www.yam.gift/tags/Life/"/>
    
      <category term="BabyGrow" scheme="https://www.yam.gift/tags/BabyGrow/"/>
    
  </entry>
  
  <entry>
    <title>基础和取舍</title>
    <link href="https://www.yam.gift/2024/09/30/Diary/2024-09-30-Work-Design/"/>
    <id>https://www.yam.gift/2024/09/30/Diary/2024-09-30-Work-Design/</id>
    <published>2024-09-30T00:00:00.000Z</published>
    <updated>2024-09-30T11:28:04.257Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;没想到居然一年多没写这样思考性的文字了，ChatGPT后遗症有点大。去年底换工作再加上孩子出生，生活一下子变得异常充实了起来。家庭和育儿方面成长很多，从一开始的没耐心，到逐步理解包容、感同身受（无论对爱人还是孩子），一年不到时间改变了非常多。工作方面也取得了一些成果，强度和深度比此前所有工作都高了一个级别，虽然很忙，但非常开心。比较不满意的是过于忙碌导致没时间夯实基础，总感觉自己比较浮。正好国庆假期，重新整理一下思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Life" scheme="https://www.yam.gift/tags/Life/"/>
    
      <category term="Growth" scheme="https://www.yam.gift/tags/Growth/"/>
    
      <category term="Dream" scheme="https://www.yam.gift/tags/Dream/"/>
    
  </entry>
  
  <entry>
    <title>MIO</title>
    <link href="https://www.yam.gift/2024/09/28/Paper/2024-09-28-MIO/"/>
    <id>https://www.yam.gift/2024/09/28/Paper/2024-09-28-MIO/</id>
    <published>2024-09-28T15:00:00.000Z</published>
    <updated>2024-10-09T15:39:23.866Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/2409.17692&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/2409.17692&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心：多模态输入输出，这里的多模态包括了文本、图像、语音和视频，相比AnyGPT多了视频。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://qnimg.lovevivian.cn/paper-mio-1.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="MIO" scheme="https://www.yam.gift/tags/MIO/"/>
    
      <category term="MultiModal" scheme="https://www.yam.gift/tags/MultiModal/"/>
    
  </entry>
  
  <entry>
    <title>Tiny LLM Continual Pre-training：RHO-1</title>
    <link href="https://www.yam.gift/2024/04/13/NLP/LLM-Training/2024-04-13-LLM-Tiny-Continual-Training-RHO-1/"/>
    <id>https://www.yam.gift/2024/04/13/NLP/LLM-Training/2024-04-13-LLM-Tiny-Continual-Training-RHO-1/</id>
    <published>2024-04-13T15:00:00.000Z</published>
    <updated>2024-06-12T08:30:34.563Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;并不是所有的Next Token都有用，&lt;a href=&quot;http://arxiv.org/abs/2404.07965&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RHO-1&lt;/a&gt;选择那些最有用的Next Token，提升了小模型的继续训练效率。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="RHO-1" scheme="https://www.yam.gift/tags/RHO-1/"/>
    
      <category term="RHO" scheme="https://www.yam.gift/tags/RHO/"/>
    
  </entry>
  
  <entry>
    <title>LLM打街霸</title>
    <link href="https://www.yam.gift/2024/04/08/NLP/2024-04-08-LLM-Colosseum/"/>
    <id>https://www.yam.gift/2024/04/08/NLP/2024-04-08-LLM-Colosseum/</id>
    <published>2024-04-08T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;国外的一个项目，看了一下比较简单，于是也拿过来玩儿一下。由于原项目没支持中文，就简单支持了一下，顺便简单地重构了一下代码。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码地址（Fork）：&lt;a href=&quot;https://github.com/hscspring/llm-colosseum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/hscspring/llm-colosseum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;项目原地址：&lt;a href=&quot;https://github.com/OpenGenerativeAI/llm-colosseum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/OpenGenerativeAI/llm-colosseum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://qnimg.lovevivian.cn/blog-llm-colosseum-1.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="LLM-Colosseum" scheme="https://www.yam.gift/tags/LLM-Colosseum/"/>
    
  </entry>
  
  <entry>
    <title>LLM中的演绎推理、归纳推理和溯因推理</title>
    <link href="https://www.yam.gift/2024/04/06/Paper/2024-04-06-Deductive-Inductive-Abductive/"/>
    <id>https://www.yam.gift/2024/04/06/Paper/2024-04-06-Deductive-Inductive-Abductive/</id>
    <published>2024-04-06T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;一篇简单探索少样本上下文学习和指令推理的文章：&lt;a href=&quot;https://arxiv.org/abs/2404.03028&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2404.03028] An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Instruction Following" scheme="https://www.yam.gift/tags/Instruction-Following/"/>
    
      <category term="Few-shot Prompting" scheme="https://www.yam.gift/tags/Few-shot-Prompting/"/>
    
      <category term="Instruction Inference" scheme="https://www.yam.gift/tags/Instruction-Inference/"/>
    
  </entry>
  
  <entry>
    <title>LLM极简科普</title>
    <link href="https://www.yam.gift/2024/03/16/AI/2024-03-16-LLM-Basic/"/>
    <id>https://www.yam.gift/2024/03/16/AI/2024-03-16-LLM-Basic/</id>
    <published>2024-03-16T15:30:00.000Z</published>
    <updated>2024-06-12T02:06:43.527Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;编者按：本文内容来自Datawhale《AI第一课》，目标是向普通大众传播AI相关知识。本文是第一稿，太过于偏技术，因此需要重新修改打磨。不过从有编程背景读者的角度看我觉得内容尚可，特记录在此。同时也是便于后面对比最终内容和最初内容的差别，提升自己科普内容创作方面的技巧。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;本节主要介绍LLM（Large Language Model，大语言模型）的基础科普。大纲如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算机如何识别文字：Token化+词嵌入（Tokenize+Embedding）&lt;/li&gt;
&lt;li&gt;大模型如何学习（训练）：下个词预测（Next Token Prediction，NTP）&lt;/li&gt;
&lt;li&gt;大模型如何理解文本：多层多头注意力（Multi-Layer+Multi-Head Self-Attention，MHA）&lt;/li&gt;
&lt;li&gt;大模型如何处理任务：上下文学习或情境学习（In-Context Learning）&lt;/li&gt;
&lt;li&gt;大模型如何生成回复：推理（Inference）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文涉及到上面提到的重要概念时，会以中文表述，括号内的是对应的英文表达。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="AIGC" scheme="https://www.yam.gift/tags/AIGC/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
  </entry>
  
</feed>
