<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2025-03-15T07:34:10.594Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>hscspring</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DeepSeek R1后应用、职业与行业影响——2025年梳理</title>
    <link href="https://www.yam.gift/2025/03/15/AI/2025-03-15-LLM-App-Develop/"/>
    <id>https://www.yam.gift/2025/03/15/AI/2025-03-15-LLM-App-Develop/</id>
    <published>2025-03-15T07:30:00.000Z</published>
    <updated>2025-03-15T07:34:10.594Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;突然就想写点应用、开发相关的东西，一方面是不断有企业和朋友问我他们可以用DeepSeek做什么，怎么用；另一方面是这个方向的职业、行业也在不知不觉中慢慢改变。干脆顺便一起梳理一下，记录在案。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="DeepSeek" scheme="https://www.yam.gift/tags/DeepSeek/"/>
    
      <category term="Agent" scheme="https://www.yam.gift/tags/Agent/"/>
    
  </entry>
  
  <entry>
    <title>DeepSeek R1后LLM新范式</title>
    <link href="https://www.yam.gift/2025/03/15/NLP/LLM-Training/2025-03-15-R1-New-Paradigm/"/>
    <id>https://www.yam.gift/2025/03/15/NLP/LLM-Training/2025-03-15-R1-New-Paradigm/</id>
    <published>2025-03-15T03:00:00.000Z</published>
    <updated>2025-03-15T03:18:57.856Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文通过10篇R1相关的研究，介绍R1后LLM的新范式。其核心就是如何进一步增强LLM的能力。&lt;/p&gt;
&lt;h2 id=&quot;基本框架&quot;&gt;基本框架&lt;/h2&gt;
&lt;p&gt;首先是整体的框架，如下所示。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Base+SFT
&lt;ul&gt;
&lt;li&gt;R1冷启动&lt;/li&gt;
&lt;li&gt;LIMO (817 Data Selection)&lt;/li&gt;
&lt;li&gt;s1 (1000)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Base+RL
&lt;ul&gt;
&lt;li&gt;GRPO: R1-Zero&lt;/li&gt;
&lt;li&gt;GRPO: oat-zero (Base can Aha、RL  enhance）&lt;/li&gt;
&lt;li&gt;PPO: LIMR (Data Selection)&lt;/li&gt;
&lt;li&gt;PPO: orz (Scaling quality, diversity)&lt;/li&gt;
&lt;li&gt;DPO: Online-DPO-R1 (Different RL Algo)&lt;/li&gt;
&lt;li&gt;DPO: LIMD (Data Selection)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;SFT+RL
&lt;ul&gt;
&lt;li&gt;R1蒸馏&lt;/li&gt;
&lt;li&gt;DeepScaleR (Length Scaling)&lt;/li&gt;
&lt;li&gt;Self-rewarding correction (LLM can reward itself, explicit Aha)、L1（LCPO）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我将其分成3个部分，前两个部分是Base模型的基础上使用SFT或RL提升效果，最后是SFT结合RL进一步提升效果。每个部分的第一个都是R1论文中的内容，上面没有R1本身，是因为R1本身是一个比较综合的过程。&lt;/p&gt;
&lt;p&gt;值得说明的是，关于R1相关的研究肯定不止这些，列出这些一方面是因为我自己精力有限，只仔细阅读了这些；另一方面是逐步整理的过程中感觉到框架基本趋于完善。因此，本文也算是一个阶段性整理的输出。&lt;/p&gt;
&lt;p&gt;本文内容相对比较通俗，如果对相关内容感兴趣，可以移步到对应的解读文章。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Post-training" scheme="https://www.yam.gift/tags/Post-training/"/>
    
      <category term="R1" scheme="https://www.yam.gift/tags/R1/"/>
    
      <category term="R1-Zero" scheme="https://www.yam.gift/tags/R1-Zero/"/>
    
      <category term="oat-zero" scheme="https://www.yam.gift/tags/oat-zero/"/>
    
      <category term="DeepScaleR" scheme="https://www.yam.gift/tags/DeepScaleR/"/>
    
      <category term="LIMO" scheme="https://www.yam.gift/tags/LIMO/"/>
    
      <category term="s1" scheme="https://www.yam.gift/tags/s1/"/>
    
      <category term="Inference Scaling" scheme="https://www.yam.gift/tags/Inference-Scaling/"/>
    
      <category term="LIMR" scheme="https://www.yam.gift/tags/LIMR/"/>
    
      <category term="LIMD" scheme="https://www.yam.gift/tags/LIMD/"/>
    
      <category term="Online-DPO-R1" scheme="https://www.yam.gift/tags/Online-DPO-R1/"/>
    
      <category term="orz" scheme="https://www.yam.gift/tags/orz/"/>
    
      <category term="L1" scheme="https://www.yam.gift/tags/L1/"/>
    
      <category term="LCPO" scheme="https://www.yam.gift/tags/LCPO/"/>
    
  </entry>
  
  <entry>
    <title>R1相关：DPO数据选择与DPO等RL算法</title>
    <link href="https://www.yam.gift/2025/03/02/NLP/LLM-Training/2025-03-02-LLM-PostTrain-DPO-Data/"/>
    <id>https://www.yam.gift/2025/03/02/NLP/LLM-Training/2025-03-02-LLM-PostTrain-DPO-Data/</id>
    <published>2025-03-02T03:00:00.000Z</published>
    <updated>2025-03-11T00:06:05.272Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍两篇与DPO以及其他RL算法相关的。R1-Zero在表现出潜力后，GRPO自不必多说，得到大家关注。PPO、Reinforce++等也被用来尝试实验，结果也很亮眼。既然如此，其他RL算法可以吗，尤其是前LLM时代流行的DPO。于是就有了本文的两篇研究。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="Post-training" scheme="https://www.yam.gift/tags/Post-training/"/>
    
      <category term="RL" scheme="https://www.yam.gift/tags/RL/"/>
    
      <category term="DPO" scheme="https://www.yam.gift/tags/DPO/"/>
    
      <category term="LIMD" scheme="https://www.yam.gift/tags/LIMD/"/>
    
      <category term="Online-DPO-R1" scheme="https://www.yam.gift/tags/Online-DPO-R1/"/>
    
  </entry>
  
  <entry>
    <title>预训练：NTP和Scaling Law</title>
    <link href="https://www.yam.gift/2025/02/28/NLP/LLM-Training/2025-02-28-LLM-Pretrain-NTP-and-ScaleLaw/"/>
    <id>https://www.yam.gift/2025/02/28/NLP/LLM-Training/2025-02-28-LLM-Pretrain-NTP-and-ScaleLaw/</id>
    <published>2025-02-28T00:00:00.000Z</published>
    <updated>2025-03-03T00:30:16.221Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;做预训练绕不开这两个主题，尤其是Scaling Law，NTP探讨智能如何产生，对宏观思想认知有极大帮助。&lt;/p&gt;
&lt;p&gt;其实，关于NTP为什么能产生智能这个问题笔者在2023年和几个从业者探讨过，见这篇&lt;a href=&quot;https://yam.gift/2023/10/15/NLP/2023-10-15-Think-About-LLM/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;博客&lt;/a&gt;，当时大家的答复并没让笔者完全信服。现在，又要开始探讨了。至于原因，大家看完文章就知道了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="NTP" scheme="https://www.yam.gift/tags/NTP/"/>
    
      <category term="Scaling Law" scheme="https://www.yam.gift/tags/Scaling-Law/"/>
    
  </entry>
  
  <entry>
    <title>LLM、强化、蒸馏讨论</title>
    <link href="https://www.yam.gift/2025/02/27/AI/2025-02-27-AI-Discussion/"/>
    <id>https://www.yam.gift/2025/02/27/AI/2025-02-27-AI-Discussion/</id>
    <published>2025-02-27T15:30:00.000Z</published>
    <updated>2025-03-01T15:28:39.519Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;2025年2月26日下午，Datawhale Paper群突然开启了一番关于AI相关的讨论，涉及成员主要包括：X、Y、D、S、M、A和C。我觉得内容相当有意思，因此记录在案备查。&lt;/p&gt;
&lt;p&gt;其中对我个人印象比较深的几个观点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X提出的新的大模型训练范式：预训练，long-cot, sft（long2short）。可以理解为先用大规模语料预训练学习知识，然后用少量SFT或RL（可以一起用）提升long-cot，然后再做SFT使其根据情况自动选择long或short。&lt;/li&gt;
&lt;li&gt;关于如何让模型自动选择思考长度（或不思考）的讨论，X认为主要靠强化，只是奖励这块需要涉及，就是是否需要思考，问题的难易，需要有个奖励来控制、设计。集成和自适应prm都是挺好的点，其实现在的重心就是什么样的奖励和怎么自动奖励。&lt;/li&gt;
&lt;li&gt;关于蒸馏分布的讨论。蒸馏之前做的不多，没想过这么细，不过如何桥接教师和学生的讨论确实有启发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对讨论结果分别使用DeepSeek和DeepSeek-R1进行了整理，前者相对比较忠于讨论内容，后者则更加抽象有高度一些，各有优势。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Distillation" scheme="https://www.yam.gift/tags/Distillation/"/>
    
      <category term="RL" scheme="https://www.yam.gift/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>R1相关：RL数据选择与Scaling</title>
    <link href="https://www.yam.gift/2025/02/27/NLP/LLM-Training/2025-02-27-LLM-PostTrain-PPO-Data/"/>
    <id>https://www.yam.gift/2025/02/27/NLP/LLM-Training/2025-02-27-LLM-PostTrain-PPO-Data/</id>
    <published>2025-02-27T15:00:00.000Z</published>
    <updated>2025-03-02T14:31:13.746Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍两篇对RL Scaling进一步探索的论文，都是关于数据方面的，结论有一定互补性。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="Post-training" scheme="https://www.yam.gift/tags/Post-training/"/>
    
      <category term="RL" scheme="https://www.yam.gift/tags/RL/"/>
    
      <category term="Scaling" scheme="https://www.yam.gift/tags/Scaling/"/>
    
      <category term="KL" scheme="https://www.yam.gift/tags/KL/"/>
    
      <category term="LIMR" scheme="https://www.yam.gift/tags/LIMR/"/>
    
      <category term="ORZ" scheme="https://www.yam.gift/tags/ORZ/"/>
    
  </entry>
  
  <entry>
    <title>R1相关：少量高质量数据SFT激活LLM推理能力</title>
    <link href="https://www.yam.gift/2025/02/18/NLP/LLM-Training/2025-02-18-LLM-PostTrain-SFT-Data/"/>
    <id>https://www.yam.gift/2025/02/18/NLP/LLM-Training/2025-02-18-LLM-PostTrain-SFT-Data/</id>
    <published>2025-02-17T23:00:00.000Z</published>
    <updated>2025-03-02T14:31:05.409Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍两篇最新的用少量高质量数据SFT激活LLM推理能力的研究，分别是LIMO和s1。众所周知，一般说到SFT，尤其是参数比较大的模型，都是需要大量数据的；再加上推理任务本身又比较复杂，所需的数据可能更多。但这两篇文章的结论有点颠覆认知。这里的核心是：LLM本身需要具备如此能力，才有可能通过少量高质量数据SFT激活，否则可能难以见效。随着R1的出现，后训练算是彻底发生改变了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="Post-training" scheme="https://www.yam.gift/tags/Post-training/"/>
    
      <category term="LIMO" scheme="https://www.yam.gift/tags/LIMO/"/>
    
      <category term="s1" scheme="https://www.yam.gift/tags/s1/"/>
    
      <category term="Inference Scaling" scheme="https://www.yam.gift/tags/Inference-Scaling/"/>
    
  </entry>
  
  <entry>
    <title>DeepSeek R1深度技术解析及其影响</title>
    <link href="https://www.yam.gift/2025/02/17/NLP/LLM-Training/2025-02-17-DeepSeek-R1/"/>
    <id>https://www.yam.gift/2025/02/17/NLP/LLM-Training/2025-02-17-DeepSeek-R1/</id>
    <published>2025-02-17T00:00:00.000Z</published>
    <updated>2025-03-15T03:17:22.519Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是2025年2月15日《2025 iFLYTEK 开发者TALK 杭州站《DeepSeek深度技术解析》分享的文字版，PPT可以在&lt;a href=&quot;https://github.com/datawhalechina/hugging-llm/tree/main/resources&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;找到。由于时间关系，实际分享是本文的简化版。文字内容是近半个月陆陆续续记录的一些阅读笔记和思考，中途接到分享邀请（还好有点积累，不然怕是难顶doge），成稿于分享后。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;距离2022年底ChatGPT发布开启LLM时代才过去两年多一点时间，刚进入2025年，DeepSeek-R1就将LLM真正推向了深度思考时代。&lt;/p&gt;
&lt;p&gt;两年多的高速发展，前所未有的按周迭代，如今想来都一阵恍惚。2023年是LLM最快速发展的一年，被称为LLM元年，新的开发范式出现（感兴趣的读者可以关注&lt;a href=&quot;https://github.com/datawhalechina/hugging-llm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HuggingLLM&lt;/a&gt;），全民AI浪潮涌现。2024年，基于LLM的应用已经开始成熟，Agent百花齐放，进入元年，各种应用层出不穷，一个人公司成为可能。&lt;/p&gt;
&lt;p&gt;当我们以为LLM基本就这样按部就班向”应用“时，R1出现了，它发迹于OpenAI-o1，但超越了o1。关于o1，我的观点和OpenAI前首席研究官Bob的观点一致：它的目标是解决复杂问题，大多数人日常工作中并不会遇到需要o1的需求（可以参考&lt;a href=&quot;https://yam.gift/2024/12/20/NLP/2024-12-20-Think-About-AI-and-Related/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;关于AI前沿的思考&lt;/a&gt;）。但是R1提升了LLM的整体能力，让模型真正在推理时进行自我反思和验证，这当然适用于复杂问题，但日常工作很多场景也能受益，AI更加像人。我觉得这是R1对整个行业的贡献，其作用不亚于ChatGPT的发布。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="Post-training" scheme="https://www.yam.gift/tags/Post-training/"/>
    
      <category term="R1" scheme="https://www.yam.gift/tags/R1/"/>
    
      <category term="R1-Zero" scheme="https://www.yam.gift/tags/R1-Zero/"/>
    
      <category term="oat-zero" scheme="https://www.yam.gift/tags/oat-zero/"/>
    
      <category term="DeepScaleR" scheme="https://www.yam.gift/tags/DeepScaleR/"/>
    
      <category term="LIMO" scheme="https://www.yam.gift/tags/LIMO/"/>
    
      <category term="s1" scheme="https://www.yam.gift/tags/s1/"/>
    
      <category term="Inference Scaling" scheme="https://www.yam.gift/tags/Inference-Scaling/"/>
    
  </entry>
  
  <entry>
    <title>我为什么做开源？</title>
    <link href="https://www.yam.gift/2025/01/12/Diary/2025-01-12-Why-OpenSource/"/>
    <id>https://www.yam.gift/2025/01/12/Diary/2025-01-12-Why-OpenSource/</id>
    <published>2025-01-12T04:00:00.000Z</published>
    <updated>2025-01-12T04:16:06.193Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;开源到书籍&quot;&gt;开源到书籍&lt;/h2&gt;
&lt;p&gt;从《ChatGPT原理与应用开发》这本书开始吧，它获得了异步2024年影响力图书。这本身是一个开源项目&lt;a href=&quot;https://github.com/datawhalechina/hugging-llm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HuggingLLM&lt;/a&gt;，当时（23年4月）的初衷很简单，就是想帮助更多的中小企业使用AI，让非算法的工程师也能借助AI实现算法相关功能和服务。另外，本书另一位作者玉琳说网上太多智商税的课程了，觉得我们应该做点什么，于是一拍即合就有了这个项目。但具体开始做的时候，我觉得还是应该有一些创新，并且内容的生命力尽量持久些。思来想去，再结合市场调研结果，决定以NLP算法常见任务为导向，通过借助LLM让普通程序员也能做NLP算法工程师的工作。同时内容尽量保持实战性，代码可直接复用到工作环境。这是从NLP范式角度展开的一本书，是最大的创新点，同时范式是不容易改变的，也保证了书籍的生命力。&lt;/p&gt;
&lt;p&gt;书籍出版后，其实还是有点担心的，我当时对这本书的评价是有一定价值，但整体质量其实一般。不过有一点我觉得是好的，就是到现在为止书的框架依然是正确的，且目测会在很长一段时间内仍然有效。后来微信读书评价果然还可以，有评价说看得出作者在NLP领域浸淫多年，这是不错的，有些东西光眼看不经历实际项目是没用的。其实我当时还看了微信读书的基本同类型书，有些书质量不错，但也有些拼凑感很重，都是网上到处整理的资料，果然，这些书的评论里就有不少人提到了，看来读者的眼睛是雪亮的。说回本书，其实我觉得整体还是太粗糙了，毕竟时间点紧，没太多时间打磨，内容呢也比较简单，是真的非常简单，毕竟是给非行业人士看的。我都跟业内人说你们别看，太简单了，不要浪费时间，当然更不要买，网上都有全书电子稿。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Growth" scheme="https://www.yam.gift/tags/Growth/"/>
    
      <category term="Dream" scheme="https://www.yam.gift/tags/Dream/"/>
    
      <category term="OpenSource" scheme="https://www.yam.gift/tags/OpenSource/"/>
    
  </entry>
  
  <entry>
    <title>实时语音交互场景下RAG的机遇和挑战</title>
    <link href="https://www.yam.gift/2025/01/05/MM/2025-01-05-RAG-and-Voice-Agent/"/>
    <id>https://www.yam.gift/2025/01/05/MM/2025-01-05-RAG-and-Voice-Agent/</id>
    <published>2025-01-05T15:00:00.000Z</published>
    <updated>2025-01-05T16:50:40.153Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;这是2025年1月4日笔者受邀参加Zilliz举办的【向心力】系列会议《中美AI应用与落地分享》专场中的演讲，特此记录。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文与演讲不完全相同，但核心内容一致。其中涉及到的内容还比较新，观点不一定准确，供参考交流。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这次分享的题目是《实时语音交互场景下RAG的机遇和挑战》，内容主要包括四个方面：主题引入、实时语音交互与RAG的结合、面临的技术挑战和未来的机遇与发展方向。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="RAG" scheme="https://www.yam.gift/tags/RAG/"/>
    
      <category term="VoiceAgent" scheme="https://www.yam.gift/tags/VoiceAgent/"/>
    
  </entry>
  
  <entry>
    <title>预训练：无处安放的躁动之心</title>
    <link href="https://www.yam.gift/2025/01/05/NLP/LLM-Training/2025-01-05-LLM-Pretrain-PreStart/"/>
    <id>https://www.yam.gift/2025/01/05/NLP/LLM-Training/2025-01-05-LLM-Pretrain-PreStart/</id>
    <published>2025-01-05T15:00:00.000Z</published>
    <updated>2025-02-28T00:47:30.706Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;这个系列打算开始做一个预训练小模型，Size暂定在1.5B。这个念头源于和几个朋友的一次聚餐，当时聊到了Scale Law，以及小模型，有两个观点促使了笔者做这个决定。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;小模型，在智能和一些大模型相媲美的时候是有意义的。&lt;/li&gt;
&lt;li&gt;Scale Law不光表现在模型层面，也表现在数据层面。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
  </entry>
  
  <entry>
    <title>LLM指令跟随论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Instruction-Following-Papers/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Instruction-Following-Papers/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:12:08.663Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;LLM指令跟随相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Instruction Following" scheme="https://www.yam.gift/tags/Instruction-Following/"/>
    
  </entry>
  
  <entry>
    <title>SLM论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-SLM-Papers/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-SLM-Papers/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:21:25.869Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;SLM相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="SLM" scheme="https://www.yam.gift/tags/SLM/"/>
    
      <category term="MM Fusion" scheme="https://www.yam.gift/tags/MM-Fusion/"/>
    
  </entry>
  
  <entry>
    <title>音频Codec论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-Codec-Papers/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-Codec-Papers/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:21:57.758Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Codec相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
  </entry>
  
  <entry>
    <title>VITS</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-VITS/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-VITS/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-13T08:50:35.814Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2106.06103&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/a&gt;&lt;br&gt;
代码：&lt;a href=&quot;https://github.com/jaywalnut310/vits&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;jaywalnut310/vits: VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一个并行的端到端TTS模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="VITS" scheme="https://www.yam.gift/tags/VITS/"/>
    
  </entry>
  
  <entry>
    <title>XTTS</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-XTTS/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-XTTS/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:27:01.808Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2406.04904&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model&lt;/a&gt;&lt;br&gt;
代码：&lt;a href=&quot;https://github.com/coqui-ai/TTS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;coqui-ai/TTS: 🐸💬 - a deep learning toolkit for Text-to-Speech, battle-tested in research and production&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;基于Tortoise的改进，自回归。本文主要关心架构。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="XTTS" scheme="https://www.yam.gift/tags/XTTS/"/>
    
  </entry>
  
  <entry>
    <title>OMNI论文速览（2024）</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-OMNI-Papers-2024/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-OMNI-Papers-2024/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-03-06T12:40:38.101Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;OMNI相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="OMNI" scheme="https://www.yam.gift/tags/OMNI/"/>
    
  </entry>
  
  <entry>
    <title>DAC</title>
    <link href="https://www.yam.gift/2024/12/30/Paper/TTS/2024-12-30-DAC/"/>
    <id>https://www.yam.gift/2024/12/30/Paper/TTS/2024-12-30-DAC/</id>
    <published>2024-12-30T15:00:00.000Z</published>
    <updated>2025-01-01T00:25:58.731Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2306.06546&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;High-Fidelity Audio Compression with Improved RVQGAN&lt;/a&gt;&lt;br&gt;
代码：&lt;a href=&quot;https://github.com/descriptinc/descript-audio-codec&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;descriptinc/descript-audio-codec: State-of-the-art audio codec with 90x compression factor. Supports 44.1kHz, 24kHz, and 16kHz mono/stereo audio.&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="DAC" scheme="https://www.yam.gift/tags/DAC/"/>
    
  </entry>
  
  <entry>
    <title>TS3-Codec</title>
    <link href="https://www.yam.gift/2024/12/27/Paper/TTS/2024-12-27-TS3-Codec/"/>
    <id>https://www.yam.gift/2024/12/27/Paper/TTS/2024-12-27-TS3-Codec/</id>
    <published>2024-12-27T15:00:00.000Z</published>
    <updated>2025-01-01T00:24:54.810Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2411.18803&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TS3-Codec: Transformer-Based Simple Streaming Single Codec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;没开源代码。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="TS3-Codec" scheme="https://www.yam.gift/tags/TS3-Codec/"/>
    
  </entry>
  
  <entry>
    <title>BigCodec</title>
    <link href="https://www.yam.gift/2024/12/26/Paper/TTS/2024-12-26-BigCodec/"/>
    <id>https://www.yam.gift/2024/12/26/Paper/TTS/2024-12-26-BigCodec/</id>
    <published>2024-12-26T15:00:00.000Z</published>
    <updated>2025-01-01T00:25:03.980Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2409.05377&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码：&lt;a href=&quot;https://github.com/Aria-K-Alethia/BigCodec&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Aria-K-Alethia/BigCodec&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="BigCodec" scheme="https://www.yam.gift/tags/BigCodec/"/>
    
  </entry>
  
</feed>
