<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2021-10-27T18:01:49.625Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>Yam</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>浅析文本分类——情感分析与自然语言处理</title>
    <link href="https://www.yam.gift/2021/10/27/NLP/2021-10-27-Senta/"/>
    <id>https://www.yam.gift/2021/10/27/NLP/2021-10-27-Senta/</id>
    <published>2021-10-27T15:00:00.000Z</published>
    <updated>2021-10-27T18:01:49.625Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;文本分类是自然语言处理（以下使用 NLP 简称）最基础核心的任务，或者换句话说，几乎所有的任务都是「分类」任务，或者涉及到「分类」这个概念。比如分词、词性标注、命名实体识别等序列标注任务其实就是 Token 粒度的分类；再比如文本生成其实也可以理解为 Token 粒度在整个词表上的分类任务。为何会如此？这篇&lt;a href=&quot;https://yam.gift/2020/11/28/AI/2020-11-28-Classification-and-AI/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;分类与AI&lt;/a&gt;可能会给您带来一点启示。&lt;/p&gt;
&lt;p&gt;本文篇幅较长，主要分为以下几个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;背景介绍：主要简单介绍情感分析相关的概念、类型，并和文本分类任务对应。&lt;/li&gt;
&lt;li&gt;基本流程：主要介绍文本分类（或常见的 NLP 任务）基本处理流程。&lt;/li&gt;
&lt;li&gt;模型简史：主要介绍 NLP 处理任务中模型变迁的简单历史。&lt;/li&gt;
&lt;li&gt;情感未来：主要探讨未来情感分析可能的发展方向。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文侧重于从宏观角度（历史演变和基本流程）对文本情感分类任务进行介绍，目的是给读者提供一个整体视角，从高远处审视情感分析、文本分类、甚至 NLP，期望能抛砖引玉，引发读者更多的思考。如果您想了解工业界的具体方案和进展，最后的《文献资料》部分有两篇来自淘系和美团的文章值得仔细研究。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文同样适合对机器学习和深度学习稍微有一些了解的非算法岗位的工程师，或其他无技术背景但对 NLP 感兴趣的非工程师。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Classification" scheme="https://www.yam.gift/tags/Classification/"/>
    
      <category term="SentimentAnalysis" scheme="https://www.yam.gift/tags/SentimentAnalysis/"/>
    
      <category term="Text Classification" scheme="https://www.yam.gift/tags/Text-Classification/"/>
    
      <category term="Senta" scheme="https://www.yam.gift/tags/Senta/"/>
    
  </entry>
  
  <entry>
    <title>TensorBay 指南</title>
    <link href="https://www.yam.gift/2021/09/21/NLP/2021-09-21-TensorBay-Intro/"/>
    <id>https://www.yam.gift/2021/09/21/NLP/2021-09-21-TensorBay-Intro/</id>
    <published>2021-09-21T15:30:00.000Z</published>
    <updated>2021-10-16T14:52:48.839Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;本篇文章主要介绍如何使用 Tensorbay 和 Tensorflow 进行简单的文本分类。&lt;/p&gt;
&lt;p&gt;我们首先介绍 Tensorbay 相关使用，然后简单介绍 NLP 领域常用的分类模型：TextCNN，最后将所有的串起来完成一个简单的文本分类任务。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="TextCNN" scheme="https://www.yam.gift/tags/TextCNN/"/>
    
      <category term="TensorBay" scheme="https://www.yam.gift/tags/TensorBay/"/>
    
      <category term="Tensorflow" scheme="https://www.yam.gift/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>在 TextCNN 上实验 Dropout 和对比学习</title>
    <link href="https://www.yam.gift/2021/08/31/AI/2021-08-31-SL-CL-Dropout/"/>
    <id>https://www.yam.gift/2021/08/31/AI/2021-08-31-SL-CL-Dropout/</id>
    <published>2021-08-31T15:00:00.000Z</published>
    <updated>2021-09-09T23:56:02.059Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;一句话概述：即使在简单模型上，使用 SimCSE 和 R-Drop 也能够起到一定效果，但太简单的模型（类似 TextCNN）效果可能不太明显。如果嫌麻烦也可以不用，但 Dropout 最好使用，主要用在稠密连接，比如 Embedding、Concat、Attention、FC 等层的后面。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果只想看结论，到这里就可以结束啦。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="SimCSE" scheme="https://www.yam.gift/tags/SimCSE/"/>
    
      <category term="R-Drop" scheme="https://www.yam.gift/tags/R-Drop/"/>
    
      <category term="Dropout" scheme="https://www.yam.gift/tags/Dropout/"/>
    
      <category term="TextCNN" scheme="https://www.yam.gift/tags/TextCNN/"/>
    
  </entry>
  
  <entry>
    <title>R-Drop</title>
    <link href="https://www.yam.gift/2021/08/18/Paper/2021-08-18-R-Drop/"/>
    <id>https://www.yam.gift/2021/08/18/Paper/2021-08-18-R-Drop/</id>
    <published>2021-08-18T15:00:00.000Z</published>
    <updated>2021-08-23T14:44:32.020Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2106.14448&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2106.14448] R-Drop: Regularized Dropout for Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/dropreg/R-Drop&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;dropreg/R-Drop&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概述：用 KL 散度作为损失的 SimCSE。&lt;/p&gt;
&lt;p&gt;摘要：Dropout 是深度学习训练时广泛使用的正则化工具，本文提出 R-Drop，强迫不同 Dropout 模型（就是带 Dropout 的模型跑两次数据）的输出分布彼此保持一致。具体通过最小化两个输出的双向 KL 散度，R-Drop 降低了模型参数的自由度并补充了 Dropout，从而降低了模型的空间复杂度，增强了泛化能力。效果那自然也是非常不错的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="R-Drop" scheme="https://www.yam.gift/tags/R-Drop/"/>
    
      <category term="Dropout" scheme="https://www.yam.gift/tags/Dropout/"/>
    
  </entry>
  
  <entry>
    <title>高性能数据处理</title>
    <link href="https://www.yam.gift/2021/08/12/DataSci/2021-08-12-Dataprocess-Performance/"/>
    <id>https://www.yam.gift/2021/08/12/DataSci/2021-08-12-Dataprocess-Performance/</id>
    <published>2021-08-12T15:00:00.000Z</published>
    <updated>2021-08-12T16:19:42.413Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;最近在写深度学习开源库时，遇到了读取语料并预处理的 API，于是趁此机会整理一下之前积累的关于高性能的内容。全文包括两个部分：第一部分主要聚焦在常用工具 Pandas 处理数据时不同操作方法的性能；第二部分主要介绍一些加速数值计算的工具；第三部分主要介绍一些能够辅助增加性能的工具。&lt;/p&gt;
&lt;p&gt;如果懒得看过程，可以直接看结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数值计算任务：&lt;code&gt;numba&lt;/code&gt; 或 &lt;code&gt;Cython&lt;/code&gt; 加速；使用 &lt;code&gt;.values&lt;/code&gt; 计算&lt;/li&gt;
&lt;li&gt;遍历的非数值计算任务：&lt;code&gt;df.itertuples&lt;/code&gt; 或 &lt;code&gt;df.apply&lt;/code&gt; 方法&lt;/li&gt;
&lt;li&gt;数据太大 Pandas 处理不了的可以使用 Arrow 和 Polars，再不行了使用 Spark&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有测试代码在：&lt;a href=&quot;https://github.com/hscspring/Note_DS/blob/master/Performance.ipynb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Note_DS/Performance.ipynb&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Pandas" scheme="https://www.yam.gift/tags/Pandas/"/>
    
      <category term="Arrow" scheme="https://www.yam.gift/tags/Arrow/"/>
    
      <category term="Polars" scheme="https://www.yam.gift/tags/Polars/"/>
    
      <category term="Numpy" scheme="https://www.yam.gift/tags/Numpy/"/>
    
      <category term="Jax" scheme="https://www.yam.gift/tags/Jax/"/>
    
      <category term="Numba" scheme="https://www.yam.gift/tags/Numba/"/>
    
      <category term="Pandarallel" scheme="https://www.yam.gift/tags/Pandarallel/"/>
    
  </entry>
  
  <entry>
    <title>Git Memo</title>
    <link href="https://www.yam.gift/2021/08/12/Unix/2021-08-15-Git/"/>
    <id>https://www.yam.gift/2021/08/12/Unix/2021-08-15-Git/</id>
    <published>2021-08-12T15:00:00.000Z</published>
    <updated>2021-08-12T16:13:34.621Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Git 相关备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Git" scheme="https://www.yam.gift/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>Docker Memo</title>
    <link href="https://www.yam.gift/2021/08/05/Unix/2021-08-05-Docker/"/>
    <id>https://www.yam.gift/2021/08/05/Unix/2021-08-05-Docker/</id>
    <published>2021-08-05T15:00:00.000Z</published>
    <updated>2021-08-12T16:19:12.067Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Docker 备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Docker" scheme="https://www.yam.gift/tags/Docker/"/>
    
      <category term="Dockerfile" scheme="https://www.yam.gift/tags/Dockerfile/"/>
    
      <category term="Docker-Compose" scheme="https://www.yam.gift/tags/Docker-Compose/"/>
    
  </entry>
  
  <entry>
    <title>UniLM</title>
    <link href="https://www.yam.gift/2021/07/31/Paper/2021-07-31-UniLM/"/>
    <id>https://www.yam.gift/2021/07/31/Paper/2021-07-31-UniLM/</id>
    <published>2021-07-31T15:00:00.000Z</published>
    <updated>2021-07-31T11:36:20.005Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/1905.03197&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1905.03197] Unified Language Model Pre-training for Natural Language Understanding and Generation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/microsoft/unilm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;microsoft/unilm: UniLM AI - Unified “Language” Model Pre-training across Tasks, Languages, and Modalities&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概述：一个通过参数共享训练多种语言模型以同时适应下游 NLU 和 NLG 微调的统一框架。&lt;/p&gt;
&lt;p&gt;摘要：UniLM，统一的预训练语言模型，可以同时微调 NLU 和 NLG 任务。做法是使用三个不同类型的语言模型任务：单向、双向、Seq2Seq 预测。具体是使用一个共享的 Transformer 网络，并利用不同的 Self-Attention Mask 来控制预测基于哪些上下文。结果自然是很好（不，极好）的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="Bert" scheme="https://www.yam.gift/tags/Bert/"/>
    
      <category term="UniLM" scheme="https://www.yam.gift/tags/UniLM/"/>
    
  </entry>
  
  <entry>
    <title>通过最优转移进行词表学习：VOLT</title>
    <link href="https://www.yam.gift/2021/07/18/Paper/2021-07-18-VOLT/"/>
    <id>https://www.yam.gift/2021/07/18/Paper/2021-07-18-VOLT/</id>
    <published>2021-07-18T15:00:00.000Z</published>
    <updated>2021-07-26T11:43:17.782Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2012.15671&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2012.15671] Vocabulary Learning via Optimal Transport for Machine Translation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/Jingjing-NLP/VOLT&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Jingjing-NLP/VOLT: Code for paper “Vocabulary Learning via Optimal Transport for Neural Machine Translation”&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概述：借鉴边际效用通过最优转移学习词表。&lt;/p&gt;
&lt;p&gt;摘要：机器翻译词表影响性能，本文旨在弄清楚什么是好的词表，以及是否可以在不尝试训练的情况下找到最佳词表。为此，作者首先从信息论的视角提供对词表作用的另一种理解。受此启发，将词汇化的探索 —— 找到具有适当大小的最佳词表—— 作为最优转移问题。提出了 VOLT，一个简单有效的无须尝试训练的解决方案。实验结果表明，VOLT 在各种场景中优于广泛使用的词表。此外，与 BPE-search 相比，VOLT 将搜索时间从 384 GPU 小时减少到 30 GPU 小时。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Vocabulary Learning" scheme="https://www.yam.gift/tags/Vocabulary-Learning/"/>
    
      <category term="NMT" scheme="https://www.yam.gift/tags/NMT/"/>
    
      <category term="BPE" scheme="https://www.yam.gift/tags/BPE/"/>
    
  </entry>
  
  <entry>
    <title>简单的对比学习框架：SimCSE</title>
    <link href="https://www.yam.gift/2021/07/10/Paper/2021-07-10-SImCSE/"/>
    <id>https://www.yam.gift/2021/07/10/Paper/2021-07-10-SImCSE/</id>
    <published>2021-07-10T15:00:00.000Z</published>
    <updated>2021-07-11T14:41:01.639Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2104.08821&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2104.08821] SimCSE: Simple Contrastive Learning of Sentence Embeddings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/princeton-nlp/SimCSE&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;princeton-nlp/SimCSE: SimCSE: Simple Contrastive Learning of Sentence Embeddings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概述：Dropout 增益句子 Embedding。&lt;/p&gt;
&lt;p&gt;摘要：本文提出一个简单的对比学习框架，极大地提高了句子的表征能力。首先是无监督的方法，使用一个输入句子，在对比目标中预测自己，这里仅使用标准的 dropout 作为噪声。接下来将 NLI 数据集中的标注对合并到对比学习中，“蕴涵”对作为正例，“矛盾”对作为负例。最后，论文还发现对比学习在理论上能够将预训练 Embedding 的各向异性空间正则化，使其更加均匀，而且有监督信号可用时，可以更好地对齐正例对。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://qnimg.lovevivian.cn/paper-simcse-1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Embedding" scheme="https://www.yam.gift/tags/Embedding/"/>
    
      <category term="Contrastive-Learning" scheme="https://www.yam.gift/tags/Contrastive-Learning/"/>
    
      <category term="SimCSE" scheme="https://www.yam.gift/tags/SimCSE/"/>
    
  </entry>
  
  <entry>
    <title>高效深度学习：让模型更小、更快、更好</title>
    <link href="https://www.yam.gift/2021/07/04/Paper/2021-07-04-Efficient-DeepLearning/"/>
    <id>https://www.yam.gift/2021/07/04/Paper/2021-07-04-Efficient-DeepLearning/</id>
    <published>2021-07-04T15:00:00.000Z</published>
    <updated>2021-10-31T14:54:34.690Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2106.08962&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2106.08962] Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/reddragon/efficient-dl-survey-paper&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;reddragon/efficient-dl-survey-paper: Efficient Deep Learning Survey Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概述：一份实用的模型训练和部署「优化」指南。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Efficient-DeepLearning" scheme="https://www.yam.gift/tags/Efficient-DeepLearning/"/>
    
      <category term="Quantization" scheme="https://www.yam.gift/tags/Quantization/"/>
    
      <category term="Distillation" scheme="https://www.yam.gift/tags/Distillation/"/>
    
      <category term="Automation" scheme="https://www.yam.gift/tags/Automation/"/>
    
      <category term="Pruning" scheme="https://www.yam.gift/tags/Pruning/"/>
    
  </entry>
  
  <entry>
    <title>机器之眼：树莓派摄像头</title>
    <link href="https://www.yam.gift/2021/07/03/Raspberrypi/2021-07-03-RaspberryPi-Camera/"/>
    <id>https://www.yam.gift/2021/07/03/Raspberrypi/2021-07-03-RaspberryPi-Camera/</id>
    <published>2021-07-03T15:00:00.000Z</published>
    <updated>2021-07-05T16:19:58.630Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;如果把树莓派比作机器人的大脑，那么摄像头相当于机器人的眼睛，我们需要使用摄像头不间断获取图片或视频流，然后通过图像识别技术判断「眼前」的物品/人，进而做出一些响应。目前已调通，可以通过摄像头获取实时画面，所以赶紧记录一下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="RaspberryPi" scheme="https://www.yam.gift/tags/RaspberryPi/"/>
    
      <category term="Camera" scheme="https://www.yam.gift/tags/Camera/"/>
    
      <category term="ffmpeg" scheme="https://www.yam.gift/tags/ffmpeg/"/>
    
      <category term="vlc" scheme="https://www.yam.gift/tags/vlc/"/>
    
      <category term="motion" scheme="https://www.yam.gift/tags/motion/"/>
    
  </entry>
  
  <entry>
    <title>Unix Cheat Sheet</title>
    <link href="https://www.yam.gift/2021/07/02/Unix/2021-07-02-Unix-Cheat-Sheet/"/>
    <id>https://www.yam.gift/2021/07/02/Unix/2021-07-02-Unix-Cheat-Sheet/</id>
    <published>2021-07-02T15:00:00.000Z</published>
    <updated>2021-08-15T03:43:12.244Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Unix &amp;amp; Linux 相关备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Unix" scheme="https://www.yam.gift/tags/Unix/"/>
    
      <category term="Linux" scheme="https://www.yam.gift/tags/Linux/"/>
    
      <category term="Raspberrypi" scheme="https://www.yam.gift/tags/Raspberrypi/"/>
    
  </entry>
  
  <entry>
    <title>机器之脑：树莓派初使用</title>
    <link href="https://www.yam.gift/2021/07/01/Raspberrypi/2021-07-01-RaspberryPi-Init/"/>
    <id>https://www.yam.gift/2021/07/01/Raspberrypi/2021-07-01-RaspberryPi-Init/</id>
    <published>2021-07-01T15:00:00.000Z</published>
    <updated>2021-07-05T15:43:46.057Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;拖延症太厉害了，这次终于下定决心要把自己一直想做的小弟（同时兼小秘）给做起来，什么时候做好不知道，但不能不开始。第一步要整的就是大脑，用一块树莓派承载，里面慢慢给灌上各种软件和模型。本文主要整理记录树莓派初始配置操作，主要针对的是&lt;strong&gt;远程 ssh 无屏幕连接无桌面版&lt;/strong&gt;树莓派（4B），请注意限制条件，其他的操作也类似。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="RaspberryPi" scheme="https://www.yam.gift/tags/RaspberryPi/"/>
    
      <category term="Tutorial" scheme="https://www.yam.gift/tags/Tutorial/"/>
    
  </entry>
  
  <entry>
    <title>预训练模型的过去、现在和未来</title>
    <link href="https://www.yam.gift/2021/06/20/Paper/2021-06-20-PretrainedModels/"/>
    <id>https://www.yam.gift/2021/06/20/Paper/2021-06-20-PretrainedModels/</id>
    <published>2021-06-20T15:59:00.000Z</published>
    <updated>2021-06-27T15:14:49.085Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2106.07139&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2106.07139] Pre-Trained Models: Past, Present and Future&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code： 无&lt;/p&gt;
&lt;p&gt;一句话概括：如题；）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="BERT" scheme="https://www.yam.gift/tags/BERT/"/>
    
      <category term="Pretrained" scheme="https://www.yam.gift/tags/Pretrained/"/>
    
      <category term="Pre-Trained" scheme="https://www.yam.gift/tags/Pre-Trained/"/>
    
      <category term="PTM" scheme="https://www.yam.gift/tags/PTM/"/>
    
      <category term="Pre-Training" scheme="https://www.yam.gift/tags/Pre-Training/"/>
    
  </entry>
  
  <entry>
    <title>Python 调用 Java</title>
    <link href="https://www.yam.gift/2021/06/14/Python/2021-06-14-Python-Call-Java/"/>
    <id>https://www.yam.gift/2021/06/14/Python/2021-06-14-Python-Call-Java/</id>
    <published>2021-06-14T12:00:00.000Z</published>
    <updated>2021-06-14T12:47:09.814Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;一直以为这样的情况不会出现，但它还是出现了：一段 Java 代码+引用 Jar 包，一段 Python 代码要使用 Java 代码中某个方法。本来想用 Python 重新实现一遍，又觉得这简直是浪费时间，何不直接在 Python 代码中使用 Java 代码的该方法呢？应该特别简单，分分钟搞定的事情，结果还是掉坑里了，特此记录，以备后查。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Java" scheme="https://www.yam.gift/tags/Java/"/>
    
      <category term="jpype" scheme="https://www.yam.gift/tags/jpype/"/>
    
  </entry>
  
  <entry>
    <title>对NLP预训练模型的思考</title>
    <link href="https://www.yam.gift/2021/06/10/NLP/2021-06-10-Pretrain-Thinking/"/>
    <id>https://www.yam.gift/2021/06/10/NLP/2021-06-10-Pretrain-Thinking/</id>
    <published>2021-06-10T15:30:00.000Z</published>
    <updated>2021-06-11T14:34:07.077Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;最近连续读了两篇关于 BERT 学习机理的文章，略有所感，记录如下。&lt;/p&gt;
&lt;p&gt;预训练模型本质是利用输入数据本身内在的结构进行学习，从自然语言处理的角度看，就是充分利用自然语言文本的上下文去学习到文本的表征。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Representation" scheme="https://www.yam.gift/tags/Representation/"/>
    
      <category term="Pretrain" scheme="https://www.yam.gift/tags/Pretrain/"/>
    
  </entry>
  
  <entry>
    <title>Jupyter Notebook Cheat Sheet</title>
    <link href="https://www.yam.gift/2021/06/07/Python/2021-06-07-JupyterCheatSheet/"/>
    <id>https://www.yam.gift/2021/06/07/Python/2021-06-07-JupyterCheatSheet/</id>
    <published>2021-06-07T15:00:00.000Z</published>
    <updated>2021-06-07T16:09:20.832Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Jupyter Notebook 的相关备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Jupyter" scheme="https://www.yam.gift/tags/Jupyter/"/>
    
  </entry>
  
  <entry>
    <title>Few-Shot NER and BERT Noisy Learning：ProtoBERT Paper Note</title>
    <link href="https://www.yam.gift/2021/06/06/Paper/2021-06-06-ProtoBERT/"/>
    <id>https://www.yam.gift/2021/06/06/Paper/2021-06-06-ProtoBERT/</id>
    <published>2021-06-06T15:00:00.000Z</published>
    <updated>2021-06-06T14:02:17.473Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2105.00828&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2105.00828 BERT memorisation and pitfalls in low-resource scenarios&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：无&lt;/p&gt;
&lt;p&gt;核心思想：结合原型网络，将少样本的标签表征为稠密向量。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="BERT" scheme="https://www.yam.gift/tags/BERT/"/>
    
      <category term="NER" scheme="https://www.yam.gift/tags/NER/"/>
    
      <category term="ProtoBERT" scheme="https://www.yam.gift/tags/ProtoBERT/"/>
    
      <category term="few-shot" scheme="https://www.yam.gift/tags/few-shot/"/>
    
  </entry>
  
  <entry>
    <title>Rust str 转 String</title>
    <link href="https://www.yam.gift/2021/06/06/Rust/2021-06-06-str2String/"/>
    <id>https://www.yam.gift/2021/06/06/Rust/2021-06-06-str2String/</id>
    <published>2021-06-06T15:00:00.000Z</published>
    <updated>2021-06-07T16:18:44.560Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;在 Rust 中，&lt;code&gt;str&lt;/code&gt; 是引用，&lt;code&gt;String&lt;/code&gt; 是字符串对象，如下所示，&lt;a href=&quot;https://play.rust-lang.org/?version=stable&amp;amp;mode=debug&amp;amp;edition=2018&amp;amp;gist=4fd8146578daef00aa903218053c6cf7&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;点击执行&lt;/a&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight rust&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 代码来自 https://github.com/rust-lang/rustlings&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;string_slice&lt;/span&gt;&lt;/span&gt;(arg: &amp;amp;&lt;span class=&quot;keyword&quot;&gt;str&lt;/span&gt;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;println!&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;&amp;#123;&amp;#125;&quot;&lt;/span&gt;, arg);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;string&lt;/span&gt;&lt;/span&gt;(arg: &lt;span class=&quot;built_in&quot;&gt;String&lt;/span&gt;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;println!&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;&amp;#123;&amp;#125;&quot;&lt;/span&gt;, arg);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;/span&gt;() &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;red&quot;&lt;/span&gt;.to_string());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;rust is fun!&quot;&lt;/span&gt;.to_owned());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;nice weather&quot;&lt;/span&gt;.into());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;built_in&quot;&gt;String&lt;/span&gt;::from(&lt;span class=&quot;string&quot;&gt;&quot;hi&quot;&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;built_in&quot;&gt;format!&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;Interpolation &amp;#123;&amp;#125;&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;Station&quot;&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;Happy Monday!&quot;&lt;/span&gt;.to_string().replace(&lt;span class=&quot;string&quot;&gt;&quot;Mon&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;Tues&quot;&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;mY sHiFt KeY iS sTiCkY&quot;&lt;/span&gt;.to_lowercase());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string_slice(&lt;span class=&quot;string&quot;&gt;&quot;blue&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string_slice(&amp;amp;&lt;span class=&quot;built_in&quot;&gt;String&lt;/span&gt;::from(&lt;span class=&quot;string&quot;&gt;&quot;abc&quot;&lt;/span&gt;)[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;..&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string_slice(&amp;amp;&lt;span class=&quot;string&quot;&gt;&quot;abc&quot;&lt;/span&gt;.to_string()[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;..&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string_slice(&lt;span class=&quot;string&quot;&gt;&quot;  hello there &quot;&lt;/span&gt;.trim());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Rust" scheme="https://www.yam.gift/tags/Rust/"/>
    
      <category term="String" scheme="https://www.yam.gift/tags/String/"/>
    
      <category term="str" scheme="https://www.yam.gift/tags/str/"/>
    
  </entry>
  
</feed>
