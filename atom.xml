<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2021-07-10T14:11:08.420Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>Yam</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>简单的对比学习框架：SimCSE</title>
    <link href="https://www.yam.gift/2021/07/10/Paper/2021-07-10-SImCSE/"/>
    <id>https://www.yam.gift/2021/07/10/Paper/2021-07-10-SImCSE/</id>
    <published>2021-07-10T15:00:00.000Z</published>
    <updated>2021-07-10T14:11:08.420Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2104.08821&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2104.08821] SimCSE: Simple Contrastive Learning of Sentence Embeddings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/princeton-nlp/SimCSE&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;princeton-nlp/SimCSE: SimCSE: Simple Contrastive Learning of Sentence Embeddings&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概述：Dropout 增益句子 Embedding。&lt;/p&gt;
&lt;p&gt;摘要：本文提出一个简单的对比学习框架，极大地提高了句子的表征能力。首先是无监督的方法，使用一个输入句子，在对比目标中预测自己，这里仅使用标准的 dropout 作为噪声。接下来将 NLI 数据集中的标注对合并到对比学习中，“蕴涵”对作为正例，“矛盾”对作为负例。最后，论文还发现对比学习在理论上能够将预训练 Embedding 的各向异性空间正则化，使其更加均匀，而且有监督信号可用时，可以更好地对齐正例对。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://qnimg.lovevivian.cn/paper-simcse-1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Embedding" scheme="https://www.yam.gift/tags/Embedding/"/>
    
      <category term="Contrastive-Learning" scheme="https://www.yam.gift/tags/Contrastive-Learning/"/>
    
      <category term="SimCSE" scheme="https://www.yam.gift/tags/SimCSE/"/>
    
  </entry>
  
  <entry>
    <title>高效深度学习：让模型更小、更快、更好</title>
    <link href="https://www.yam.gift/2021/07/04/Paper/2021-07-04-Efficient-DeepLearning/"/>
    <id>https://www.yam.gift/2021/07/04/Paper/2021-07-04-Efficient-DeepLearning/</id>
    <published>2021-07-04T15:00:00.000Z</published>
    <updated>2021-07-04T15:27:50.488Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2106.08962&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2106.08962] Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/reddragon/efficient-dl-survey-paper&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;reddragon/efficient-dl-survey-paper: Efficient Deep Learning Survey Paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概述：一份实用的模型训练和部署「优化」指南。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Efficient-DeepLearning" scheme="https://www.yam.gift/tags/Efficient-DeepLearning/"/>
    
      <category term="Quantization" scheme="https://www.yam.gift/tags/Quantization/"/>
    
      <category term="Distillation" scheme="https://www.yam.gift/tags/Distillation/"/>
    
      <category term="Automation" scheme="https://www.yam.gift/tags/Automation/"/>
    
      <category term="Pruning" scheme="https://www.yam.gift/tags/Pruning/"/>
    
  </entry>
  
  <entry>
    <title>机器之眼：树莓派摄像头</title>
    <link href="https://www.yam.gift/2021/07/03/Raspberrypi/2021-07-03-RaspberryPi-Camera/"/>
    <id>https://www.yam.gift/2021/07/03/Raspberrypi/2021-07-03-RaspberryPi-Camera/</id>
    <published>2021-07-03T15:00:00.000Z</published>
    <updated>2021-07-05T16:19:58.630Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;如果把树莓派比作机器人的大脑，那么摄像头相当于机器人的眼睛，我们需要使用摄像头不间断获取图片或视频流，然后通过图像识别技术判断「眼前」的物品/人，进而做出一些响应。目前已调通，可以通过摄像头获取实时画面，所以赶紧记录一下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="RaspberryPi" scheme="https://www.yam.gift/tags/RaspberryPi/"/>
    
      <category term="Camera" scheme="https://www.yam.gift/tags/Camera/"/>
    
      <category term="ffmpeg" scheme="https://www.yam.gift/tags/ffmpeg/"/>
    
      <category term="vlc" scheme="https://www.yam.gift/tags/vlc/"/>
    
      <category term="motion" scheme="https://www.yam.gift/tags/motion/"/>
    
  </entry>
  
  <entry>
    <title>Unix Cheat Sheet</title>
    <link href="https://www.yam.gift/2021/07/02/Unix/2021-07-02-Unix-Cheat-Sheet/"/>
    <id>https://www.yam.gift/2021/07/02/Unix/2021-07-02-Unix-Cheat-Sheet/</id>
    <published>2021-07-02T15:00:00.000Z</published>
    <updated>2021-07-06T13:48:58.757Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Unix &amp;amp; Linux 相关备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Unix" scheme="https://www.yam.gift/tags/Unix/"/>
    
      <category term="Linux" scheme="https://www.yam.gift/tags/Linux/"/>
    
      <category term="Raspberrypi" scheme="https://www.yam.gift/tags/Raspberrypi/"/>
    
  </entry>
  
  <entry>
    <title>机器之脑：树莓派初使用</title>
    <link href="https://www.yam.gift/2021/07/01/Raspberrypi/2021-07-01-RaspberryPi-Init/"/>
    <id>https://www.yam.gift/2021/07/01/Raspberrypi/2021-07-01-RaspberryPi-Init/</id>
    <published>2021-07-01T15:00:00.000Z</published>
    <updated>2021-07-05T15:43:46.057Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;拖延症太厉害了，这次终于下定决心要把自己一直想做的小弟（同时兼小秘）给做起来，什么时候做好不知道，但不能不开始。第一步要整的就是大脑，用一块树莓派承载，里面慢慢给灌上各种软件和模型。本文主要整理记录树莓派初始配置操作，主要针对的是&lt;strong&gt;远程 ssh 无屏幕连接无桌面版&lt;/strong&gt;树莓派（4B），请注意限制条件，其他的操作也类似。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="RaspberryPi" scheme="https://www.yam.gift/tags/RaspberryPi/"/>
    
      <category term="Tutorial" scheme="https://www.yam.gift/tags/Tutorial/"/>
    
  </entry>
  
  <entry>
    <title>预训练模型的过去、现在和未来</title>
    <link href="https://www.yam.gift/2021/06/20/Paper/2021-06-20-PretrainedModels/"/>
    <id>https://www.yam.gift/2021/06/20/Paper/2021-06-20-PretrainedModels/</id>
    <published>2021-06-20T15:59:00.000Z</published>
    <updated>2021-06-27T15:14:49.085Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2106.07139&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2106.07139] Pre-Trained Models: Past, Present and Future&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code： 无&lt;/p&gt;
&lt;p&gt;一句话概括：如题；）&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="BERT" scheme="https://www.yam.gift/tags/BERT/"/>
    
      <category term="Pretrained" scheme="https://www.yam.gift/tags/Pretrained/"/>
    
      <category term="Pre-Trained" scheme="https://www.yam.gift/tags/Pre-Trained/"/>
    
      <category term="PTM" scheme="https://www.yam.gift/tags/PTM/"/>
    
      <category term="Pre-Training" scheme="https://www.yam.gift/tags/Pre-Training/"/>
    
  </entry>
  
  <entry>
    <title>Python 调用 Java</title>
    <link href="https://www.yam.gift/2021/06/14/Python/2021-06-14-Python-Call-Java/"/>
    <id>https://www.yam.gift/2021/06/14/Python/2021-06-14-Python-Call-Java/</id>
    <published>2021-06-14T12:00:00.000Z</published>
    <updated>2021-06-14T12:47:09.814Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;一直以为这样的情况不会出现，但它还是出现了：一段 Java 代码+引用 Jar 包，一段 Python 代码要使用 Java 代码中某个方法。本来想用 Python 重新实现一遍，又觉得这简直是浪费时间，何不直接在 Python 代码中使用 Java 代码的该方法呢？应该特别简单，分分钟搞定的事情，结果还是掉坑里了，特此记录，以备后查。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Java" scheme="https://www.yam.gift/tags/Java/"/>
    
      <category term="jpype" scheme="https://www.yam.gift/tags/jpype/"/>
    
  </entry>
  
  <entry>
    <title>对NLP预训练模型的思考</title>
    <link href="https://www.yam.gift/2021/06/10/NLP/2021-06-10-Pretrain-Thinking/"/>
    <id>https://www.yam.gift/2021/06/10/NLP/2021-06-10-Pretrain-Thinking/</id>
    <published>2021-06-10T15:30:00.000Z</published>
    <updated>2021-06-11T14:34:07.077Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;最近连续读了两篇关于 BERT 学习机理的文章，略有所感，记录如下。&lt;/p&gt;
&lt;p&gt;预训练模型本质是利用输入数据本身内在的结构进行学习，从自然语言处理的角度看，就是充分利用自然语言文本的上下文去学习到文本的表征。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Representation" scheme="https://www.yam.gift/tags/Representation/"/>
    
      <category term="Pretrain" scheme="https://www.yam.gift/tags/Pretrain/"/>
    
  </entry>
  
  <entry>
    <title>Jupyter Notebook Cheat Sheet</title>
    <link href="https://www.yam.gift/2021/06/07/Python/2021-06-07-JupyterCheatSheet/"/>
    <id>https://www.yam.gift/2021/06/07/Python/2021-06-07-JupyterCheatSheet/</id>
    <published>2021-06-07T15:00:00.000Z</published>
    <updated>2021-06-07T16:09:20.832Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Jupyter Notebook 的相关备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Jupyter" scheme="https://www.yam.gift/tags/Jupyter/"/>
    
  </entry>
  
  <entry>
    <title>Few-Shot NER and BERT Noisy Learning：ProtoBERT Paper Note</title>
    <link href="https://www.yam.gift/2021/06/06/Paper/2021-06-06-ProtoBERT/"/>
    <id>https://www.yam.gift/2021/06/06/Paper/2021-06-06-ProtoBERT/</id>
    <published>2021-06-06T15:00:00.000Z</published>
    <updated>2021-06-06T14:02:17.473Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2105.00828&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;2105.00828 BERT memorisation and pitfalls in low-resource scenarios&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：无&lt;/p&gt;
&lt;p&gt;核心思想：结合原型网络，将少样本的标签表征为稠密向量。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="BERT" scheme="https://www.yam.gift/tags/BERT/"/>
    
      <category term="NER" scheme="https://www.yam.gift/tags/NER/"/>
    
      <category term="ProtoBERT" scheme="https://www.yam.gift/tags/ProtoBERT/"/>
    
      <category term="few-shot" scheme="https://www.yam.gift/tags/few-shot/"/>
    
  </entry>
  
  <entry>
    <title>Rust str 转 String</title>
    <link href="https://www.yam.gift/2021/06/06/Rust/2021-06-06-str2String/"/>
    <id>https://www.yam.gift/2021/06/06/Rust/2021-06-06-str2String/</id>
    <published>2021-06-06T15:00:00.000Z</published>
    <updated>2021-06-07T16:18:44.560Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;在 Rust 中，&lt;code&gt;str&lt;/code&gt; 是引用，&lt;code&gt;String&lt;/code&gt; 是字符串对象，如下所示，&lt;a href=&quot;https://play.rust-lang.org/?version=stable&amp;amp;mode=debug&amp;amp;edition=2018&amp;amp;gist=4fd8146578daef00aa903218053c6cf7&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;点击执行&lt;/a&gt;：&lt;/p&gt;
&lt;figure class=&quot;highlight rust&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 代码来自 https://github.com/rust-lang/rustlings&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;string_slice&lt;/span&gt;&lt;/span&gt;(arg: &amp;amp;&lt;span class=&quot;keyword&quot;&gt;str&lt;/span&gt;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;println!&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;&amp;#123;&amp;#125;&quot;&lt;/span&gt;, arg);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;string&lt;/span&gt;&lt;/span&gt;(arg: &lt;span class=&quot;built_in&quot;&gt;String&lt;/span&gt;) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;println!&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;&amp;#123;&amp;#125;&quot;&lt;/span&gt;, arg);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;/span&gt;() &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;red&quot;&lt;/span&gt;.to_string());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;rust is fun!&quot;&lt;/span&gt;.to_owned());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;nice weather&quot;&lt;/span&gt;.into());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;built_in&quot;&gt;String&lt;/span&gt;::from(&lt;span class=&quot;string&quot;&gt;&quot;hi&quot;&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;built_in&quot;&gt;format!&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;Interpolation &amp;#123;&amp;#125;&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;Station&quot;&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;Happy Monday!&quot;&lt;/span&gt;.to_string().replace(&lt;span class=&quot;string&quot;&gt;&quot;Mon&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;Tues&quot;&lt;/span&gt;));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string(&lt;span class=&quot;string&quot;&gt;&quot;mY sHiFt KeY iS sTiCkY&quot;&lt;/span&gt;.to_lowercase());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string_slice(&lt;span class=&quot;string&quot;&gt;&quot;blue&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string_slice(&amp;amp;&lt;span class=&quot;built_in&quot;&gt;String&lt;/span&gt;::from(&lt;span class=&quot;string&quot;&gt;&quot;abc&quot;&lt;/span&gt;)[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;..&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string_slice(&amp;amp;&lt;span class=&quot;string&quot;&gt;&quot;abc&quot;&lt;/span&gt;.to_string()[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;..&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    string_slice(&lt;span class=&quot;string&quot;&gt;&quot;  hello there &quot;&lt;/span&gt;.trim());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Rust" scheme="https://www.yam.gift/tags/Rust/"/>
    
      <category term="String" scheme="https://www.yam.gift/tags/String/"/>
    
      <category term="str" scheme="https://www.yam.gift/tags/str/"/>
    
  </entry>
  
  <entry>
    <title>深度探索 Bert：BERTology Paper Note</title>
    <link href="https://www.yam.gift/2021/05/22/Paper/2021-05-22-BERTology/"/>
    <id>https://www.yam.gift/2021/05/22/Paper/2021-05-22-BERTology/</id>
    <published>2021-05-22T04:00:00.000Z</published>
    <updated>2021-06-06T13:59:30.553Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2002.12327&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2002.12327] A Primer in BERTology: What we know about how BERT works&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心：全方位研究 BERT 到底学到了什么，怎么学的，效果如何，怎么改善。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="BERT" scheme="https://www.yam.gift/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>AI 工程师养成记（上）</title>
    <link href="https://www.yam.gift/2021/02/19/ExpSum/2021-02-19-AI-Engineer-Growing-I/"/>
    <id>https://www.yam.gift/2021/02/19/ExpSum/2021-02-19-AI-Engineer-Growing-I/</id>
    <published>2021-02-19T15:00:00.000Z</published>
    <updated>2021-02-21T12:59:03.789Z</updated>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B7%A5%E4%BD%9C%E7%AF%87&quot;&gt;工作篇&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B&quot;&gt;工作流程&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87&quot;&gt;数据准备&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90&quot;&gt;数据分析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2&quot;&gt;模型部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B7%A5%E7%A8%8B%E5%BC%80%E5%8F%91&quot;&gt;工程开发&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E8%BF%90%E7%BB%B4%E7%9B%91%E6%8E%A7&quot;&gt;运维监控&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%9B%B4%E6%96%B0%E8%BF%AD%E4%BB%A3&quot;&gt;更新迭代&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B7%A5%E4%BD%9C%E6%96%B9%E6%B3%95&quot;&gt;工作方法&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E8%87%AA%E5%8A%A8%E5%8C%96&quot;&gt;自动化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%89%B9%E9%87%8F%E5%8C%96&quot;&gt;批量化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E7%AE%80%E5%8D%95%E5%8C%96&quot;&gt;简单化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E8%BE%B9%E9%99%85%E6%94%B6%E7%9B%8A%E6%9C%80%E5%A4%A7%E5%8C%96&quot;&gt;边际收益最大化&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%9C%89%E6%95%88%E6%B2%9F%E9%80%9A&quot;&gt;有效沟通&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%98%8E%E7%A1%AE%E9%9C%80%E6%B1%82&quot;&gt;明确需求&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E7%BB%9D%E5%AF%B9%E5%9D%90%E6%A0%87&quot;&gt;绝对坐标&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E4%B8%8D%E5%90%8C%E5%B1%82%E7%BA%A7&quot;&gt;不同层级&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E8%AE%A8%E8%AE%BA%E9%97%AE%E9%A2%98&quot;&gt;讨论问题&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%B9%B3%E7%A8%B3%E5%BF%83%E6%80%81&quot;&gt;平稳心态&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%88%9B%E4%B8%9A%E5%BF%83%E6%80%81&quot;&gt;创业心态&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E6%88%90%E9%95%BF%E5%BF%83%E6%80%81&quot;&gt;成长心态&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#%E5%BD%93%E4%B8%8B%E5%BF%83%E6%80%81&quot;&gt;当下心态&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一转眼转行已四年时间，这是转行以来第一次写关于个人对转行后感想心得的文章。一方面是因为所跨行业过大，行业内也有很多细分领域，要进一步明确方向需要不断试探。事实上，这几年基本能碰的都折腾过了，总算逐渐坚定；另一方面也是感觉一直没有从维度上得到提升，多个领域始终处于不得要领阶段，走了非常多的弯路。这次感受源于与一位资深算法工程师的沟通，又经几篇关于算法工程师工作日常和修养的好文，再加上这些年的积累，认真反思了几日，终于感觉到自己有了质的突破。虽然技能并没有多掌握，但确实比之前强大了不少（虽然依然很弱），有点像炼气期满筑基，奋斗之路刚刚开始，是有此文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="Skill" scheme="https://www.yam.gift/tags/Skill/"/>
    
  </entry>
  
  <entry>
    <title>SqueezeBERT 论文笔记</title>
    <link href="https://www.yam.gift/2021/01/17/Paper/2021-01-17-SqueezeBERT/"/>
    <id>https://www.yam.gift/2021/01/17/Paper/2021-01-17-SqueezeBERT/</id>
    <published>2021-01-17T15:00:00.000Z</published>
    <updated>2021-02-17T14:14:21.013Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2006.11316&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2006.11316] SqueezeBERT: What can computer vision teach NLP about efficient neural networks?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/src/transformers/models/squeezebert&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;transformers/src/transformers/models/squeezebert at master · huggingface/transformers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：把全连接全部替换为卷积的 BERT。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="BERT" scheme="https://www.yam.gift/tags/BERT/"/>
    
      <category term="SqueezeBERT" scheme="https://www.yam.gift/tags/SqueezeBERT/"/>
    
  </entry>
  
  <entry>
    <title>从 Sentence-BERT 谈句子表征</title>
    <link href="https://www.yam.gift/2020/12/27/Paper/2020-12-27-Sentence-Bert/"/>
    <id>https://www.yam.gift/2020/12/27/Paper/2020-12-27-Sentence-Bert/</id>
    <published>2020-12-27T04:00:00.000Z</published>
    <updated>2020-12-27T03:36:48.291Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;在之前那篇 &lt;a href=&quot;https://yam.gift/2020/12/12/2020-12-12-NLP-Representation-History-Future/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NLP 表征的历史与未来 | Yam&lt;/a&gt; 里，我们几乎从头到尾都在提及句子表征，也提出过一个很重要的概念：“句子” 才是语义理解的最小单位。不过当时并没有太过深入细节，直到做到文本相似度任务时才发现早已经有人将其 BERT 化了。这就是本文要提到的一篇很重要但又很顺其自然的一篇论文——Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks。其实说到相似度，大家多少都会想到大名鼎鼎的 Siamese Recurrent Networks，他们当时（2016 年）用的是 LSTM 对句子表征，那是因为那时候 LSTM 效果是最好的。Sentence-BERT 其实就是将 LSTM 替换为 BERT。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="BERT" scheme="https://www.yam.gift/tags/BERT/"/>
    
      <category term="Sentence-BERT" scheme="https://www.yam.gift/tags/Sentence-BERT/"/>
    
      <category term="Siamese" scheme="https://www.yam.gift/tags/Siamese/"/>
    
      <category term="Sentence Similarity" scheme="https://www.yam.gift/tags/Sentence-Similarity/"/>
    
      <category term="Semantic Similarity" scheme="https://www.yam.gift/tags/Semantic-Similarity/"/>
    
      <category term="Cosine Similarity" scheme="https://www.yam.gift/tags/Cosine-Similarity/"/>
    
  </entry>
  
  <entry>
    <title>Bert-Flow 论文笔记</title>
    <link href="https://www.yam.gift/2020/12/13/Paper/2020-12-13-Bert-Flow/"/>
    <id>https://www.yam.gift/2020/12/13/Paper/2020-12-13-Bert-Flow/</id>
    <published>2020-12-13T15:00:00.000Z</published>
    <updated>2020-12-13T15:49:06.279Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2011.05864&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2011.05864] On the Sentence Embeddings from Pre-trained Language Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/bohanli/BERT-flow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;bohanli/BERT-flow: TensorFlow implementation of On the Sentence Embeddings from Pre-trained Language Models (EMNLP 2020)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：无监督方法将 Bert 产生的非平滑各向异性的句子语义空间分布转换为各向同性的高斯分布。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Bert" scheme="https://www.yam.gift/tags/Bert/"/>
    
      <category term="Bert-Flow" scheme="https://www.yam.gift/tags/Bert-Flow/"/>
    
      <category term="Glow" scheme="https://www.yam.gift/tags/Glow/"/>
    
      <category term="Normalizing Flow" scheme="https://www.yam.gift/tags/Normalizing-Flow/"/>
    
  </entry>
  
  <entry>
    <title>NLP 表征的历史与未来</title>
    <link href="https://www.yam.gift/2020/12/12/NLP/2020-12-12-NLP-Representation-History-Future/"/>
    <id>https://www.yam.gift/2020/12/12/NLP/2020-12-12-NLP-Representation-History-Future/</id>
    <published>2020-12-12T15:00:00.000Z</published>
    <updated>2020-12-12T15:46:05.259Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;toc&quot;&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#历史&quot; data-toc-modified-id=&quot;历史-1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1&amp;nbsp;&amp;nbsp;&lt;/span&gt;历史&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#词向量&quot; data-toc-modified-id=&quot;词向量-1.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;词向量&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#CNN&quot; data-toc-modified-id=&quot;CNN-1.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;CNN&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#RNN&quot; data-toc-modified-id=&quot;RNN-1.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;RNN&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#BERT-系列&quot; data-toc-modified-id=&quot;BERT-系列-1.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;BERT 系列&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#未来&quot; data-toc-modified-id=&quot;未来-2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2&amp;nbsp;&amp;nbsp;&lt;/span&gt;未来&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#认知角度&quot; data-toc-modified-id=&quot;认知角度-2.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;认知角度&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#哲学角度&quot; data-toc-modified-id=&quot;哲学角度-2.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;哲学角度&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#模式角度&quot; data-toc-modified-id=&quot;模式角度-2.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;模式角度&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#方法角度&quot; data-toc-modified-id=&quot;方法角度-2.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;方法角度&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Reference&quot; data-toc-modified-id=&quot;Reference-3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3&amp;nbsp;&amp;nbsp;&lt;/span&gt;Reference&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;p&gt;从 Ngram 这样最简单的 SLM（Statistical Language Model）和 OneHot、LSA 以及其他 Co-Occurrence 的 VSM 模型，到 Word2Vec Glove 等考虑简单上下文的词向量模型，再到 CNN RNN BI-LSTM 等更多上下文和更复杂结构的模型，再到基于 Self-Attention 的 Bert 等考虑注意力的模型。我们能够感觉到每一次的变革一定是某种 ”模式“ 层面发生了变化。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Representation" scheme="https://www.yam.gift/tags/Representation/"/>
    
  </entry>
  
  <entry>
    <title>分类与 AI</title>
    <link href="https://www.yam.gift/2020/11/28/AI/2020-11-28-Classification-and-AI/"/>
    <id>https://www.yam.gift/2020/11/28/AI/2020-11-28-Classification-and-AI/</id>
    <published>2020-11-28T15:00:00.000Z</published>
    <updated>2020-11-28T12:11:06.079Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;分类作为机器学习或深度学习的基础任务之一，相信任何一位算法工程师都能说得头头是道。不过，能深入思考其背后涉及到的认知过程和机理的就凤毛麟角了。本文涉及到的思考部分从我 2017 年一开始接触人工智能与 NLP 就开始萌芽了，这源于我的切入点与正常人不同。由于个人经历关系，我一开始是从认知科学这个角度开始自己的工程师生涯的，刚开始看的论文也更加偏向于思考如何构建真正的人工智能。比如，Few-Shot 或 One-Shot Learning、因果推理、快速思考、学习如何学习，甚至开始思考语言学以及究竟什么是智能。很自然地也熟知了图灵、冯诺依曼、维特根斯坦。直至现在依然对这些理论相当沉迷，这也是我当初下定决心从事 AI 领域的原因。虽然目前从事 NLP 研发工作，但我对自己的定位一直都是 AI 工程师，AI 不应该被割裂，他从来都是个整体，作为成年人，我们自然是都要。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="Classification" scheme="https://www.yam.gift/tags/Classification/"/>
    
  </entry>
  
  <entry>
    <title>GBTD + LR 论文笔记</title>
    <link href="https://www.yam.gift/2020/10/30/Paper/2020-10-30-GBTD-LR/"/>
    <id>https://www.yam.gift/2020/10/30/Paper/2020-10-30-GBTD-LR/</id>
    <published>2020-10-30T15:00:00.000Z</published>
    <updated>2020-10-30T16:45:15.000Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;2014 年，Facebook 在论文 &lt;a href=&quot;https://research.fb.com/wp-content/uploads/2016/11/practical-lessons-from-predicting-clicks-on-ads-at-facebook.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;practical-lessons-from-predicting-clicks-on-ads-at-facebook&lt;/a&gt; 中提出了一个将决策树算法和逻辑回归整合起来的模型，大致做法就是将输入的实数特征通过决策树转换为一个二进制的向量，该模型比其他方法在整体性能上提高超过 3 个百分点。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Recommendation" scheme="https://www.yam.gift/tags/Recommendation/"/>
    
      <category term="GBTD" scheme="https://www.yam.gift/tags/GBTD/"/>
    
      <category term="LR" scheme="https://www.yam.gift/tags/LR/"/>
    
  </entry>
  
  <entry>
    <title>Wide and Deep Learning for Recommender System 论文笔记</title>
    <link href="https://www.yam.gift/2020/10/27/Paper/2020-10-27-WideDeepLearning4RecSys/"/>
    <id>https://www.yam.gift/2020/10/27/Paper/2020-10-27-WideDeepLearning4RecSys/</id>
    <published>2020-10-27T15:00:00.000Z</published>
    <updated>2020-10-27T15:46:20.000Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;推荐系统可以看作是一个搜索排序系统，其中 input 是一组用户和上下文信息，output 是排好序的商品列表。推荐系统的一个挑战就是同时达到 memorization（记忆化）和 generalization（泛化）。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;memorization：&lt;ul&gt;
&lt;li&gt;大致可定义为学习特征或商品的频繁共现关系并探索相关性&lt;/li&gt;
&lt;li&gt;与用户已经执行操作的商品直接相关&lt;/li&gt;
&lt;li&gt;可以通过使用稀疏特征上的交叉乘积变换（cross-product transformation）有效地实现，如 &lt;code&gt;AND(installed_app=netfix, impression_app=pandora)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;generalization：&lt;ul&gt;
&lt;li&gt;基于相关性的传递性探索之前很少出现或没出现过的新特征组合&lt;/li&gt;
&lt;li&gt;倾向于提高推荐结果的多样性&lt;/li&gt;
&lt;li&gt;可以通过使用不太精细的特征增加泛化，如 &lt;code&gt;AND(installed_category=video, impression_category=music)&lt;/code&gt;，一般需要人工进行特征处理&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Recommendation" scheme="https://www.yam.gift/tags/Recommendation/"/>
    
      <category term="Wide" scheme="https://www.yam.gift/tags/Wide/"/>
    
      <category term="Deep" scheme="https://www.yam.gift/tags/Deep/"/>
    
  </entry>
  
</feed>
