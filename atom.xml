<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2025-02-27T15:18:06.510Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>hscspring</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LLM、强化、蒸馏讨论</title>
    <link href="https://www.yam.gift/2025/02/27/AI/2025-02-27-AI-Discussion/"/>
    <id>https://www.yam.gift/2025/02/27/AI/2025-02-27-AI-Discussion/</id>
    <published>2025-02-27T15:30:00.000Z</published>
    <updated>2025-02-27T15:18:06.510Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;2025年2月26日下午，Datawhale Paper群突然开启了一番关于AI相关的讨论，涉及成员主要包括：X、Y、D、CV、M、A和C。我觉得内容相当有意思，因此记录在案备查。&lt;/p&gt;
&lt;p&gt;其中对我个人印象比较深的几个观点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;X提出的新的大模型训练范式：预训练，long-cot, sft（long2short）。可以理解为先用大规模语料预训练学习知识，然后用少量SFT或RL（可以一起用）提升long-cot，然后再做SFT使其根据情况自动选择long或short。&lt;/li&gt;
&lt;li&gt;关于如何让模型自动选择思考长度（或不思考）的讨论，X认为主要靠强化，只是奖励这块需要涉及，就是是否需要思考，问题的难易，需要有个奖励来控制、设计。集成和自适应prm都是挺好的点，其实现在的重心就是什么样的奖励和怎么自动奖励。&lt;/li&gt;
&lt;li&gt;关于蒸馏分布的讨论。蒸馏之前做的不多，没想过这么细，不过如何桥接教师和学生的讨论确实有启发。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对讨论结果分别使用DeepSeek和DeepSeek-R1进行了整理，前者相对比较忠于讨论内容，后者则更加抽象有高度一些，各有优势。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Distillation" scheme="https://www.yam.gift/tags/Distillation/"/>
    
      <category term="RL" scheme="https://www.yam.gift/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>少量高质量数据SFT激活LLM推理能力</title>
    <link href="https://www.yam.gift/2025/02/18/NLP/LLM-Training/2025-02-18-LLM-PostTrain-LIMO-and-s1/"/>
    <id>https://www.yam.gift/2025/02/18/NLP/LLM-Training/2025-02-18-LLM-PostTrain-LIMO-and-s1/</id>
    <published>2025-02-17T23:00:00.000Z</published>
    <updated>2025-02-17T23:18:28.191Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文介绍两篇最新的用少量高质量数据SFT激活LLM推理能力的研究，分别是LIMO和s1。众所周知，一般说到SFT，尤其是参数比较大的模型，都是需要大量数据的；再加上推理任务本身又比较复杂，所需的数据可能更多。但这两篇文章的结论有点颠覆认知。这里的核心是：LLM本身需要具备如此能力，才有可能通过少量高质量数据SFT激活，否则可能难以见效。随着R1的出现，后训练算是彻底发生改变了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="Post-training" scheme="https://www.yam.gift/tags/Post-training/"/>
    
      <category term="LIMO" scheme="https://www.yam.gift/tags/LIMO/"/>
    
      <category term="s1" scheme="https://www.yam.gift/tags/s1/"/>
    
      <category term="Inference Scaling" scheme="https://www.yam.gift/tags/Inference-Scaling/"/>
    
  </entry>
  
  <entry>
    <title>DeepSeek R1深度技术解析及其影响</title>
    <link href="https://www.yam.gift/2025/02/17/NLP/LLM-Training/2025-02-17-DeepSeek-R1/"/>
    <id>https://www.yam.gift/2025/02/17/NLP/LLM-Training/2025-02-17-DeepSeek-R1/</id>
    <published>2025-02-17T00:00:00.000Z</published>
    <updated>2025-02-22T01:53:46.386Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文是2025年2月15日《2025 iFLYTEK 开发者TALK 杭州站《DeepSeek深度技术解析》分享的文字版，PPT可以在&lt;a href=&quot;https://github.com/datawhalechina/hugging-llm/tree/main/resources&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;找到。由于时间关系，实际分享是本文的简化版。文字内容是近半个月陆陆续续记录的一些阅读笔记和思考，中途接到分享邀请（还好有点积累，不然怕是难顶doge），成稿于分享后。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;距离2022年底ChatGPT发布开启LLM时代才过去两年多一点时间，刚进入2025年，DeepSeek-R1就将LLM真正推向了深度思考时代。&lt;/p&gt;
&lt;p&gt;两年多的高速发展，前所未有的按周迭代，如今想来都一阵恍惚。2023年是LLM最快速发展的一年，被称为LLM元年，新的开发范式出现（感兴趣的读者可以关注&lt;a href=&quot;https://github.com/datawhalechina/hugging-llm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HuggingLLM&lt;/a&gt;），全民AI浪潮涌现。2024年，基于LLM的应用已经开始成熟，Agent百花齐放，进入元年，各种应用层出不穷，一个人公司成为可能。&lt;/p&gt;
&lt;p&gt;当我们以为LLM基本就这样按部就班向”应用“时，R1出现了，它发迹于OpenAI-o1，但超越了o1。关于o1，我的观点和OpenAI前首席研究官Bob的观点一致：它的目标是解决复杂问题，大多数人日常工作中并不会遇到需要o1的需求（可以参考&lt;a href=&quot;https://yam.gift/2024/12/20/NLP/2024-12-20-Think-About-AI-and-Related/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;关于AI前沿的思考&lt;/a&gt;）。但是R1提升了LLM的整体能力，让模型真正在推理时进行自我反思和验证，这当然适用于复杂问题，但日常工作很多场景也能受益，AI更加像人。我觉得这是R1对整个行业的贡献，其作用不亚于ChatGPT的发布。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="Post-training" scheme="https://www.yam.gift/tags/Post-training/"/>
    
      <category term="R1" scheme="https://www.yam.gift/tags/R1/"/>
    
      <category term="R1-Zero" scheme="https://www.yam.gift/tags/R1-Zero/"/>
    
      <category term="oat-zero" scheme="https://www.yam.gift/tags/oat-zero/"/>
    
      <category term="DeepScaleR" scheme="https://www.yam.gift/tags/DeepScaleR/"/>
    
      <category term="LIMO" scheme="https://www.yam.gift/tags/LIMO/"/>
    
      <category term="s1" scheme="https://www.yam.gift/tags/s1/"/>
    
      <category term="Inference Scaling" scheme="https://www.yam.gift/tags/Inference-Scaling/"/>
    
  </entry>
  
  <entry>
    <title>我为什么做开源？</title>
    <link href="https://www.yam.gift/2025/01/12/Diary/2025-01-12-Why-OpenSource/"/>
    <id>https://www.yam.gift/2025/01/12/Diary/2025-01-12-Why-OpenSource/</id>
    <published>2025-01-12T04:00:00.000Z</published>
    <updated>2025-01-12T04:16:06.193Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;开源到书籍&quot;&gt;开源到书籍&lt;/h2&gt;
&lt;p&gt;从《ChatGPT原理与应用开发》这本书开始吧，它获得了异步2024年影响力图书。这本身是一个开源项目&lt;a href=&quot;https://github.com/datawhalechina/hugging-llm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HuggingLLM&lt;/a&gt;，当时（23年4月）的初衷很简单，就是想帮助更多的中小企业使用AI，让非算法的工程师也能借助AI实现算法相关功能和服务。另外，本书另一位作者玉琳说网上太多智商税的课程了，觉得我们应该做点什么，于是一拍即合就有了这个项目。但具体开始做的时候，我觉得还是应该有一些创新，并且内容的生命力尽量持久些。思来想去，再结合市场调研结果，决定以NLP算法常见任务为导向，通过借助LLM让普通程序员也能做NLP算法工程师的工作。同时内容尽量保持实战性，代码可直接复用到工作环境。这是从NLP范式角度展开的一本书，是最大的创新点，同时范式是不容易改变的，也保证了书籍的生命力。&lt;/p&gt;
&lt;p&gt;书籍出版后，其实还是有点担心的，我当时对这本书的评价是有一定价值，但整体质量其实一般。不过有一点我觉得是好的，就是到现在为止书的框架依然是正确的，且目测会在很长一段时间内仍然有效。后来微信读书评价果然还可以，有评价说看得出作者在NLP领域浸淫多年，这是不错的，有些东西光眼看不经历实际项目是没用的。其实我当时还看了微信读书的基本同类型书，有些书质量不错，但也有些拼凑感很重，都是网上到处整理的资料，果然，这些书的评论里就有不少人提到了，看来读者的眼睛是雪亮的。说回本书，其实我觉得整体还是太粗糙了，毕竟时间点紧，没太多时间打磨，内容呢也比较简单，是真的非常简单，毕竟是给非行业人士看的。我都跟业内人说你们别看，太简单了，不要浪费时间，当然更不要买，网上都有全书电子稿。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Growth" scheme="https://www.yam.gift/tags/Growth/"/>
    
      <category term="Dream" scheme="https://www.yam.gift/tags/Dream/"/>
    
      <category term="OpenSource" scheme="https://www.yam.gift/tags/OpenSource/"/>
    
  </entry>
  
  <entry>
    <title>实时语音交互场景下RAG的机遇和挑战</title>
    <link href="https://www.yam.gift/2025/01/05/MM/2025-01-05-RAG-and-Voice-Agent/"/>
    <id>https://www.yam.gift/2025/01/05/MM/2025-01-05-RAG-and-Voice-Agent/</id>
    <published>2025-01-05T15:00:00.000Z</published>
    <updated>2025-01-05T16:50:40.153Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;这是2025年1月4日笔者受邀参加Zilliz举办的【向心力】系列会议《中美AI应用与落地分享》专场中的演讲，特此记录。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本文与演讲不完全相同，但核心内容一致。其中涉及到的内容还比较新，观点不一定准确，供参考交流。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这次分享的题目是《实时语音交互场景下RAG的机遇和挑战》，内容主要包括四个方面：主题引入、实时语音交互与RAG的结合、面临的技术挑战和未来的机遇与发展方向。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="RAG" scheme="https://www.yam.gift/tags/RAG/"/>
    
      <category term="VoiceAgent" scheme="https://www.yam.gift/tags/VoiceAgent/"/>
    
  </entry>
  
  <entry>
    <title>预训练（0）：无处安放的躁动之心</title>
    <link href="https://www.yam.gift/2025/01/05/NLP/LLM-Training/2025-01-05-LLM-Pretrain-0PreStart/"/>
    <id>https://www.yam.gift/2025/01/05/NLP/LLM-Training/2025-01-05-LLM-Pretrain-0PreStart/</id>
    <published>2025-01-05T15:00:00.000Z</published>
    <updated>2025-01-07T00:32:22.253Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;这个系列打算开始做一个预训练小模型，Size暂定在1.5B。这个念头源于和几个朋友的一次聚餐，当时聊到了Scale Law，以及小模型，有两个观点促使了笔者做这个决定。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;小模型，在智能和一些大模型相媲美的时候是有意义的。&lt;/li&gt;
&lt;li&gt;Scale Law不光表现在模型层面，也表现在数据层面。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
  </entry>
  
  <entry>
    <title>LLM指令跟随论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Instruction-Following-Papers/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/LLM/2024-12-31-Instruction-Following-Papers/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:12:08.663Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;LLM指令跟随相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Instruction Following" scheme="https://www.yam.gift/tags/Instruction-Following/"/>
    
  </entry>
  
  <entry>
    <title>OMNI论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-OMNI-Papers/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-OMNI-Papers/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:15:48.535Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;OMNI相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="OMNI" scheme="https://www.yam.gift/tags/OMNI/"/>
    
  </entry>
  
  <entry>
    <title>SLM论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-SLM-Papers/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/MM/2024-12-31-SLM-Papers/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:21:25.869Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;SLM相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="SLM" scheme="https://www.yam.gift/tags/SLM/"/>
    
      <category term="MM Fusion" scheme="https://www.yam.gift/tags/MM-Fusion/"/>
    
  </entry>
  
  <entry>
    <title>音频Codec论文速览</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-Codec-Papers/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-Codec-Papers/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:21:57.758Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Codec相关论文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
  </entry>
  
  <entry>
    <title>VITS</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-VITS/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-VITS/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-13T08:50:35.814Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2106.06103&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/a&gt;&lt;br&gt;
代码：&lt;a href=&quot;https://github.com/jaywalnut310/vits&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;jaywalnut310/vits: VITS: Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一个并行的端到端TTS模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="VITS" scheme="https://www.yam.gift/tags/VITS/"/>
    
  </entry>
  
  <entry>
    <title>XTTS</title>
    <link href="https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-XTTS/"/>
    <id>https://www.yam.gift/2024/12/31/Paper/TTS/2024-12-31-XTTS/</id>
    <published>2024-12-31T15:00:00.000Z</published>
    <updated>2025-01-01T00:27:01.808Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2406.04904&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model&lt;/a&gt;&lt;br&gt;
代码：&lt;a href=&quot;https://github.com/coqui-ai/TTS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;coqui-ai/TTS: 🐸💬 - a deep learning toolkit for Text-to-Speech, battle-tested in research and production&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;基于Tortoise的改进，自回归。本文主要关心架构。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="XTTS" scheme="https://www.yam.gift/tags/XTTS/"/>
    
  </entry>
  
  <entry>
    <title>DAC</title>
    <link href="https://www.yam.gift/2024/12/30/Paper/TTS/2024-12-30-DAC/"/>
    <id>https://www.yam.gift/2024/12/30/Paper/TTS/2024-12-30-DAC/</id>
    <published>2024-12-30T15:00:00.000Z</published>
    <updated>2025-01-01T00:25:58.731Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2306.06546&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;High-Fidelity Audio Compression with Improved RVQGAN&lt;/a&gt;&lt;br&gt;
代码：&lt;a href=&quot;https://github.com/descriptinc/descript-audio-codec&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;descriptinc/descript-audio-codec: State-of-the-art audio codec with 90x compression factor. Supports 44.1kHz, 24kHz, and 16kHz mono/stereo audio.&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="DAC" scheme="https://www.yam.gift/tags/DAC/"/>
    
  </entry>
  
  <entry>
    <title>TS3-Codec</title>
    <link href="https://www.yam.gift/2024/12/27/Paper/TTS/2024-12-27-TS3-Codec/"/>
    <id>https://www.yam.gift/2024/12/27/Paper/TTS/2024-12-27-TS3-Codec/</id>
    <published>2024-12-27T15:00:00.000Z</published>
    <updated>2025-01-01T00:24:54.810Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2411.18803&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TS3-Codec: Transformer-Based Simple Streaming Single Codec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;没开源代码。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="TS3-Codec" scheme="https://www.yam.gift/tags/TS3-Codec/"/>
    
  </entry>
  
  <entry>
    <title>BigCodec</title>
    <link href="https://www.yam.gift/2024/12/26/Paper/TTS/2024-12-26-BigCodec/"/>
    <id>https://www.yam.gift/2024/12/26/Paper/TTS/2024-12-26-BigCodec/</id>
    <published>2024-12-26T15:00:00.000Z</published>
    <updated>2025-01-01T00:25:03.980Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://arxiv.org/abs/2409.05377&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码：&lt;a href=&quot;https://github.com/Aria-K-Alethia/BigCodec&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Aria-K-Alethia/BigCodec&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="TTS" scheme="https://www.yam.gift/tags/TTS/"/>
    
      <category term="Codec" scheme="https://www.yam.gift/tags/Codec/"/>
    
      <category term="BigCodec" scheme="https://www.yam.gift/tags/BigCodec/"/>
    
  </entry>
  
  <entry>
    <title>关于AI前沿的思考</title>
    <link href="https://www.yam.gift/2024/12/20/NLP/2024-12-20-Think-About-AI-and-Related/"/>
    <id>https://www.yam.gift/2024/12/20/NLP/2024-12-20-Think-About-AI-and-Related/</id>
    <published>2024-12-19T17:30:00.000Z</published>
    <updated>2025-01-01T01:29:04.866Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;上一次写关于类似（大语言模型）的&lt;a href=&quot;https://yam.gift/2023/10/15/NLP/2023-10-15-Think-About-LLM/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;思考&lt;/a&gt;是在去年10月份了，主要是关于LLM机理、预训练、微调等算法层面的思考。不过后面也提到了未来的方向，以及行业的思考。到今天看那些内容依然实用，而且有种“预判”逐渐成真的感觉。其实我个人很不喜欢预言、预测或诸如此类的事物，但当我们对一个行业了解的足够多、足够深时，很多时候对一些方向性问题的判断就会比较准确。&lt;/p&gt;
&lt;p&gt;言归正传，今天正好看到了OpenAI前首席研究官Bob McGrew采访（&lt;a href=&quot;https://mp.weixin.qq.com/s/t42LuAP_O8WnjkXOwHLhXQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;中文版&lt;/a&gt;），又有了新的想法，正好也谈一谈最近的一些思考。主要围绕着采访中的主题，谈谈自己的看法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Multi-Modal" scheme="https://www.yam.gift/tags/Multi-Modal/"/>
    
      <category term="Embodied AI" scheme="https://www.yam.gift/tags/Embodied-AI/"/>
    
  </entry>
  
  <entry>
    <title>《真希望父母读过这本书》读书笔记</title>
    <link href="https://www.yam.gift/2024/09/30/BabyGrow/2024-09-30-Hope-Parents-Read-the-Book/"/>
    <id>https://www.yam.gift/2024/09/30/BabyGrow/2024-09-30-Hope-Parents-Read-the-Book/</id>
    <published>2024-09-30T14:00:00.000Z</published>
    <updated>2024-10-08T05:01:50.599Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;周六下午和爱人一起去了天目里，散步、聊天、看风景、喝茉酸奶、吃烧鸟、去鸟屋书店读书、去饸饹面馆吃面。这是我们第二次去这里了，主要是那家面馆的面很符合我们口味，但也不能打车过去就吃个面，所以每次都要看一两个小时书。这本书就是这天我读的两本书之一，还剩最后一章没读完，我觉得问题不大，得赶紧把读过的记下来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Life" scheme="https://www.yam.gift/tags/Life/"/>
    
      <category term="BabyGrow" scheme="https://www.yam.gift/tags/BabyGrow/"/>
    
  </entry>
  
  <entry>
    <title>基础和取舍</title>
    <link href="https://www.yam.gift/2024/09/30/Diary/2024-09-30-Work-Design/"/>
    <id>https://www.yam.gift/2024/09/30/Diary/2024-09-30-Work-Design/</id>
    <published>2024-09-30T00:00:00.000Z</published>
    <updated>2024-09-30T11:28:04.257Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;没想到居然一年多没写这样思考性的文字了，ChatGPT后遗症有点大。去年底换工作再加上孩子出生，生活一下子变得异常充实了起来。家庭和育儿方面成长很多，从一开始的没耐心，到逐步理解包容、感同身受（无论对爱人还是孩子），一年不到时间改变了非常多。工作方面也取得了一些成果，强度和深度比此前所有工作都高了一个级别，虽然很忙，但非常开心。比较不满意的是过于忙碌导致没时间夯实基础，总感觉自己比较浮。正好国庆假期，重新整理一下思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Life" scheme="https://www.yam.gift/tags/Life/"/>
    
      <category term="Growth" scheme="https://www.yam.gift/tags/Growth/"/>
    
      <category term="Dream" scheme="https://www.yam.gift/tags/Dream/"/>
    
  </entry>
  
  <entry>
    <title>MIO</title>
    <link href="https://www.yam.gift/2024/09/28/Paper/2024-09-28-MIO/"/>
    <id>https://www.yam.gift/2024/09/28/Paper/2024-09-28-MIO/</id>
    <published>2024-09-28T15:00:00.000Z</published>
    <updated>2024-10-09T15:39:23.866Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/2409.17692&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/abs/2409.17692&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心：多模态输入输出，这里的多模态包括了文本、图像、语音和视频，相比AnyGPT多了视频。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://qnimg.lovevivian.cn/paper-mio-1.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="MIO" scheme="https://www.yam.gift/tags/MIO/"/>
    
      <category term="MultiModal" scheme="https://www.yam.gift/tags/MultiModal/"/>
    
  </entry>
  
  <entry>
    <title>Tiny LLM Continual Pre-training：RHO-1</title>
    <link href="https://www.yam.gift/2024/04/13/NLP/LLM-Training/2024-04-13-LLM-Tiny-Continual-Training-RHO-1/"/>
    <id>https://www.yam.gift/2024/04/13/NLP/LLM-Training/2024-04-13-LLM-Tiny-Continual-Training-RHO-1/</id>
    <published>2024-04-13T15:00:00.000Z</published>
    <updated>2024-06-12T08:30:34.563Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;并不是所有的Next Token都有用，&lt;a href=&quot;http://arxiv.org/abs/2404.07965&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RHO-1&lt;/a&gt;选择那些最有用的Next Token，提升了小模型的继续训练效率。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="RHO-1" scheme="https://www.yam.gift/tags/RHO-1/"/>
    
      <category term="RHO" scheme="https://www.yam.gift/tags/RHO/"/>
    
  </entry>
  
</feed>
