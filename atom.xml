<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2024-06-12T08:30:34.563Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>hscspring</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Tiny LLM Continual Pre-training：RHO-1</title>
    <link href="https://www.yam.gift/2024/04/13/NLP/LLM-Training/2024-04-13-LLM-Tiny-Continual-Training-RHO-1/"/>
    <id>https://www.yam.gift/2024/04/13/NLP/LLM-Training/2024-04-13-LLM-Tiny-Continual-Training-RHO-1/</id>
    <published>2024-04-13T15:00:00.000Z</published>
    <updated>2024-06-12T08:30:34.563Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;并不是所有的Next Token都有用，&lt;a href=&quot;http://arxiv.org/abs/2404.07965&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RHO-1&lt;/a&gt;选择那些最有用的Next Token，提升了小模型的继续训练效率。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
      <category term="RHO-1" scheme="https://www.yam.gift/tags/RHO-1/"/>
    
      <category term="RHO" scheme="https://www.yam.gift/tags/RHO/"/>
    
  </entry>
  
  <entry>
    <title>LLM打街霸</title>
    <link href="https://www.yam.gift/2024/04/08/NLP/2024-04-08-LLM-Colosseum/"/>
    <id>https://www.yam.gift/2024/04/08/NLP/2024-04-08-LLM-Colosseum/</id>
    <published>2024-04-08T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;国外的一个项目，看了一下比较简单，于是也拿过来玩儿一下。由于原项目没支持中文，就简单支持了一下，顺便简单地重构了一下代码。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;代码地址（Fork）：&lt;a href=&quot;https://github.com/hscspring/llm-colosseum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/hscspring/llm-colosseum&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;项目原地址：&lt;a href=&quot;https://github.com/OpenGenerativeAI/llm-colosseum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/OpenGenerativeAI/llm-colosseum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://qnimg.lovevivian.cn/blog-llm-colosseum-1.jpg&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="LLM-Colosseum" scheme="https://www.yam.gift/tags/LLM-Colosseum/"/>
    
  </entry>
  
  <entry>
    <title>LLM中的演绎推理、归纳推理和溯因推理</title>
    <link href="https://www.yam.gift/2024/04/06/NLP/2024-04-06-Deductive-Inductive-Abductive/"/>
    <id>https://www.yam.gift/2024/04/06/NLP/2024-04-06-Deductive-Inductive-Abductive/</id>
    <published>2024-04-06T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;一篇简单探索少样本上下文学习和指令推理的文章：&lt;a href=&quot;https://arxiv.org/abs/2404.03028&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2404.03028] An Incomplete Loop: Deductive, Inductive, and Abductive Learning in Large Language Models&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Instruction Following" scheme="https://www.yam.gift/tags/Instruction-Following/"/>
    
      <category term="Few-shot Prompting" scheme="https://www.yam.gift/tags/Few-shot-Prompting/"/>
    
      <category term="Instruction Inference" scheme="https://www.yam.gift/tags/Instruction-Inference/"/>
    
  </entry>
  
  <entry>
    <title>LLM极简科普</title>
    <link href="https://www.yam.gift/2024/03/16/AI/2024-03-16-LLM-Basic/"/>
    <id>https://www.yam.gift/2024/03/16/AI/2024-03-16-LLM-Basic/</id>
    <published>2024-03-16T15:30:00.000Z</published>
    <updated>2024-06-12T02:06:43.527Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;编者按：本文内容来自Datawhale《AI第一课》，目标是向普通大众传播AI相关知识。本文是第一稿，太过于偏技术，因此需要重新修改打磨。不过从有编程背景读者的角度看我觉得内容尚可，特记录在此。同时也是便于后面对比最终内容和最初内容的差别，提升自己科普内容创作方面的技巧。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;本节主要介绍LLM（Large Language Model，大语言模型）的基础科普。大纲如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算机如何识别文字：Token化+词嵌入（Tokenize+Embedding）&lt;/li&gt;
&lt;li&gt;大模型如何学习（训练）：下个词预测（Next Token Prediction，NTP）&lt;/li&gt;
&lt;li&gt;大模型如何理解文本：多层多头注意力（Multi-Layer+Multi-Head Self-Attention，MHA）&lt;/li&gt;
&lt;li&gt;大模型如何处理任务：上下文学习或情境学习（In-Context Learning）&lt;/li&gt;
&lt;li&gt;大模型如何生成回复：推理（Inference）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文涉及到上面提到的重要概念时，会以中文表述，括号内的是对应的英文表达。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="AIGC" scheme="https://www.yam.gift/tags/AIGC/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Programming Language Environment Cheat Sheet</title>
    <link href="https://www.yam.gift/2024/03/06/Unix/2024-03-06-LanguageEnvCheatSheet/"/>
    <id>https://www.yam.gift/2024/03/06/Unix/2024-03-06-LanguageEnvCheatSheet/</id>
    <published>2024-03-06T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.553Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;编程语言环境相关备忘（我只想复制粘贴）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Rust" scheme="https://www.yam.gift/tags/Rust/"/>
    
      <category term="NodeJS" scheme="https://www.yam.gift/tags/NodeJS/"/>
    
  </entry>
  
  <entry>
    <title>LLM Tiny Pretrain：H2O-Danube and Stable LM</title>
    <link href="https://www.yam.gift/2024/02/03/NLP/LLM-Training/2024-02-03-LLM-Tiny-Pretrain/"/>
    <id>https://www.yam.gift/2024/02/03/NLP/LLM-Training/2024-02-03-LLM-Tiny-Pretrain/</id>
    <published>2024-02-03T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.539Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://huggingface.co/collections/stabilityai/stable-lm-650852cfd55dd4e15cdcb30a&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;StableLM&lt;/a&gt; 一直致力于小模型（从7B、3B 到 1.6B），不过 License 商用有些限制，&lt;a href=&quot;http://arxiv.org/abs/2401.16818&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;H2O-Danube&lt;/a&gt; 是 Apache2.0 的小模型（1.8B），整体指标略逊于 StableLM。本文通过这两篇 Paper，记录小模型的预训练。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="H2O-Danube" scheme="https://www.yam.gift/tags/H2O-Danube/"/>
    
      <category term="Stable LM" scheme="https://www.yam.gift/tags/Stable-LM/"/>
    
  </entry>
  
  <entry>
    <title>LLM DataManagement：Weaver</title>
    <link href="https://www.yam.gift/2024/02/01/NLP/LLM-DM/2024-02-01-LLM-DataManagement-Weaver/"/>
    <id>https://www.yam.gift/2024/02/01/NLP/LLM-DM/2024-02-01-LLM-DataManagement-Weaver/</id>
    <published>2024-02-01T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文记录 &lt;a href=&quot;http://arxiv.org/abs/2401.17268&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Weaver&lt;/a&gt; 的数据处理。&lt;/p&gt;
&lt;p&gt;Weaver是一个垂直领域（文字创作）的LLM，做的是继续训练，训练上循规蹈矩，没有什么好说的。稍微有一点点特色的是数据这块，对垂直领域可能有一定借鉴意义。&lt;/p&gt;
&lt;p&gt;另外有提出一个Constitutional DPO的东西，其实就是利用专家写的规则（原则）合成违反这些规则的负样本。相较而言，遵循这些规则的就是正样本。这其实和数据有点关系，垂直领域往往有不少正样本（比如文字创作领域大家的小说、散文等），但负样本却不好找，所以就违反”好“的规则生成负样本。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="DataManagement" scheme="https://www.yam.gift/tags/DataManagement/"/>
    
      <category term="Continual Pretraining" scheme="https://www.yam.gift/tags/Continual-Pretraining/"/>
    
  </entry>
  
  <entry>
    <title>LLM DataManagement：Ziya2</title>
    <link href="https://www.yam.gift/2024/01/29/NLP/LLM-DM/2024-01-29-LLM-DataManagement-Ziya2/"/>
    <id>https://www.yam.gift/2024/01/29/NLP/LLM-DM/2024-01-29-LLM-DataManagement-Ziya2/</id>
    <published>2024-01-29T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;高质量的数据对LLM的重要性无需赘述，本文记录&lt;a href=&quot;http://arxiv.org/abs/2311.03301&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ziya2&lt;/a&gt;的数据管理。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="DataManagement" scheme="https://www.yam.gift/tags/DataManagement/"/>
    
      <category term="Ziya" scheme="https://www.yam.gift/tags/Ziya/"/>
    
      <category term="Continual Pretraining" scheme="https://www.yam.gift/tags/Continual-Pretraining/"/>
    
  </entry>
  
  <entry>
    <title>LLM Continual Pre-training：Ziya2</title>
    <link href="https://www.yam.gift/2024/01/23/NLP/LLM-Training/2024-01-23-LLM-Continual-Training-Ziya2/"/>
    <id>https://www.yam.gift/2024/01/23/NLP/LLM-Training/2024-01-23-LLM-Continual-Training-Ziya2/</id>
    <published>2024-01-23T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.539Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;LLM的训练策略非常讲究，本文主要记录&lt;a href=&quot;http://arxiv.org/abs/2311.03301&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ziya2&lt;/a&gt;的继续训练策略。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Ziya" scheme="https://www.yam.gift/tags/Ziya/"/>
    
      <category term="Continual Pre-training" scheme="https://www.yam.gift/tags/Continual-Pre-training/"/>
    
  </entry>
  
  <entry>
    <title>【Rust与AI】LLM模型基本架构</title>
    <link href="https://www.yam.gift/2023/12/24/Rust/RustAI/2023-12-24-Rust-and-AI-LLM/"/>
    <id>https://www.yam.gift/2023/12/24/Rust/RustAI/2023-12-24-Rust-and-AI-LLM/</id>
    <published>2023-12-24T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.552Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本篇是《Rust与AI》系列的第二篇，上一篇我们主要介绍了本系列的概览和方向，定下了一个基调。本篇我们将介绍LLM的基本架构，我们会以迄今为止使用最广泛的开源模型LLaMA为例展开介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Rust" scheme="https://www.yam.gift/tags/Rust/"/>
    
      <category term="Decoding" scheme="https://www.yam.gift/tags/Decoding/"/>
    
      <category term="Llama" scheme="https://www.yam.gift/tags/Llama/"/>
    
  </entry>
  
  <entry>
    <title>【Rust与AI】概览和方向</title>
    <link href="https://www.yam.gift/2023/12/03/Rust/RustAI/2023-12-03-Rust-and-AI-Introduction/"/>
    <id>https://www.yam.gift/2023/12/03/Rust/RustAI/2023-12-03-Rust-and-AI-Introduction/</id>
    <published>2023-12-03T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.552Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本系列主要介绍Rust与AI的那些天作之合（开源项目），我们会以开源项目代码阅读的方式推进，以Rust为主，同时科普AI相关知识，目的是让更多非算法、非Rust的程序员进一步学习Rust和AI相关知识。当然，很显然地，我们也希望Rust程序员和AI算法工程师能从中有所收获。前者可以关注AI算法的设计和优化，后者可以关注Rust如何助力AI算法。&lt;/p&gt;
&lt;p&gt;本篇是系列第一篇，主要介绍Rust和AI各自的特点与发展近况，以及它俩的遇见会碰撞出怎样的火花。我们热爱AI，我们喜欢Rust语言，仅此而已。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="Rust" scheme="https://www.yam.gift/tags/Rust/"/>
    
  </entry>
  
  <entry>
    <title>OpenAIGC大赛小结</title>
    <link href="https://www.yam.gift/2023/11/04/AI/2023-11-04-OpenAIGC/"/>
    <id>https://www.yam.gift/2023/11/04/AI/2023-11-04-OpenAIGC/</id>
    <published>2023-11-04T15:30:00.000Z</published>
    <updated>2024-06-12T02:06:43.527Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;很荣幸周末参加了第一届OpenAIGC开发者大赛，并担任大赛评委之一。期间收获良多，感慨万千，特记录如下。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="AIGC" scheme="https://www.yam.gift/tags/AIGC/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>关于大语言模型的思考</title>
    <link href="https://www.yam.gift/2023/10/15/NLP/2023-10-15-Think-About-LLM/"/>
    <id>https://www.yam.gift/2023/10/15/NLP/2023-10-15-Think-About-LLM/</id>
    <published>2023-10-15T03:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;从ChatGPT去年11月底发布到现在差不多一年时间了，短短的一年，整个NLP行业发生了翻天覆地的变化。应用方面，整个AI行业甚至其他行业都受到很大冲击，感觉所有人都在+大模型，都在试图重构产品和服务；研究方面，LLM现在几乎成为所有从业人员研究的热点，各种各样的研究成果层出不穷，让人眼花缭乱，直呼看不过来。&lt;/p&gt;
&lt;p&gt;本人作为一名NLP工程师，自然深度参与。从一开始的Prompt技巧，到InstructGPT三阶段训练研究，再到千奇百怪的高效微调、知识编辑，再到各种量化推理、剪枝、小模型实践，再到目前重新思考预训练。这是一个不断深入的过程，也是一个不断学习的过程。从一开始的“我草牛逼”，到“看起来好像不复杂”，再到“咋回事，咋做的，咋这么多坑，咋办”。&lt;/p&gt;
&lt;p&gt;本文主要记录一点当下最新的思考，包括算法和行业两个方面。我会尽量让自己的观点鲜明，不模棱两可。另外，我们也不是搞预测，只是纯粹的分析和感悟，甚至有一些个人偏好。总的来说，都是个人观点，限于能力，不一定准确（很有可能有错误），希望能借此和同好一起讨论。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 开发指南：Hugging LLM Hugging Future</title>
    <link href="https://www.yam.gift/2023/04/22/NLP/2023-04-22-ChatGPT-Development/"/>
    <id>https://www.yam.gift/2023/04/22/NLP/2023-04-22-ChatGPT-Development/</id>
    <published>2023-04-22T04:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.538Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文属于DataWhale &lt;a href=&quot;https://github.com/datawhalechina/hugging-llm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hugging-LLM&lt;/a&gt;开源教程介绍内容，详细教程请跳转链接。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;随着ChatGPT的爆火，我们相信未来会有越来越多的大模型及类似OpenAI提供的服务出现，AI 正在逐渐平民化，将来每个人都可以利用大模型轻松地做出自己的AI产品。&lt;/p&gt;
&lt;p&gt;HuggingLLM是一个面向非算法、有一定编程基础、对AI和ChatGPT（或类似模型）感兴趣的，基于ChatGPT API开发相关应用的开源项目。当然部分内容不需要任何编程经验也可以学习，算法工程师也可能从中受益。项目主要包括ChatGPT基础科普、ChatGPT实现各种NLP常见任务（相似匹配、句词分类、编辑生成、推理等大类）、ChatGPT局限和商业应用等内容。&lt;/p&gt;
&lt;p&gt;项目名为 HuggingLLM，因为我们相信正在经历一个伟大的时代，我们相信这是一个值得每个人全身心拥抱的时代，我们更加相信这个世界必将会因此而变得更加美好。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="LM" scheme="https://www.yam.gift/tags/LM/"/>
    
      <category term="HuggingLLM" scheme="https://www.yam.gift/tags/HuggingLLM/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 基础科普：知其一点所以然</title>
    <link href="https://www.yam.gift/2023/04/15/NLP/2023-04-15-ChatGPT-Introduction/"/>
    <id>https://www.yam.gift/2023/04/15/NLP/2023-04-15-ChatGPT-Introduction/</id>
    <published>2023-04-15T10:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.537Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文属于&lt;a href=&quot;https://datawhale.club/#/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DataWhale&lt;/a&gt;开源组织&lt;a href=&quot;https://github.com/datawhalechina/hugging-llm.git&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HuggingLLM&lt;/a&gt;开源项目内容，更多内容请移步开源项目。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;2022年底，ChatGPT突然间在AI圈开始流行，准确来说是搞自然语言处理（Natural Language Processing，NLP）圈子里先火起来了。遥想当时，本以为就会在圈内火一阵，结果现在……没想到居然成了AI的救命稻草，当然对AI工程师尤其是NLP工程师是什么就不好说了。海内外沸腾之后就是第一时间的跟进，结果自然是努力对齐而无功，连牛逼的Google和FaceBook都翻车了。不过总归是折腾出来一些东西，大伙儿也都有了目标，圈子又有了新活力。希望OpenAI继续发力，我等再多苟些日子。&lt;/p&gt;
&lt;p&gt;无论是ChatGPT还是后来的模仿者，它们其实都是语言模型，准确来说——大语言模型。使用时，无论是调用API还是开源项目，总有一些参数可能需要调整。对大部分内行人士来说应该都不成问题，但对外行就有点玄乎了。基于此，本文将简要介绍ChatGPT相关技术基本原理，行文将站在外行人角度，尝试将内容尽量平民化。虽然不能深入细节，但知晓原理足以很好使用了。&lt;/p&gt;
&lt;p&gt;本文共分为四个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LM：这是ChatGPT的基石的基石，是一个最基本的概念，绕不开，逃不过，没办法。&lt;/li&gt;
&lt;li&gt;Transformer：这是ChatGPT的基石，准确来说它的一部分是基石。&lt;/li&gt;
&lt;li&gt;GPT：本体，从GPT-1，一直到现在的GPT-4，按openai自己的说法，那模型都是那个模型，只是它长大了，变胖了，不过更好看了。关于这点，大家基本都没想到。现在好了，攀不上了。&lt;/li&gt;
&lt;li&gt;RLHF：ChatGPT神兵利器，有此利刃，ChatGPT才是那个ChatGPT，不然就只能是GPT-3。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="InstructGPT" scheme="https://www.yam.gift/tags/InstructGPT/"/>
    
      <category term="LM" scheme="https://www.yam.gift/tags/LM/"/>
    
      <category term="GPT-3" scheme="https://www.yam.gift/tags/GPT-3/"/>
    
      <category term="GPT-2" scheme="https://www.yam.gift/tags/GPT-2/"/>
    
      <category term="GPT-1" scheme="https://www.yam.gift/tags/GPT-1/"/>
    
      <category term="RLHF" scheme="https://www.yam.gift/tags/RLHF/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>「+AI」需要什么？</title>
    <link href="https://www.yam.gift/2023/02/27/AI/2023-02-27-Enpower-AI/"/>
    <id>https://www.yam.gift/2023/02/27/AI/2023-02-27-Enpower-AI/</id>
    <published>2023-02-27T15:07:19.000Z</published>
    <updated>2024-06-12T02:06:43.527Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;随着互联网业务到达天花板，与其相关的 AI 算法也开始逐渐变得寒气逼人。说到底，AI 还是个工具，即便它是非常了不得的工具，但毕竟大部分时候也没法脱离业务存在。二十大报告也提到要「脱虚向实」，进一步壮大、升级实体经济。所以，现在社会以及更多的人开始思考如何利用 AI 为行业赋能。换句通俗的话说，卷死互联网，现在来卷其他行业了。虽说几乎任何行业都可以通过「+AI」受益，但这中间有些业务和公司可能比较特殊，不太适合或无法 +AI。所以，本文就简单探讨下要想 +AI 究竟需要什么条件。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="Business" scheme="https://www.yam.gift/tags/Business/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 影响冲击：职业、行业与产业</title>
    <link href="https://www.yam.gift/2023/02/21/NLP/2023-02-21-ChatGPT-Impact/"/>
    <id>https://www.yam.gift/2023/02/21/NLP/2023-02-21-ChatGPT-Impact/</id>
    <published>2023-02-21T15:30:00.000Z</published>
    <updated>2024-06-12T02:06:43.537Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;2022 年末的时候，ChatGPT 横空出世，朋友第一时间关注试玩后与我分享。当时听他说效果很好，不过我并没有特别放心上，毕竟，对话机器人已经不知道来过多少轮了，都快麻了。直到过了几天他给我看了一个非常亮眼的 Case——把我们平时工作中的业务文本直接丢给 ChatGPT，让它做实体抽取任务。结果完成的非常好，甚至可以按照指定的格式（如 Json）输出，而且如果你再告诉它一些特有规则，它还能进一步提取。这就很厉害了，至少之前的对话机器人可做不到这点。于是赶紧关注起来，先看介绍——哇靠，居然有强化学习（个人兴趣，一直比较关注强化学习在 NLP 方面的应用【相关文献1和2】），顿时来了兴趣——再看，发现 InstructGPT 这篇 Paper 在 11 月已经读过了，顿时恍然——原来是这篇。然后就上淘宝买了个账号开始玩起来，玩着玩着就感觉到这东西对 NLP 这个职业的冲击，但当时并没有想到它能出圈，能影响整个行业甚至产业。&lt;/p&gt;
&lt;p&gt;过年的时候，在用它写春节祝福时发现 Prompt 技能不够用了，搜了一下才发现是自己狭隘了，于是赶紧补充了一波，写下了这篇 Prompt 工程：&lt;a href=&quot;https://yam.gift/2023/01/25/NLP/2023-01-25-ChatGPT-Prompt-Engineering/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ChatGPT Prompt 工程：设计、实践与思考 | Yam&lt;/a&gt;。然后，我感觉好像应该差不多了吧，没想到，一切才刚刚开始……现在，大家都知道了……在 Prompt 工程中，我在文末写到：“本想继续谈谈关于 ChatGPT 对 NLP 行业甚至 AI 领域的影响，以及是否马上就会出现强 AI，以及与此相关的影响等，由于与本文主旨关系不大，我将择文再议”。其实后面一直想写，只不过因为要研究 ChatGPT 的实现和&lt;a href=&quot;https://yam.gift/2023/02/19/NLP/2023-02-19-ChatGPT-Labeling/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;标注&lt;/a&gt;，所以耽搁到现在，现在总算可以把这个坑给填上，只不过我把影响范围进一步扩大了——到产业级别。&lt;/p&gt;
&lt;p&gt;本文主要就 ChatGPT 对职业、行业和产业的影响展开讨论，为了避免被其他信息影响，最近一段时间几乎没看（刻意为之）类似新闻或文章，所以内容更多会偏主观，仅供参考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 标注指南：任务、数据与规范</title>
    <link href="https://www.yam.gift/2023/02/19/NLP/2023-02-19-ChatGPT-Labeling/"/>
    <id>https://www.yam.gift/2023/02/19/NLP/2023-02-19-ChatGPT-Labeling/</id>
    <published>2023-02-19T15:30:00.000Z</published>
    <updated>2024-06-12T02:06:43.537Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;ChatGPT 刚刚出来时，业内人士一致认为高质量的数据是一个非常关键的因素。且不论这个结论在 ChatGPT 这里是否正确，但高质量的数据对模型大有裨益却是公认的。而且，我们也可以从公开的 InstructGPT 标注指南中对此窥探一二。本文主要就围绕这份指南进行介绍，有点标题党了，但是考虑到 ChatGPT 和 InstructGPT 是兄弟关系，我们有理由相信 ChatGPT 的标注也是基于 InstructGPT 给出的指南进行的。当然不一定是全部，但至少我们可以从中学习和借鉴一些东西，是有此文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Prompt" scheme="https://www.yam.gift/tags/Prompt/"/>
    
      <category term="InstructGPT" scheme="https://www.yam.gift/tags/InstructGPT/"/>
    
      <category term="Labeling" scheme="https://www.yam.gift/tags/Labeling/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT Prompt 示例</title>
    <link href="https://www.yam.gift/2023/01/31/NLP/2023-01-31-ChatGPT-Prompt-Example/"/>
    <id>https://www.yam.gift/2023/01/31/NLP/2023-01-31-ChatGPT-Prompt-Example/</id>
    <published>2023-01-31T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.537Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;下面的 Case 主要收集自网络，我会在后面添加上出处。关于 Prompt 设计技巧可以参考之前的文章：&lt;a href=&quot;https://yam.gift/2023/01/25/NLP/2023-01-25-ChatGPT-Prompt-Engineering/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ChatGPT Prompt 工程：设计、实践与思考 | Yam&lt;/a&gt;，这里面的一些代表性 Case 也挪过来了。&lt;/p&gt;
&lt;p&gt;特别说明：我们还是尽量从「设计」的角度给出 Case，而不是任务或内容。&lt;/p&gt;
&lt;p&gt;另外需要说明：经测试，有些在中文下效果不如英文好（英文 Prompt 中文版本都是 ChatGPT 翻译的）。目前已有 Case 如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接问答、解释（不需要设计）&lt;/li&gt;
&lt;li&gt;扮演互动&lt;/li&gt;
&lt;li&gt;扮演+任务+步骤+上下文+目标+格式&lt;/li&gt;
&lt;li&gt;使用扮演绕过限制&lt;/li&gt;
&lt;li&gt;目标+上下文+多任务&lt;/li&gt;
&lt;li&gt;标题+指定对象+任务&lt;/li&gt;
&lt;li&gt;合作创作&lt;/li&gt;
&lt;li&gt;表格转换&lt;/li&gt;
&lt;li&gt;简化长 Prompt&lt;/li&gt;
&lt;li&gt;综合多个结果&lt;/li&gt;
&lt;li&gt;创造力增强&lt;/li&gt;
&lt;li&gt;游戏引擎&lt;/li&gt;
&lt;li&gt;推荐&lt;/li&gt;
&lt;li&gt;思维树&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Prompt" scheme="https://www.yam.gift/tags/Prompt/"/>
    
  </entry>
  
  <entry>
    <title>语言模型级联</title>
    <link href="https://www.yam.gift/2023/01/27/Paper/2023-01-27-Language-Model-Cascades/"/>
    <id>https://www.yam.gift/2023/01/27/Paper/2023-01-27-Language-Model-Cascades/</id>
    <published>2023-01-27T15:00:00.000Z</published>
    <updated>2024-06-12T02:06:43.549Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇来自 Google 的研究结果，通过一定的方法和策略，比如多个预训练模型结合，进一步提升模型整体推理能力。本文主要是对这方面的研究做了一个整体统一的划分，包括：思维链（Chain-of-Thought），验证器（Verifiers, STaR）选择-推理（Selection-Inference），工具使用（Tool Use）等，这些统称为：语言模型级联（Language Model Cascades）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Cascades" scheme="https://www.yam.gift/tags/Cascades/"/>
    
      <category term="CoT" scheme="https://www.yam.gift/tags/CoT/"/>
    
      <category term="Verifier" scheme="https://www.yam.gift/tags/Verifier/"/>
    
      <category term="STaR" scheme="https://www.yam.gift/tags/STaR/"/>
    
      <category term="Selection-Inference" scheme="https://www.yam.gift/tags/Selection-Inference/"/>
    
  </entry>
  
</feed>
