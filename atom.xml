<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2020-10-22T08:48:54.717Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>Yam</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>协同过滤</title>
    <link href="https://www.yam.gift/2020/10/22/RecSys/2020-10-22-CollaborativeFiltering/"/>
    <id>https://www.yam.gift/2020/10/22/RecSys/2020-10-22-CollaborativeFiltering/</id>
    <published>2020-10-22T15:00:00.000Z</published>
    <updated>2020-10-22T08:48:54.717Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;协同过滤是推荐领域的经典算法，它的思想非常朴素：和我们兴趣相似的人喜好也类似。所以，自然而然，只要找到和我们兴趣相似的人，将他们喜好的商品（又不在我们喜好商品的列表中）推送给我们就完成了推荐任务。本文主要介绍该算法，具体包括以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相似度计算&lt;/li&gt;
&lt;li&gt;基于用户的协同过滤&lt;/li&gt;
&lt;li&gt;基于商品的协同过滤&lt;/li&gt;
&lt;li&gt;算法评估&lt;/li&gt;
&lt;li&gt;算法评价&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Recommendation" scheme="https://www.yam.gift/tags/Recommendation/"/>
    
      <category term="Collaborative Filtering" scheme="https://www.yam.gift/tags/Collaborative-Filtering/"/>
    
      <category term="UserCF" scheme="https://www.yam.gift/tags/UserCF/"/>
    
      <category term="ItemCF" scheme="https://www.yam.gift/tags/ItemCF/"/>
    
      <category term="Similarity" scheme="https://www.yam.gift/tags/Similarity/"/>
    
      <category term="Jaccard" scheme="https://www.yam.gift/tags/Jaccard/"/>
    
      <category term="Cosine" scheme="https://www.yam.gift/tags/Cosine/"/>
    
      <category term="Pearson" scheme="https://www.yam.gift/tags/Pearson/"/>
    
      <category term="Minkowski" scheme="https://www.yam.gift/tags/Minkowski/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统概述</title>
    <link href="https://www.yam.gift/2020/10/19/RecSys/2020-10-19-RecIntroduction/"/>
    <id>https://www.yam.gift/2020/10/19/RecSys/2020-10-19-RecIntroduction/</id>
    <published>2020-10-19T15:00:00.000Z</published>
    <updated>2020-10-19T14:45:43.007Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文主要从整体角度介绍推荐系统，主要内容包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推荐系统简介&lt;/li&gt;
&lt;li&gt;推荐系统架构&lt;/li&gt;
&lt;li&gt;如何评价一个推荐系统&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Embedding" scheme="https://www.yam.gift/tags/Embedding/"/>
    
      <category term="Recommendation" scheme="https://www.yam.gift/tags/Recommendation/"/>
    
      <category term="Metric" scheme="https://www.yam.gift/tags/Metric/"/>
    
  </entry>
  
  <entry>
    <title>Funnel Transformer 论文笔记</title>
    <link href="https://www.yam.gift/2020/10/13/Paper/2020-10-13-FunnelTransformer/"/>
    <id>https://www.yam.gift/2020/10/13/Paper/2020-10-13-FunnelTransformer/</id>
    <published>2020-10-13T15:00:00.000Z</published>
    <updated>2020-10-13T08:12:12.016Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2006.03236&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2006.03236] Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/laiguokun/Funnel-Transformer/tree/master/tensorflow&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Funnel-Transformer/tensorflow at master · laiguokun/Funnel-Transformer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：Block 卷积的 Transformer。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="Self-Attention" scheme="https://www.yam.gift/tags/Self-Attention/"/>
    
      <category term="Funnel Transformer" scheme="https://www.yam.gift/tags/Funnel-Transformer/"/>
    
      <category term="Pooling" scheme="https://www.yam.gift/tags/Pooling/"/>
    
  </entry>
  
  <entry>
    <title>模型融合</title>
    <link href="https://www.yam.gift/2020/09/26/ML/2020-09-26-ModelFusing/"/>
    <id>https://www.yam.gift/2020/09/26/ML/2020-09-26-ModelFusing/</id>
    <published>2020-09-26T15:00:00.000Z</published>
    <updated>2020-09-26T15:36:40.351Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;模型融合思想很简单，就是将多种不同类型的模型结合起来共同预测结果——”三个臭皮匠，顶个诸葛亮“。模型融合主要有以下方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平均：简单平均和加权平均&lt;/li&gt;
&lt;li&gt;投票：简单投票和加权投票&lt;/li&gt;
&lt;li&gt;stacking：多层模型，利用预测结果再拟合预测&lt;/li&gt;
&lt;li&gt;blending：选取部分数据预测，得到的值作为新特征&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Data Science" scheme="https://www.yam.gift/tags/Data-Science/"/>
    
      <category term="Machine Learning" scheme="https://www.yam.gift/tags/Machine-Learning/"/>
    
      <category term="Voting" scheme="https://www.yam.gift/tags/Voting/"/>
    
      <category term="Stacking" scheme="https://www.yam.gift/tags/Stacking/"/>
    
      <category term="Blending" scheme="https://www.yam.gift/tags/Blending/"/>
    
      <category term="StratifiedKFold" scheme="https://www.yam.gift/tags/StratifiedKFold/"/>
    
  </entry>
  
  <entry>
    <title>建模调参</title>
    <link href="https://www.yam.gift/2020/09/24/ML/2020-09-24-ModelParameters/"/>
    <id>https://www.yam.gift/2020/09/24/ML/2020-09-24-ModelParameters/</id>
    <published>2020-09-24T15:00:00.000Z</published>
    <updated>2020-09-25T14:05:25.179Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;通过前面的 &lt;a href=&quot;https://yam.gift/2020/09/18/ML/2020-09-18-EDA/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;EDA&lt;/a&gt; 和&lt;a href=&quot;https://yam.gift/2020/09/21/ML/2020-09-21-FeatureEngineering/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;特征工程&lt;/a&gt;探索，想必应该已经对数据有了比较深入的了解，那么接下来就是利用之前所学来建模看看实战效果了。因为之前是系统性学习，所以并不一定所有的技术都要用到，而且建模应该是个结合对数据已有了解的基础上进行重新思考的过程。&lt;/p&gt;
&lt;p&gt;本文分为以下几个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;重新思考梳理 Pipeline 流程&lt;/li&gt;
&lt;li&gt;建模&lt;/li&gt;
&lt;li&gt;调参&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Data Science" scheme="https://www.yam.gift/tags/Data-Science/"/>
    
      <category term="Machine Learning" scheme="https://www.yam.gift/tags/Machine-Learning/"/>
    
      <category term="Tuning" scheme="https://www.yam.gift/tags/Tuning/"/>
    
      <category term="Model Evaluation" scheme="https://www.yam.gift/tags/Model-Evaluation/"/>
    
      <category term="Data Preprocess" scheme="https://www.yam.gift/tags/Data-Preprocess/"/>
    
  </entry>
  
  <entry>
    <title>特征工程</title>
    <link href="https://www.yam.gift/2020/09/21/ML/2020-09-21-FeatureEngineering/"/>
    <id>https://www.yam.gift/2020/09/21/ML/2020-09-21-FeatureEngineering/</id>
    <published>2020-09-21T15:00:00.000Z</published>
    <updated>2020-09-21T15:44:29.647Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;常听一句话说 “你还能玩儿出花来”，我觉得特征工程就是这么个把那些看上去普普通通的 “数据” 玩儿出花的过程。如果用 DIKW 模型（Data Information Knowledge Wisdom）来理解，Data 显然就是原始的一个个数据值，Information 就是对数据进行分析、处理后得到的具有一定意义的东西。&lt;/p&gt;
&lt;p&gt;严格的定义如下：特征工程是对原始数据进行一系列工程处理，将其提炼为特征根，作为模型的输入。它旨在去除原数据中的杂质和冗余，使得模型与预测值之间能够以此建立联系。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Data Science" scheme="https://www.yam.gift/tags/Data-Science/"/>
    
      <category term="Machine Learning" scheme="https://www.yam.gift/tags/Machine-Learning/"/>
    
      <category term="Feature Engineering" scheme="https://www.yam.gift/tags/Feature-Engineering/"/>
    
      <category term="binning" scheme="https://www.yam.gift/tags/binning/"/>
    
      <category term="LOF" scheme="https://www.yam.gift/tags/LOF/"/>
    
      <category term="Isolation Forest" scheme="https://www.yam.gift/tags/Isolation-Forest/"/>
    
      <category term="IQR" scheme="https://www.yam.gift/tags/IQR/"/>
    
      <category term="RFE" scheme="https://www.yam.gift/tags/RFE/"/>
    
      <category term="Chi2" scheme="https://www.yam.gift/tags/Chi2/"/>
    
      <category term="Z-Score" scheme="https://www.yam.gift/tags/Z-Score/"/>
    
  </entry>
  
  <entry>
    <title>EDA</title>
    <link href="https://www.yam.gift/2020/09/18/ML/2020-09-18-EDA/"/>
    <id>https://www.yam.gift/2020/09/18/ML/2020-09-18-EDA/</id>
    <published>2020-09-18T15:00:00.000Z</published>
    <updated>2020-09-19T23:31:48.249Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;探索性数据分析 EDA（Exploratory Data Analysis）是数据分析和挖掘的第一步，主要是对数据集进行了解，包括基本情况、特征情况、特征间关系等等，为进一步的分析和挖掘提供信息。&lt;/p&gt;
&lt;p&gt;一个完整的 EDA 过程一般大致包括四步：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题定义：问题定义涉及的主要任务是定义分析的主要目标，定义主要的可交付成果，概述主要角色和职责，获取数据的当前状态，定义时间表以及执行成本/收益分析。&lt;/li&gt;
&lt;li&gt;数据准备：包括数据源定义、数据 schema 定义、数据特征了解、数据清理、数据转换、数据分割等。&lt;/li&gt;
&lt;li&gt;数据分析：这是处理描述性统计信息和数据分析的最关键步骤之一。 主要任务包括汇总数据，发现数据之间隐藏的关联和关系，开发预测模型，评估模型以及计算精度。&lt;/li&gt;
&lt;li&gt;结果展示：以图表、摘要、地图和图表的形式将数据集呈现给目标受众。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Data Science" scheme="https://www.yam.gift/tags/Data-Science/"/>
    
      <category term="EDA" scheme="https://www.yam.gift/tags/EDA/"/>
    
  </entry>
  
  <entry>
    <title>Metrics</title>
    <link href="https://www.yam.gift/2020/09/15/ML/2020-09-15-Metrics/"/>
    <id>https://www.yam.gift/2020/09/15/ML/2020-09-15-Metrics/</id>
    <published>2020-09-15T12:00:00.000Z</published>
    <updated>2020-10-20T05:35:29.864Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;机器学习的数据集一般被划分为训练集和测试集，训练集用于训练模型，测试集则用于评估模型。针对不同的机器学习问题（分类、排序、回归、序列预测等），评估指标的选择也有所不同。本文主要介绍机器学习中常用的模型评估指标。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="Machine Learning" scheme="https://www.yam.gift/tags/Machine-Learning/"/>
    
      <category term="Accuracy" scheme="https://www.yam.gift/tags/Accuracy/"/>
    
      <category term="Precision" scheme="https://www.yam.gift/tags/Precision/"/>
    
      <category term="Recall" scheme="https://www.yam.gift/tags/Recall/"/>
    
      <category term="RMSE" scheme="https://www.yam.gift/tags/RMSE/"/>
    
      <category term="ROC" scheme="https://www.yam.gift/tags/ROC/"/>
    
      <category term="AUC" scheme="https://www.yam.gift/tags/AUC/"/>
    
      <category term="P-R" scheme="https://www.yam.gift/tags/P-R/"/>
    
      <category term="KS" scheme="https://www.yam.gift/tags/KS/"/>
    
      <category term="WOE" scheme="https://www.yam.gift/tags/WOE/"/>
    
  </entry>
  
  <entry>
    <title>PEGASUS 论文笔记</title>
    <link href="https://www.yam.gift/2020/09/13/Paper/2020-09-13-PEGASUS/"/>
    <id>https://www.yam.gift/2020/09/13/Paper/2020-09-13-PEGASUS/</id>
    <published>2020-09-13T04:00:00.000Z</published>
    <updated>2020-09-13T03:34:19.064Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/1912.08777&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1912.08777] PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/google-research/pegasus&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;google-research/pegasus&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：基于 GSG 的 Transformer 在文本摘要上的应用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="PEGASUS" scheme="https://www.yam.gift/tags/PEGASUS/"/>
    
      <category term="Summarization" scheme="https://www.yam.gift/tags/Summarization/"/>
    
      <category term="GSG" scheme="https://www.yam.gift/tags/GSG/"/>
    
  </entry>
  
  <entry>
    <title>核方法 和 SMO</title>
    <link href="https://www.yam.gift/2020/09/09/ML/2020-09-09-Kernel-SMO/"/>
    <id>https://www.yam.gift/2020/09/09/ML/2020-09-09-Kernel-SMO/</id>
    <published>2020-09-09T04:00:00.000Z</published>
    <updated>2020-09-11T00:33:51.017Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://yam.gift/2020/08/13/ML/2020-08-13-SVM-Hard-Soft-KKT/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;上一部分&lt;/a&gt;介绍了硬间隔和软间隔支持向量机，本部分介绍非线性支持向量机（核方法）和序列最小最优化算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Machine Learning" scheme="https://www.yam.gift/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="https://www.yam.gift/tags/SVM/"/>
    
      <category term="Kernel" scheme="https://www.yam.gift/tags/Kernel/"/>
    
      <category term="Kernel Method" scheme="https://www.yam.gift/tags/Kernel-Method/"/>
    
      <category term="Kernel Function" scheme="https://www.yam.gift/tags/Kernel-Function/"/>
    
      <category term="SMO" scheme="https://www.yam.gift/tags/SMO/"/>
    
      <category term="Coordinate Ascent" scheme="https://www.yam.gift/tags/Coordinate-Ascent/"/>
    
  </entry>
  
  <entry>
    <title>Find First and Last Position of Element in Sorted Array (LeetCode 34)</title>
    <link href="https://www.yam.gift/2020/08/31/LeetCode/2020-08-31-Find-First-and-Last-Position-of-Element-in-Sorted-Array/"/>
    <id>https://www.yam.gift/2020/08/31/LeetCode/2020-08-31-Find-First-and-Last-Position-of-Element-in-Sorted-Array/</id>
    <published>2020-08-31T15:00:00.000Z</published>
    <updated>2020-08-31T15:06:50.242Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Given an array of integers &lt;code&gt;nums&lt;/code&gt; sorted in ascending order, find the starting and ending position of a given &lt;code&gt;target&lt;/code&gt; value.&lt;/p&gt;
&lt;p&gt;Your algorithm’s runtime complexity must be in the order of &lt;em&gt;O&lt;/em&gt;(log &lt;em&gt;n&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;If the target is not found in the array, return &lt;code&gt;[-1, -1]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Input: nums = [5,7,7,8,8,10], target = 8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Output: [3,4]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Example 2:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Input: nums = [5,7,7,8,8,10], target = 6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Output: [-1,-1]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Constraints:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;0 &amp;lt;= nums.length &amp;lt;= 10^5&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-10^9 &amp;lt;= nums[i] &amp;lt;= 10^9&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nums&lt;/code&gt; is a non decreasing array.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-10^9 &amp;lt;= target &amp;lt;= 10^9&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Binary Search" scheme="https://www.yam.gift/tags/Binary-Search/"/>
    
  </entry>
  
  <entry>
    <title>Search in Rotated Sorted Array (LeetCode 33, 81, 153)</title>
    <link href="https://www.yam.gift/2020/08/22/LeetCode/2020-08-22-Search-in-Rotated-Sorted-Array/"/>
    <id>https://www.yam.gift/2020/08/22/LeetCode/2020-08-22-Search-in-Rotated-Sorted-Array/</id>
    <published>2020-08-22T15:00:00.000Z</published>
    <updated>2020-08-31T13:59:02.392Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Given an integer array &lt;code&gt;nums&lt;/code&gt; sorted in ascending order, and an integer &lt;code&gt;target&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Suppose that &lt;code&gt;nums&lt;/code&gt; is rotated at some pivot unknown to you beforehand (i.e., &lt;code&gt;[0,1,2,4,5,6,7]&lt;/code&gt; might become &lt;code&gt;[4,5,6,7,0,1,2]&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;You should search for &lt;code&gt;target&lt;/code&gt; in &lt;code&gt;nums&lt;/code&gt; and if you found return its index, otherwise return &lt;code&gt;-1&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Input: nums = [4,5,6,7,0,1,2], target = 0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Output: 4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Example 2:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Input: nums = [4,5,6,7,0,1,2], target = 3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Output: -1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Example 3:&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Input: nums = [1], target = 0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Output: -1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Constraints:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;1 &amp;lt;= nums.length &amp;lt;= 5000&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-10^4 &amp;lt;= nums[i] &amp;lt;= 10^4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;All values of &lt;code&gt;nums&lt;/code&gt; are &lt;strong&gt;unique&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nums&lt;/code&gt; is guranteed to be rotated at some pivot.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;-10^4 &amp;lt;= target &amp;lt;= 10^4&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Search" scheme="https://www.yam.gift/tags/Search/"/>
    
      <category term="Binary Search" scheme="https://www.yam.gift/tags/Binary-Search/"/>
    
      <category term="LinkedList" scheme="https://www.yam.gift/tags/LinkedList/"/>
    
      <category term="Rotated Sorted Array" scheme="https://www.yam.gift/tags/Rotated-Sorted-Array/"/>
    
  </entry>
  
  <entry>
    <title>Swap Nodes in Paris (LeetCode 24)</title>
    <link href="https://www.yam.gift/2020/08/20/LeetCode/2020-08-20-Swap-Nodes-in-Paris/"/>
    <id>https://www.yam.gift/2020/08/20/LeetCode/2020-08-20-Swap-Nodes-in-Paris/</id>
    <published>2020-08-20T15:00:00.000Z</published>
    <updated>2020-08-20T15:35:36.867Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Given a linked list, swap every two adjacent nodes and return its head.&lt;/p&gt;
&lt;p&gt;You may &lt;strong&gt;not&lt;/strong&gt; modify the values in the list’s nodes, only nodes itself may be changed.&lt;/p&gt;
&lt;figure class=&quot;highlight&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Given 1-&amp;gt;2-&amp;gt;3-&amp;gt;4, you should return the list as 2-&amp;gt;1-&amp;gt;4-&amp;gt;3.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Recursion" scheme="https://www.yam.gift/tags/Recursion/"/>
    
      <category term="LinkedList" scheme="https://www.yam.gift/tags/LinkedList/"/>
    
      <category term="Swap" scheme="https://www.yam.gift/tags/Swap/"/>
    
  </entry>
  
  <entry>
    <title>Hard-SVM, Soft-SVM 和 KKT</title>
    <link href="https://www.yam.gift/2020/08/13/ML/2020-08-13-SVM-Hard-Soft-KKT/"/>
    <id>https://www.yam.gift/2020/08/13/ML/2020-08-13-SVM-Hard-Soft-KKT/</id>
    <published>2020-08-13T04:00:00.000Z</published>
    <updated>2020-09-02T00:37:30.115Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;SVM 是机器学习在神经网络兴起前最经典、有效的算法。它的思想主要是用一个超平面对数据集进行划分，但是能够分开数据集的超平面一般都有无数个，支持向量机的做法是 “间隔最大化”，也就是选择 “支持向量” 到分割平面距离之和最大的，进而将问题转换为一个凸优化问题。&lt;/p&gt;
&lt;p&gt;支持向量机根据数据集可分程度的不同分为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线性可分支持向量机：数据线性可分，硬间隔支持向量机&lt;/li&gt;
&lt;li&gt;线性（不可分）支持向量机：数据近似线性可分，软间隔支持向量机&lt;/li&gt;
&lt;li&gt;非线性支持向量机：数据线性不可分，核技巧 + 软间隔最大化&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SVM 是一套完整的数据处理算法，核方法的引入使得它具有了对非线性数据的处理能力。具体的方式是将低维数据映射到高维，这样原来不可分的数据自然就可分了。比如假设两类数据点完全是均匀随机分布的，此时如果在平面内无论使用直线还是曲线都无法将它们分开，但假设我们有能力让某一类数据点全部脱离二维进入三维（此处可以想象桌子上散乱着小米和钢珠，你猛地用双手拍桌子，小米会跳起来进入第三维），那它们之间任意的平面都可以轻易将它们隔开。事实上，神经网络使用了类似的方法，感知机的中间隐层做的也是类似的事情。&lt;/p&gt;
&lt;p&gt;本部分只介绍线性可分支持向量机和线性支持向量机。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Machine Learning" scheme="https://www.yam.gift/tags/Machine-Learning/"/>
    
      <category term="SVM" scheme="https://www.yam.gift/tags/SVM/"/>
    
      <category term="Hard-SVM" scheme="https://www.yam.gift/tags/Hard-SVM/"/>
    
      <category term="Soft-SVM" scheme="https://www.yam.gift/tags/Soft-SVM/"/>
    
      <category term="KKT" scheme="https://www.yam.gift/tags/KKT/"/>
    
      <category term="Hinge Loss" scheme="https://www.yam.gift/tags/Hinge-Loss/"/>
    
  </entry>
  
  <entry>
    <title>Generate Parentheses (LeetCode 22)</title>
    <link href="https://www.yam.gift/2020/08/12/LeetCode/2020-08-12-Generate-Parentheses/"/>
    <id>https://www.yam.gift/2020/08/12/LeetCode/2020-08-12-Generate-Parentheses/</id>
    <published>2020-08-12T15:00:00.000Z</published>
    <updated>2020-08-14T16:16:05.266Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Given &lt;em&gt;n&lt;/em&gt; pairs of parentheses, write a function to generate all combinations of well-formed parentheses.&lt;/p&gt;
&lt;p&gt;For example, given &lt;em&gt;n&lt;/em&gt; = 3, a solution set is:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;&quot;((()))&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;&quot;(()())&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;&quot;(())()&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;&quot;()(())&quot;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;string&quot;&gt;&quot;()()()&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Recursion" scheme="https://www.yam.gift/tags/Recursion/"/>
    
      <category term="Backtracking" scheme="https://www.yam.gift/tags/Backtracking/"/>
    
      <category term="Catalan" scheme="https://www.yam.gift/tags/Catalan/"/>
    
      <category term="Stirling" scheme="https://www.yam.gift/tags/Stirling/"/>
    
  </entry>
  
  <entry>
    <title>AI 小课堂：Activation Function</title>
    <link href="https://www.yam.gift/2020/07/05/AIBK/2020-07-05-Activation/"/>
    <id>https://www.yam.gift/2020/07/05/AIBK/2020-07-05-Activation/</id>
    <published>2020-07-05T15:00:00.000Z</published>
    <updated>2020-07-06T03:37:31.000Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本思想&quot;&gt;&lt;a href=&quot;#基本思想&quot; class=&quot;headerlink&quot; title=&quot;基本思想&quot;&gt;&lt;/a&gt;基本思想&lt;/h2&gt;&lt;p&gt;激活函数在深度学习中的作用就跟神经元中 “细胞体” 的功能类似：确定输出中哪些要被激活。我们都知道 SVM 通过核方法对非线性可分的数据进行分类，这其实是一种提升的方法（机器学习中很多问题都是类似的降个维度或升个维度）。为啥提升维度就能够让原本线性不可分的数据可分呢？我们以下图为例：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://qnimg.lovevivian.cn/aitc-activation-2.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;两组不同标签的数据构成一个近似的同心圆。要想将两种不同的点分开，靠二维的一条直线肯定是没办法了，此时我们可以把数据映射到三维空间，我们可以想象让同心圆之间再插入一个圆，然后让这个圆以内的整块都凸起来，也就是让它脱离原来的维度。这时候我们只要在两个平面中间任意选择一个平面就可以将数据集分开了。那这和我们的激活函数有啥关系呢？其实激活函数所提供的 “非线性” 变换正是类似的方式。也就是说，只要有非线性的激活函数，三层（输入、1 个隐层、输出层）的神经网络理论上可以逼近任意函数。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="Activation" scheme="https://www.yam.gift/tags/Activation/"/>
    
      <category term="GELU" scheme="https://www.yam.gift/tags/GELU/"/>
    
      <category term="RELU" scheme="https://www.yam.gift/tags/RELU/"/>
    
      <category term="Sigmoid" scheme="https://www.yam.gift/tags/Sigmoid/"/>
    
      <category term="Softmax" scheme="https://www.yam.gift/tags/Softmax/"/>
    
      <category term="TanH" scheme="https://www.yam.gift/tags/TanH/"/>
    
  </entry>
  
  <entry>
    <title>QA 小课堂：Introduction</title>
    <link href="https://www.yam.gift/2020/07/05/AIQA/2020-07-05-Introduction/"/>
    <id>https://www.yam.gift/2020/07/05/AIQA/2020-07-05-Introduction/</id>
    <published>2020-07-05T03:00:00.000Z</published>
    <updated>2020-08-23T10:43:44.713Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;说明：仅用于学习和技术交流，不提供任何投资建议。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;基本流程&quot;&gt;&lt;a href=&quot;#基本流程&quot; class=&quot;headerlink&quot; title=&quot;基本流程&quot;&gt;&lt;/a&gt;基本流程&lt;/h2&gt;&lt;p&gt;量化在金融分析中有两种主要的用途：选择标的，确定时机。一个完整的流程大概如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先有一个想法&lt;/li&gt;
&lt;li&gt;将想法变成策略&lt;/li&gt;
&lt;li&gt;将策略变成模型&lt;/li&gt;
&lt;li&gt;验证优化&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="Quant" scheme="https://www.yam.gift/tags/Quant/"/>
    
  </entry>
  
  <entry>
    <title>DeBERTa 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/06/27/Paper/2020-06-27-DeBERTa/"/>
    <id>https://www.yam.gift/2020/06/27/Paper/2020-06-27-DeBERTa/</id>
    <published>2020-06-27T15:00:00.000Z</published>
    <updated>2020-09-23T01:45:38.913Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2006.03654&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2006.03654] DeBERTa: Decoding-enhanced BERT with Disentangled Attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/microsoft/DeBERTa&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;microsoft/DeBERTa: The implementation of DeBERTa&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：增加位置-内容与内容-位置的自注意力增强位置和内容之间的依赖，用 EMD 缓解 BERT 预训练和精调因为 MASK 造成的不匹配问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Bert" scheme="https://www.yam.gift/tags/Bert/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="DeBERTa" scheme="https://www.yam.gift/tags/DeBERTa/"/>
    
      <category term="Disentangled Attention" scheme="https://www.yam.gift/tags/Disentangled-Attention/"/>
    
      <category term="EMD" scheme="https://www.yam.gift/tags/EMD/"/>
    
  </entry>
  
  <entry>
    <title>RoBERTa 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/06/25/Paper/2020-06-25-RoBERTa/"/>
    <id>https://www.yam.gift/2020/06/25/Paper/2020-06-25-RoBERTa/</id>
    <published>2020-06-25T13:00:00.000Z</published>
    <updated>2020-06-25T13:12:06.000Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1907.11692] RoBERTa: A Robustly Optimized BERT Pretraining Approach&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/roberta&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;fairseq/examples/roberta at master · pytorch/fairseq&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：&lt;/p&gt;
&lt;p&gt;对 BERT 几个小点（主要是动态 Mask 和不使用 NSP）进行优化取得了比较好的实践结果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Bert" scheme="https://www.yam.gift/tags/Bert/"/>
    
      <category term="RoBERTa" scheme="https://www.yam.gift/tags/RoBERTa/"/>
    
      <category term="Dynamic-Mask" scheme="https://www.yam.gift/tags/Dynamic-Mask/"/>
    
  </entry>
  
  <entry>
    <title>Bart 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/06/13/Paper/2020-06-13-Bart/"/>
    <id>https://www.yam.gift/2020/06/13/Paper/2020-06-13-Bart/</id>
    <published>2020-06-13T15:00:00.000Z</published>
    <updated>2020-06-14T03:49:13.000Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/pdf/1910.13461.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1910.13461.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/pytorch/fairseq&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/pytorch/fairseq&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：基于 Transformer Seq2Seq 架构适应各种不同的输入噪声。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="Bart" scheme="https://www.yam.gift/tags/Bart/"/>
    
  </entry>
  
</feed>
