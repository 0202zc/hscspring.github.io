<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2020-06-29T03:22:27.148Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>Yam</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>DeBERTa 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/06/27/Paper/2020-06-27-DeBERTa/"/>
    <id>https://www.yam.gift/2020/06/27/Paper/2020-06-27-DeBERTa/</id>
    <published>2020-06-27T15:00:00.000Z</published>
    <updated>2020-06-29T03:22:27.148Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2006.03654&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2006.03654] DeBERTa: Decoding-enhanced BERT with Disentangled Attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/microsoft/DeBERTa&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;microsoft/DeBERTa: The implementation of DeBERTa&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：增加位置-内容与内容-位置的自注意力增强位置和内容之间的依赖，用 EMD 缓解 BERT 预训练和精调因为 MASK 造成的不匹配问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="Bert" scheme="https://www.yam.gift/tags/Bert/"/>
    
      <category term="DeBERTa" scheme="https://www.yam.gift/tags/DeBERTa/"/>
    
      <category term="Disentangled Attention" scheme="https://www.yam.gift/tags/Disentangled-Attention/"/>
    
      <category term="EMD" scheme="https://www.yam.gift/tags/EMD/"/>
    
  </entry>
  
  <entry>
    <title>RoBERTa 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/06/25/Paper/2020-06-25-RoBERTa/"/>
    <id>https://www.yam.gift/2020/06/25/Paper/2020-06-25-RoBERTa/</id>
    <published>2020-06-25T13:00:00.000Z</published>
    <updated>2020-06-25T13:12:06.844Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[1907.11692] RoBERTa: A Robustly Optimized BERT Pretraining Approach&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/pytorch/fairseq/tree/master/examples/roberta&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;fairseq/examples/roberta at master · pytorch/fairseq&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：&lt;/p&gt;
&lt;p&gt;对 BERT 几个小点（主要是动态 Mask 和不使用 NSP）进行优化取得了比较好的实践结果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Bert" scheme="https://www.yam.gift/tags/Bert/"/>
    
      <category term="RoBERTa" scheme="https://www.yam.gift/tags/RoBERTa/"/>
    
      <category term="Dynamic-Mask" scheme="https://www.yam.gift/tags/Dynamic-Mask/"/>
    
  </entry>
  
  <entry>
    <title>Bart 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/06/13/Paper/2020-06-13-Bart/"/>
    <id>https://www.yam.gift/2020/06/13/Paper/2020-06-13-Bart/</id>
    <published>2020-06-13T15:00:00.000Z</published>
    <updated>2020-06-14T03:49:13.042Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/pdf/1910.13461.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1910.13461.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/pytorch/fairseq&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/pytorch/fairseq&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：基于 Transformer Seq2Seq 架构适应各种不同的输入噪声。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="Bart" scheme="https://www.yam.gift/tags/Bart/"/>
    
  </entry>
  
  <entry>
    <title>ALBERT 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/05/10/Paper/2020-05-10-ALBERT/"/>
    <id>https://www.yam.gift/2020/05/10/Paper/2020-05-10-ALBERT/</id>
    <published>2020-05-10T15:00:00.000Z</published>
    <updated>2020-05-11T04:57:20.486Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/pdf/1909.11942.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1909.11942.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/google-research/ALBERT&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;google-research/albert: ALBERT: A Lite BERT for Self-supervised Learning of Language Representations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：基于 Bert 的改进版本：分解 Embedding 参数、层间参数共享、SOP 替代 NSP。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Bert" scheme="https://www.yam.gift/tags/Bert/"/>
    
      <category term="ALBERT" scheme="https://www.yam.gift/tags/ALBERT/"/>
    
  </entry>
  
  <entry>
    <title>From Python to Engineer</title>
    <link href="https://www.yam.gift/2020/05/01/Collection/From-Python-to-Engineer/"/>
    <id>https://www.yam.gift/2020/05/01/Collection/From-Python-to-Engineer/</id>
    <published>2020-05-01T06:00:08.000Z</published>
    <updated>2020-05-13T01:34:02.704Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Python is one the most popular programming languages nowadays. It’s easy to learn and use. Maybe Python is your best choice if you want to be an engineer in future, or you want to work with programming.  Here I list many kinds of materials most of which comes from my collection. Hope this could help you to start your programming road. It might be a little hard, but a lot of fun as well. Let’s go.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I will update this blog continually when it needs to. By the way, If you are a very beginner to AI or computer, I recommend you read &lt;a href=&quot;https://github.com/hscspring/AIToolBox&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AIToolBox&lt;/a&gt; first.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Table-of-Contents&quot;&gt;&lt;a href=&quot;#Table-of-Contents&quot; class=&quot;headerlink&quot; title=&quot;Table of Contents&quot;&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt;&lt;div class=&quot;toc&quot;&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Skill&quot; data-toc-modified-id=&quot;Skill-1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Skill&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Basic&quot; data-toc-modified-id=&quot;Basic-1.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Basic&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Trick&quot; data-toc-modified-id=&quot;Trick-1.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;Trick&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#State&quot; data-toc-modified-id=&quot;State-1.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;State&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Pipe&quot; data-toc-modified-id=&quot;Pipe-1.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;Pipe&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#FP&quot; data-toc-modified-id=&quot;FP-1.5&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.5&amp;nbsp;&amp;nbsp;&lt;/span&gt;FP&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Datastructure-and-Algorithm&quot; data-toc-modified-id=&quot;Datastructure-and-Algorithm-1.6&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;1.6&amp;nbsp;&amp;nbsp;&lt;/span&gt;Datastructure and Algorithm&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#AI&quot; data-toc-modified-id=&quot;AI-2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2&amp;nbsp;&amp;nbsp;&lt;/span&gt;AI&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Deploy&quot; data-toc-modified-id=&quot;Deploy-2.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Deploy&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Toolkit&quot; data-toc-modified-id=&quot;Toolkit-2.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;Toolkit&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#DataAnnotation&quot; data-toc-modified-id=&quot;DataAnnotation-2.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;DataAnnotation&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#DeepLearning&quot; data-toc-modified-id=&quot;DeepLearning-2.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;DeepLearning&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#MachineLearning&quot; data-toc-modified-id=&quot;MachineLearning-2.5&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;2.5&amp;nbsp;&amp;nbsp;&lt;/span&gt;MachineLearning&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Engineering&quot; data-toc-modified-id=&quot;Engineering-3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3&amp;nbsp;&amp;nbsp;&lt;/span&gt;Engineering&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Design&quot; data-toc-modified-id=&quot;Design-3.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Design&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Test&quot; data-toc-modified-id=&quot;Test-3.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;Test&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Style&quot; data-toc-modified-id=&quot;Style-3.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;Style&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Profile&quot; data-toc-modified-id=&quot;Profile-3.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;Profile&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Safety&quot; data-toc-modified-id=&quot;Safety-3.5&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.5&amp;nbsp;&amp;nbsp;&lt;/span&gt;Safety&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Distribute&quot; data-toc-modified-id=&quot;Distribute-3.6&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.6&amp;nbsp;&amp;nbsp;&lt;/span&gt;Distribute&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#FrameWork&quot; data-toc-modified-id=&quot;FrameWork-3.7&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.7&amp;nbsp;&amp;nbsp;&lt;/span&gt;FrameWork&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Boilerplate&quot; data-toc-modified-id=&quot;Boilerplate-3.8&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.8&amp;nbsp;&amp;nbsp;&lt;/span&gt;Boilerplate&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#JWT&quot; data-toc-modified-id=&quot;JWT-3.9&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.9&amp;nbsp;&amp;nbsp;&lt;/span&gt;JWT&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Coroutine&quot; data-toc-modified-id=&quot;Coroutine-3.10&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.10&amp;nbsp;&amp;nbsp;&lt;/span&gt;Coroutine&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Concurrency&quot; data-toc-modified-id=&quot;Concurrency-3.11&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.11&amp;nbsp;&amp;nbsp;&lt;/span&gt;Concurrency&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Memory&quot; data-toc-modified-id=&quot;Memory-3.12&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.12&amp;nbsp;&amp;nbsp;&lt;/span&gt;Memory&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#API&quot; data-toc-modified-id=&quot;API-3.13&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.13&amp;nbsp;&amp;nbsp;&lt;/span&gt;API&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#gRPC&quot; data-toc-modified-id=&quot;gRPC-3.14&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.14&amp;nbsp;&amp;nbsp;&lt;/span&gt;gRPC&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Database&quot; data-toc-modified-id=&quot;Database-3.15&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.15&amp;nbsp;&amp;nbsp;&lt;/span&gt;Database&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Project&quot; data-toc-modified-id=&quot;Project-3.16&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;3.16&amp;nbsp;&amp;nbsp;&lt;/span&gt;Project&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Tool&quot; data-toc-modified-id=&quot;Tool-4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4&amp;nbsp;&amp;nbsp;&lt;/span&gt;Tool&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Spider&quot; data-toc-modified-id=&quot;Spider-4.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Spider&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Audio&quot; data-toc-modified-id=&quot;Audio-4.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;Audio&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Chat&quot; data-toc-modified-id=&quot;Chat-4.3&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.3&amp;nbsp;&amp;nbsp;&lt;/span&gt;Chat&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Text&quot; data-toc-modified-id=&quot;Text-4.4&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.4&amp;nbsp;&amp;nbsp;&lt;/span&gt;Text&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Task&quot; data-toc-modified-id=&quot;Task-4.5&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.5&amp;nbsp;&amp;nbsp;&lt;/span&gt;Task&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Visualization&quot; data-toc-modified-id=&quot;Visualization-4.6&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.6&amp;nbsp;&amp;nbsp;&lt;/span&gt;Visualization&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Logic&quot; data-toc-modified-id=&quot;Logic-4.7&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;4.7&amp;nbsp;&amp;nbsp;&lt;/span&gt;Logic&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Comprehensive&quot; data-toc-modified-id=&quot;Comprehensive-5&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5&amp;nbsp;&amp;nbsp;&lt;/span&gt;Comprehensive&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Resources&quot; data-toc-modified-id=&quot;Resources-5.1&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.1&amp;nbsp;&amp;nbsp;&lt;/span&gt;Resources&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#Courses&quot; data-toc-modified-id=&quot;Courses-5.2&quot;&gt;&lt;span class=&quot;toc-item-num&quot;&gt;5.2&amp;nbsp;&amp;nbsp;&lt;/span&gt;Courses&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="DeepLearning" scheme="https://www.yam.gift/tags/DeepLearning/"/>
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>多贝里《清醒思考的艺术》读书笔记</title>
    <link href="https://www.yam.gift/2020/04/28/Mind/2020-04-28-The-Art-of-Clearly-Thinking/"/>
    <id>https://www.yam.gift/2020/04/28/Mind/2020-04-28-The-Art-of-Clearly-Thinking/</id>
    <published>2020-04-28T06:00:00.000Z</published>
    <updated>2020-04-28T07:43:50.332Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;52 个思维偏误和提示清单：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;幸存偏误：多看看失败的事，自然会发现成功并不是他们所说的那样。&lt;br&gt;游泳选手身材错觉：因为他那样，所以他看起来才这样。你想成为这样，你得先那样。&lt;br&gt;过度自信效应：大家都会盲目自信，但请你作好最坏打算。还记得查理芒格说的吗，知道自己可能死在哪里才不会去那里。&lt;br&gt;从众心理：你行为上可以去从众，但思想上一定要保持独立。&lt;br&gt;纠缠于沉没成本：千万别被过去影响，基于现在和未来判断！当断不断反受其乱！&lt;br&gt;互惠偏误：一段感情（关系）确定之前，多欠别人的；确定之后，全心全意。商业或普通关系，相反，直到变成你想要的为止。&lt;br&gt;确认偏误之一：随时准备好反驳自己并及时反驳，因为你没那么牛逼，也没那么幸运，世界更没那么简单，但你会自己骗自己。&lt;br&gt;确认偏误之二：不带任何色彩地重新审视，要注意精确和反驳证据。切忌先入为主。&lt;br&gt;权威偏误：大胆藐视权威吧，他可能比你好不到哪里去。&lt;br&gt;对比效应：为别人建立基准线，避免自己被基准线困扰。&lt;br&gt;现成偏误：千万别把经验当做理由。&lt;br&gt;在好转之前会先恶化陷阱：虽然无法预测未来，但也不要拿经验当依据，而应该看过程中的各项指标情况。&lt;br&gt;故事偏误：对一切故事保持足够理性，拆了它，解剖它；但，学会给别人讲故事。。&lt;br&gt;事后诸葛亮偏误：没事别瞎BB，更别轻易下结论。俗话说：“你知道个鸟。”&lt;br&gt;司机的知识：别像赵括一样。万一遇到这种人，呵呵就行了。&lt;br&gt;控制错觉：你基本上影响不了什么，正确认识问题和自己真正能影响的。&lt;br&gt;激励过敏倾向：任何难以理解的事其实只是你没发现其中的“因”。要控制“因”，比如提前约定服务价格。&lt;br&gt;回归均值：很多时候变好或变坏和你做一些不相干的事情没什么关系，那只是周期性的 “回归均值”。&lt;br&gt;公地悲剧：如果可能获得好处而又不损失什么，人类最喜欢了。&lt;br&gt;结果偏误：关注结果，更加关注过程以及其他条件。&lt;br&gt;选择的悖论：不忘初心，方得始终。&lt;br&gt;讨喜偏误：能接受我的不好，才能拥有我的好。先想着不好。&lt;br&gt;禀赋效应：赤条条来赤条条走，该舍当舍。&lt;br&gt;奇迹：小概率不等于零概率，只所以你觉得怪是因为你见得少。&lt;br&gt;团体迷思：无论何时，只要需要，请发表你独立思考得来的观点。&lt;br&gt;忽视概率偏误：用数字说话，可以的话，懂点统计学。&lt;br&gt;零风险偏误：明白一个道理，没有什么是确定的，唯一确定的是不确定本身。&lt;br&gt;稀少性谬误：关注作用和需要而不要关注稀缺性。&lt;br&gt;忽视基本概率：不要用绝对概率，用贝叶斯概率，即不要忽视基本概率。&lt;br&gt;赌徒谬误：如果是独立的随机事件，那它每一次都如第一次。&lt;br&gt;锚定效应：在不对称信息下，请警惕锚定效应。&lt;br&gt;归纳法：不要错误估计个人对系统的影响力，即使方法没问题。&lt;br&gt;规避损失：面对 “损失” 理性一点。&lt;br&gt;社会性懈怠：权力责任到人，突显个人效率。&lt;br&gt;指数增长：事关增长率时，不要相信感觉。&lt;br&gt;赢家的诅咒：不要参与拍卖。&lt;br&gt;基本特征谬误：关注情境或事物而不只是人。&lt;br&gt;错误的因果关系：相关只是表面，因果方为本质。&lt;br&gt;光环效应：客观、理性，只关注当下关注的点（说起来就是这么容易）。&lt;br&gt;替代途径：客观、基于数据衡量收益和风险。&lt;br&gt;预测的错觉：条件反射地对预测持谨慎态度。&lt;br&gt;关联谬误：做重要决定时不要轻信自己的直觉。&lt;br&gt;框架效应：时刻注意框架效应的影响，剔除所有无关描述，只关注信息。&lt;br&gt;行动偏误：如果情况不明，不要采取任何行动。&lt;br&gt;不作为偏误：不作为可以是一时的选择，但不要成为永久的策略。&lt;br&gt;自利偏误：找个直言不讳的朋友或者让对手来评价你。&lt;br&gt;享乐适应症：把时间花在你真正爱做的事情上。&lt;br&gt;自我选择偏误：当你觉得自己倒霉时，先想想这是不是本来就是个概率事件。&lt;br&gt;联想偏误：客观分析、就事论事，不要胡思乱想。&lt;br&gt;新手的运气：别多想，没有什么新手运气。&lt;br&gt;认知失调：承认自己的错误需要很大的勇气，但好过掩耳盗铃。&lt;br&gt;双曲贴现：自制力很难，但很有必要。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Thought" scheme="https://www.yam.gift/tags/Thought/"/>
    
      <category term="Cognition" scheme="https://www.yam.gift/tags/Cognition/"/>
    
      <category term="Psychology" scheme="https://www.yam.gift/tags/Psychology/"/>
    
  </entry>
  
  <entry>
    <title>DistilBERT 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/04/27/Paper/2020-04-27-DistilBERT/"/>
    <id>https://www.yam.gift/2020/04/27/Paper/2020-04-27-DistilBERT/</id>
    <published>2020-04-27T15:00:00.000Z</published>
    <updated>2020-05-03T03:28:51.771Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/pdf/1910.01108.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1910.01108.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/huggingface/transformers/tree/master/examples/distillation&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;transformers/examples/distillation at master · huggingface/transformers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：通过知识蒸馏（在 logits，hidden_states 上计算学生与教师的 loss）训练一个小（主要是层数）模型实现和大模型类似的效果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Bert" scheme="https://www.yam.gift/tags/Bert/"/>
    
      <category term="DistilBERT" scheme="https://www.yam.gift/tags/DistilBERT/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 代码笔记</title>
    <link href="https://www.yam.gift/2020/04/23/Paper/2020-04-23-Transformer/"/>
    <id>https://www.yam.gift/2020/04/23/Paper/2020-04-23-Transformer/</id>
    <published>2020-04-23T15:00:00.000Z</published>
    <updated>2020-05-07T08:21:40.143Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;之前写过一篇关于 &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention Is All You Need&lt;/a&gt; 的 &lt;a href=&quot;https://yam.gift/2019/08/04/Paper/2019-08-04-Transformer-Paper/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文笔记&lt;/a&gt;，不过那时候写的笔记都没有深入 Code 环节，再加上其实已经有了一篇 &lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The Annotated Transformer&lt;/a&gt;，也没必要做重复工作。不过现在 Transformer 已经大放异彩到几乎成为了标准配件，所以觉得有必要单独拿出来就组件角度再次学习一遍，于是就有了这篇文章。&lt;/p&gt;
&lt;p&gt;本文代码主要基于 &lt;a href=&quot;https://github.com/OpenNMT/OpenNMT-py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;OpenNMT&lt;/a&gt;，另外也参考了一点 &lt;a href=&quot;https://github.com/pytorch/fairseq&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;fairseq&lt;/a&gt;，这俩都是 PyTorch 实现的。Tensorflow 实现的版本相对更多一些，详见 Appendix 部分。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Attention" scheme="https://www.yam.gift/tags/Attention/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="Self-Attention" scheme="https://www.yam.gift/tags/Self-Attention/"/>
    
      <category term="Multi-Head Attention" scheme="https://www.yam.gift/tags/Multi-Head-Attention/"/>
    
      <category term="Encoder" scheme="https://www.yam.gift/tags/Encoder/"/>
    
      <category term="Decoder" scheme="https://www.yam.gift/tags/Decoder/"/>
    
  </entry>
  
  <entry>
    <title>Luong Attention 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/04/14/Paper/2020-04-14-Luong-Attention/"/>
    <id>https://www.yam.gift/2020/04/14/Paper/2020-04-14-Luong-Attention/</id>
    <published>2020-04-14T04:00:00.000Z</published>
    <updated>2020-05-03T03:29:19.254Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/pdf/1508.04025v5.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1508.04025v5.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：文章未提供，见 Appendix&lt;/p&gt;
&lt;p&gt;核心思想：通过在 Decoder 的每一步使用 Encoder 信息，并对 Encoder 信息赋予不同权重来获得更好的 Decoder 结果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Attention" scheme="https://www.yam.gift/tags/Attention/"/>
    
      <category term="Luong Attention" scheme="https://www.yam.gift/tags/Luong-Attention/"/>
    
  </entry>
  
  <entry>
    <title>GPT-2 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/04/07/Paper/2020-04-07-GPT2/"/>
    <id>https://www.yam.gift/2020/04/07/Paper/2020-04-07-GPT2/</id>
    <published>2020-04-07T04:00:00.000Z</published>
    <updated>2020-05-03T03:29:30.011Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/openai/gpt-2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;openai/gpt-2: Code for the paper “Language Models are Unsupervised Multitask Learners”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/minimaxir/gpt-2-simple&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;minimaxir/gpt-2-simple: Python package to easily retrain OpenAI’s GPT-2 text-generating model on new texts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;核心思想：基于 Transformer 的更加 General 的语言模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="GPT-2" scheme="https://www.yam.gift/tags/GPT-2/"/>
    
      <category term="Language Model" scheme="https://www.yam.gift/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>Node2Vec 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/03/30/Paper/2020-03-30-Node2Vec/"/>
    <id>https://www.yam.gift/2020/03/30/Paper/2020-03-30-Node2Vec/</id>
    <published>2020-03-30T15:55:00.000Z</published>
    <updated>2020-05-03T03:29:44.791Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/pdf/1607.00653.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;node2vec: Scalable Feature Learning for Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/aditya-grover/node2vec&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;aditya-grover/node2vec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：通过给网络节点的邻居定义一个灵活的概念，并设计了一个能够有效探索邻居多样性的有偏随机游走程序，来学习网络的节点表征。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Graph" scheme="https://www.yam.gift/tags/Graph/"/>
    
      <category term="node2vec" scheme="https://www.yam.gift/tags/node2vec/"/>
    
      <category term="DeepGraph" scheme="https://www.yam.gift/tags/DeepGraph/"/>
    
  </entry>
  
  <entry>
    <title>TextRank Keyword Extraction 论文+代码笔记</title>
    <link href="https://www.yam.gift/2020/03/21/Paper/2020-03-21-Text-Rank/"/>
    <id>https://www.yam.gift/2020/03/21/Paper/2020-03-21-Text-Rank/</id>
    <published>2020-03-21T10:00:00.000Z</published>
    <updated>2020-05-03T03:34:31.873Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://www.aclweb.org/anthology/W04-3252.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TextRank: Bringing Order into Texts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码：&lt;a href=&quot;https://github.com/networkx/networkx/blob/master/networkx/algorithms/link_analysis/pagerank_alg.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;networkx/pagerank_alg.py at master · networkx/networkx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：TextRank 是基于 Google PageRank 的一种关键词（句子）提取方法，它的本质是对文本 Token 按窗口构建节点和边（实际为节点在一定窗口范围内的共现关系），根据 PageRank 得到节点的 Score 排序。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Graph" scheme="https://www.yam.gift/tags/Graph/"/>
    
      <category term="Keyword" scheme="https://www.yam.gift/tags/Keyword/"/>
    
      <category term="TextRank" scheme="https://www.yam.gift/tags/TextRank/"/>
    
      <category term="PageRank" scheme="https://www.yam.gift/tags/PageRank/"/>
    
  </entry>
  
  <entry>
    <title>Bahdanau Attention 论文笔记</title>
    <link href="https://www.yam.gift/2020/02/08/Paper/2020-02-08-Bahdanau-Attention-Paper/"/>
    <id>https://www.yam.gift/2020/02/08/Paper/2020-02-08-Bahdanau-Attention-Paper/</id>
    <published>2020-02-08T09:00:00.000Z</published>
    <updated>2020-05-03T03:30:16.855Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1409.0473.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;作者猜测 encoder 中使用固定长度的向量（即将句子编码成一个固定长度的向量）可能是 performance 的瓶颈。因此提出一种能够自动 search 源句子中与预测词相关的部分。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Attention" scheme="https://www.yam.gift/tags/Attention/"/>
    
      <category term="Bahdanau Attention" scheme="https://www.yam.gift/tags/Bahdanau-Attention/"/>
    
  </entry>
  
  <entry>
    <title>Neural Architectures for Named Entity Recognition 论文笔记</title>
    <link href="https://www.yam.gift/2019/12/28/Paper/2019-12-28-Bi-LSTM-CRF-NER-Paper/"/>
    <id>https://www.yam.gift/2019/12/28/Paper/2019-12-28-Bi-LSTM-CRF-NER-Paper/</id>
    <published>2019-12-28T09:00:00.000Z</published>
    <updated>2020-05-03T03:30:35.877Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/1603.01360.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1603.01360.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;code: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/clab/stack-lstm-ner&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;clab/stack-lstm-ner: NER system based on stack LSTMs&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/glample/tagger&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;glample/tagger: Named Entity Recognition Tool&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;核心思想：pretrained + character-based 词表示分别学习形态和拼写，Bi-LSTM + CRF 和基于转移的模型均可以对输出标签的依赖关系建模。&lt;/p&gt;
&lt;p&gt;看了 Related Work 后发现很多想法其实早就冒出来了，不同的论文在不同点上使用了不同的方法，本篇恰好用这样的方法取得了最好的效果。其实，我觉得更加有意思的是基于转移的模型，它构建了一个 action 的时间序列，感觉更加抽象，想法更加精妙。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="NER" scheme="https://www.yam.gift/tags/NER/"/>
    
      <category term="CRF" scheme="https://www.yam.gift/tags/CRF/"/>
    
      <category term="Bi-LSTM" scheme="https://www.yam.gift/tags/Bi-LSTM/"/>
    
      <category term="Embedding" scheme="https://www.yam.gift/tags/Embedding/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch6）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch6/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch6/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-26T01:17:12.037Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch3）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch3/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch3/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-23T15:12:09.172Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;toc&quot;&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#第三章：高质量的代码&quot; data-toc-modified-id=&quot;第三章：高质量的代码-1&quot;&gt;第三章：高质量的代码&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-16：数值的整数次方&quot; data-toc-modified-id=&quot;面试题-16：数值的整数次方-1.1&quot;&gt;面试题 16：数值的整数次方&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-17：打印-1-到最大的-n-位数&quot; data-toc-modified-id=&quot;面试题-17：打印-1-到最大的-n-位数-1.2&quot;&gt;面试题 17：打印 1 到最大的 n 位数&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-18（一）：在-O-(1)-时间删除链表节点&quot; data-toc-modified-id=&quot;面试题-18（一）：在-O-(1)-时间删除链表节点-1.3&quot;&gt;面试题 18（一）：在 O (1) 时间删除链表节点&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-18（二）：删除链表中重复的节点&quot; data-toc-modified-id=&quot;面试题-18（二）：删除链表中重复的节点-1.4&quot;&gt;面试题 18（二）：删除链表中重复的节点&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-19：正则表达式匹配&quot; data-toc-modified-id=&quot;面试题-19：正则表达式匹配-1.5&quot;&gt;面试题 19：正则表达式匹配&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-20：表示数值的字符串&quot; data-toc-modified-id=&quot;面试题-20：表示数值的字符串-1.6&quot;&gt;面试题 20：表示数值的字符串&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-21：调整数组顺序使奇数位于偶数前面&quot; data-toc-modified-id=&quot;面试题-21：调整数组顺序使奇数位于偶数前面-1.7&quot;&gt;面试题 21：调整数组顺序使奇数位于偶数前面&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-22：链表中倒数第-k-个节点&quot; data-toc-modified-id=&quot;面试题-22：链表中倒数第-k-个节点-1.8&quot;&gt;面试题 22：链表中倒数第 k 个节点&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-23：链表中环的入口节点&quot; data-toc-modified-id=&quot;面试题-23：链表中环的入口节点-1.9&quot;&gt;面试题 23：链表中环的入口节点&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-24：反转链表&quot; data-toc-modified-id=&quot;面试题-24：反转链表-1.10&quot;&gt;面试题 24：反转链表&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-25：合并两个排序的链表&quot; data-toc-modified-id=&quot;面试题-25：合并两个排序的链表-1.11&quot;&gt;面试题 25：合并两个排序的链表&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-26：树的子结构&quot; data-toc-modified-id=&quot;面试题-26：树的子结构-1.12&quot;&gt;面试题 26：树的子结构&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch2）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch2/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch2/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-23T15:11:57.000Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;toc&quot;&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#第一章：面试的流程&quot; data-toc-modified-id=&quot;第一章：面试的流程-1&quot;&gt;第一章：面试的流程 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#第二章：面试需要的基础知识&quot; data-toc-modified-id=&quot;第二章：面试需要的基础知识-2&quot;&gt;第二章：面试需要的基础知识 &lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-2：实现-Singleton-模式&quot; data-toc-modified-id=&quot;面试题-2：实现-Singleton-模式-2.1&quot;&gt;面试题 2：实现 Singleton 模式 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-3（一）：找出数组中重复的数字&quot; data-toc-modified-id=&quot;面试题-3（一）：找出数组中重复的数字-2.2&quot;&gt;面试题 3（一）：找出数组中重复的数字 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-3（二）：不修改数组找出重复的数字&quot; data-toc-modified-id=&quot;面试题-3（二）：不修改数组找出重复的数字-2.3&quot;&gt;面试题 3（二）：不修改数组找出重复的数字 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-4：二维数组中的查找&quot; data-toc-modified-id=&quot;面试题-4：二维数组中的查找-2.4&quot;&gt;面试题 4：二维数组中的查找 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-5：替换空格&quot; data-toc-modified-id=&quot;面试题-5：替换空格-2.5&quot;&gt;面试题 5：替换空格 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-6：从尾到头打印链表&quot; data-toc-modified-id=&quot;面试题-6：从尾到头打印链表-2.6&quot;&gt;面试题 6：从尾到头打印链表 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-7：重建二叉树&quot; data-toc-modified-id=&quot;面试题-7：重建二叉树-2.7&quot;&gt;面试题 7：重建二叉树 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-8：二叉树的下一个节点&quot; data-toc-modified-id=&quot;面试题-8：二叉树的下一个节点-2.8&quot;&gt;面试题 8：二叉树的下一个节点 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-9：用两个栈实现队列&quot; data-toc-modified-id=&quot;面试题-9：用两个栈实现队列-2.9&quot;&gt;面试题 9：用两个栈实现队列 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-10：斐波那契数列&quot; data-toc-modified-id=&quot;面试题-10：斐波那契数列-2.10&quot;&gt;面试题 10：斐波那契数列 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-11：旋转数组的最小数字&quot; data-toc-modified-id=&quot;面试题-11：旋转数组的最小数字-2.11&quot;&gt;面试题 11：旋转数组的最小数字 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-12：矩阵中的路径&quot; data-toc-modified-id=&quot;面试题-12：矩阵中的路径-2.12&quot;&gt;面试题 12：矩阵中的路径 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-13：机器人的运动范围&quot; data-toc-modified-id=&quot;面试题-13：机器人的运动范围-2.13&quot;&gt;面试题 13：机器人的运动范围 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-14：剪绳子&quot; data-toc-modified-id=&quot;面试题-14：剪绳子-2.14&quot;&gt;面试题 14：剪绳子 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-15：二进制中-1-的个数&quot; data-toc-modified-id=&quot;面试题-15：二进制中-1-的个数-2.15&quot;&gt;面试题 15：二进制中 1 的个数 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch5）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch5/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch5/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-26T01:16:56.062Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch4）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch4/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch4/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-23T15:12:17.759Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;toc&quot;&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#第四章：解决面试题的思路&quot; data-toc-modified-id=&quot;第四章：解决面试题的思路-1&quot;&gt;第四章：解决面试题的思路&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-27：二叉树的镜像&quot; data-toc-modified-id=&quot;面试题-27：二叉树的镜像-1.1&quot;&gt;面试题 27：二叉树的镜像&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-28：对称的二叉树&quot; data-toc-modified-id=&quot;面试题-28：对称的二叉树-1.2&quot;&gt;面试题 28：对称的二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-29：顺时针打印矩阵&quot; data-toc-modified-id=&quot;面试题-29：顺时针打印矩阵-1.3&quot;&gt;面试题 29：顺时针打印矩阵&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-30：包含-min-函数的栈&quot; data-toc-modified-id=&quot;面试题-30：包含-min-函数的栈-1.4&quot;&gt;面试题 30：包含 min 函数的栈&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-31：栈的压入、弹出序列&quot; data-toc-modified-id=&quot;面试题-31：栈的压入、弹出序列-1.5&quot;&gt;面试题 31：栈的压入、弹出序列&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-32（一）：不分行从上往下打印二叉树&quot; data-toc-modified-id=&quot;面试题-32（一）：不分行从上往下打印二叉树-1.6&quot;&gt;面试题 32（一）：不分行从上往下打印二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-32（二）：分行从上到下打印二叉树&quot; data-toc-modified-id=&quot;面试题-32（二）：分行从上到下打印二叉树-1.7&quot;&gt;面试题 32（二）：分行从上到下打印二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-32（三）：之字形打印二叉树&quot; data-toc-modified-id=&quot;面试题-32（三）：之字形打印二叉树-1.8&quot;&gt;面试题 32（三）：之字形打印二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-33：二叉搜索树的后序遍历序列&quot; data-toc-modified-id=&quot;面试题-33：二叉搜索树的后序遍历序列-1.9&quot;&gt;面试题 33：二叉搜索树的后序遍历序列&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-34：二叉树中和为某一值的路径&quot; data-toc-modified-id=&quot;面试题-34：二叉树中和为某一值的路径-1.10&quot;&gt;面试题 34：二叉树中和为某一值的路径&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-35：复杂链表的复制&quot; data-toc-modified-id=&quot;面试题-35：复杂链表的复制-1.11&quot;&gt;面试题 35：复杂链表的复制&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-36：二叉搜索树与双向链表&quot; data-toc-modified-id=&quot;面试题-36：二叉搜索树与双向链表-1.12&quot;&gt;面试题 36：二叉搜索树与双向链表&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-37：序列化二叉树&quot; data-toc-modified-id=&quot;面试题-37：序列化二叉树-1.13&quot;&gt;面试题 37：序列化二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-38：字符串的排列&quot; data-toc-modified-id=&quot;面试题-38：字符串的排列-1.14&quot;&gt;面试题 38：字符串的排列&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Few-Shot Charge Prediction with Discriminative Legal Attributes 论文笔记</title>
    <link href="https://www.yam.gift/2019/12/15/Paper/2019-12-15-Label-Attributes-Representation-Paper/"/>
    <id>https://www.yam.gift/2019/12/15/Paper/2019-12-15-Label-Attributes-Representation-Paper/</id>
    <published>2019-12-15T11:00:00.000Z</published>
    <updated>2020-05-03T03:30:51.039Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;http://nlp.csai.tsinghua.edu.cn/~tcc/publications/coling2018_attribute.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;coling2018_attribute.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;code: &lt;a href=&quot;https://github.com/thunlp/attribute_charge&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;thunlp/attribute_charge&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：基于类别属性的注意力机制共同学习属性感知和无属性的文本表示。&lt;/p&gt;
&lt;p&gt;这是 COLING2018 上的一篇老论文了，最近因为一些事情正好遇上，当时大概看了一下就发现这篇文章正好解决了我之前在做多分类&lt;a href=&quot;https://github.com/hscspring/Multi-Label-Text-Classification-for-Chinese#others&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;任务&lt;/a&gt;时没有解决的问题。所以拿来记录一下，顺便研究下代码。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Few-Shot" scheme="https://www.yam.gift/tags/Few-Shot/"/>
    
      <category term="Imbalance Data" scheme="https://www.yam.gift/tags/Imbalance-Data/"/>
    
      <category term="Confusing Labels" scheme="https://www.yam.gift/tags/Confusing-Labels/"/>
    
  </entry>
  
</feed>
