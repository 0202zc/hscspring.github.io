<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2023-10-15T03:42:14.428Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>Yam</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>关于大语言模型的思考</title>
    <link href="https://www.yam.gift/2023/10/15/NLP/2023-10-15-Think-About-LLM/"/>
    <id>https://www.yam.gift/2023/10/15/NLP/2023-10-15-Think-About-LLM/</id>
    <published>2023-10-15T03:00:00.000Z</published>
    <updated>2023-10-15T03:42:14.428Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;从ChatGPT去年11月底发布到现在差不多一年时间了，短短的一年，整个NLP行业发生了翻天覆地的变化。应用方面，整个AI行业甚至其他行业都受到很大冲击，感觉所有人都在+大模型，都在试图重构产品和服务；研究方面，LLM现在几乎成为所有从业人员研究的热点，各种各样的研究成果层出不穷，让人眼花缭乱，直呼看不过来。&lt;/p&gt;
&lt;p&gt;本人作为一名NLP工程师，自然深度参与。从一开始的Prompt技巧，到InstructGPT三阶段训练研究，再到千奇百怪的高效微调、知识编辑，再到各种量化推理、剪枝、小模型实践，再到目前重新思考预训练。这是一个不断深入的过程，也是一个不断学习的过程。从一开始的“我草牛逼”，到“看起来好像不复杂”，再到“咋回事，咋做的，咋这么多坑，咋办”。&lt;/p&gt;
&lt;p&gt;本文主要记录一点当下最新的思考，包括算法和行业两个方面。我会尽量让自己的观点鲜明，不模棱两可。另外，我们也不是搞预测，只是纯粹的分析和感悟，甚至有一些个人偏好。总的来说，都是个人观点，限于能力，不一定准确（很有可能有错误），希望能借此和同好一起讨论。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thingking" scheme="https://www.yam.gift/categories/Thingking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 开发指南：Hugging LLM Hugging Future</title>
    <link href="https://www.yam.gift/2023/04/22/NLP/2023-04-22-ChatGPT-Development/"/>
    <id>https://www.yam.gift/2023/04/22/NLP/2023-04-22-ChatGPT-Development/</id>
    <published>2023-04-22T04:00:00.000Z</published>
    <updated>2023-04-22T09:47:04.534Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文属于DataWhale &lt;a href=&quot;https://github.com/datawhalechina/hugging-llm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hugging-LLM&lt;/a&gt;开源教程介绍内容，详细教程请跳转链接。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;随着ChatGPT的爆火，我们相信未来会有越来越多的大模型及类似OpenAI提供的服务出现，AI 正在逐渐平民化，将来每个人都可以利用大模型轻松地做出自己的AI产品。&lt;/p&gt;
&lt;p&gt;HuggingLLM是一个面向非算法、有一定编程基础、对AI和ChatGPT（或类似模型）感兴趣的，基于ChatGPT API开发相关应用的开源项目。当然部分内容不需要任何编程经验也可以学习，算法工程师也可能从中受益。项目主要包括ChatGPT基础科普、ChatGPT实现各种NLP常见任务（相似匹配、句词分类、编辑生成、推理等大类）、ChatGPT局限和商业应用等内容。&lt;/p&gt;
&lt;p&gt;项目名为 HuggingLLM，因为我们相信正在经历一个伟大的时代，我们相信这是一个值得每个人全身心拥抱的时代，我们更加相信这个世界必将会因此而变得更加美好。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LM" scheme="https://www.yam.gift/tags/LM/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
      <category term="HuggingLLM" scheme="https://www.yam.gift/tags/HuggingLLM/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 基础科普：知其一点所以然</title>
    <link href="https://www.yam.gift/2023/04/15/NLP/2023-04-15-ChatGPT-Introduction/"/>
    <id>https://www.yam.gift/2023/04/15/NLP/2023-04-15-ChatGPT-Introduction/</id>
    <published>2023-04-15T10:00:00.000Z</published>
    <updated>2023-04-15T13:01:05.465Z</updated>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文属于&lt;a href=&quot;https://datawhale.club/#/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DataWhale&lt;/a&gt;开源组织&lt;a href=&quot;https://github.com/datawhalechina/hugging-llm.git&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;HuggingLLM&lt;/a&gt;开源项目内容，更多内容请移步开源项目。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;2022年底，ChatGPT突然间在AI圈开始流行，准确来说是搞自然语言处理（Natural Language Processing，NLP）圈子里先火起来了。遥想当时，本以为就会在圈内火一阵，结果现在……没想到居然成了AI的救命稻草，当然对AI工程师尤其是NLP工程师是什么就不好说了。海内外沸腾之后就是第一时间的跟进，结果自然是努力对齐而无功，连牛逼的Google和FaceBook都翻车了。不过总归是折腾出来一些东西，大伙儿也都有了目标，圈子又有了新活力。希望OpenAI继续发力，我等再多苟些日子。&lt;/p&gt;
&lt;p&gt;无论是ChatGPT还是后来的模仿者，它们其实都是语言模型，准确来说——大语言模型。使用时，无论是调用API还是开源项目，总有一些参数可能需要调整。对大部分内行人士来说应该都不成问题，但对外行就有点玄乎了。基于此，本文将简要介绍ChatGPT相关技术基本原理，行文将站在外行人角度，尝试将内容尽量平民化。虽然不能深入细节，但知晓原理足以很好使用了。&lt;/p&gt;
&lt;p&gt;本文共分为四个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LM：这是ChatGPT的基石的基石，是一个最基本的概念，绕不开，逃不过，没办法。&lt;/li&gt;
&lt;li&gt;Transformer：这是ChatGPT的基石，准确来说它的一部分是基石。&lt;/li&gt;
&lt;li&gt;GPT：本体，从GPT-1，一直到现在的GPT-4，按openai自己的说法，那模型都是那个模型，只是它长大了，变胖了，不过更好看了。关于这点，大家基本都没想到。现在好了，攀不上了。&lt;/li&gt;
&lt;li&gt;RLHF：ChatGPT神兵利器，有此利刃，ChatGPT才是那个ChatGPT，不然就只能是GPT-3。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="GPT-2" scheme="https://www.yam.gift/tags/GPT-2/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="LM" scheme="https://www.yam.gift/tags/LM/"/>
    
      <category term="InstructGPT" scheme="https://www.yam.gift/tags/InstructGPT/"/>
    
      <category term="GPT-3" scheme="https://www.yam.gift/tags/GPT-3/"/>
    
      <category term="GPT-1" scheme="https://www.yam.gift/tags/GPT-1/"/>
    
      <category term="RLHF" scheme="https://www.yam.gift/tags/RLHF/"/>
    
      <category term="LLM" scheme="https://www.yam.gift/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>「+AI」需要什么？</title>
    <link href="https://www.yam.gift/2023/02/27/AI/2023-02-27-Enpower-AI/"/>
    <id>https://www.yam.gift/2023/02/27/AI/2023-02-27-Enpower-AI/</id>
    <published>2023-02-27T15:07:19.000Z</published>
    <updated>2023-09-11T00:02:44.606Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;随着互联网业务到达天花板，与其相关的 AI 算法也开始逐渐变得寒气逼人。说到底，AI 还是个工具，即便它是非常了不得的工具，但毕竟大部分时候也没法脱离业务存在。二十大报告也提到要「脱虚向实」，进一步壮大、升级实体经济。所以，现在社会以及更多的人开始思考如何利用 AI 为行业赋能。换句通俗的话说，卷死互联网，现在来卷其他行业了。虽说几乎任何行业都可以通过「+AI」受益，但这中间有些业务和公司可能比较特殊，不太适合或无法 +AI。所以，本文就简单探讨下要想 +AI 究竟需要什么条件。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="Business" scheme="https://www.yam.gift/tags/Business/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 影响冲击：职业、行业与产业</title>
    <link href="https://www.yam.gift/2023/02/21/NLP/2023-02-21-ChatGPT-Impact/"/>
    <id>https://www.yam.gift/2023/02/21/NLP/2023-02-21-ChatGPT-Impact/</id>
    <published>2023-02-21T15:30:00.000Z</published>
    <updated>2023-09-10T00:41:41.818Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;2022 年末的时候，ChatGPT 横空出世，朋友第一时间关注试玩后与我分享。当时听他说效果很好，不过我并没有特别放心上，毕竟，对话机器人已经不知道来过多少轮了，都快麻了。直到过了几天他给我看了一个非常亮眼的 Case——把我们平时工作中的业务文本直接丢给 ChatGPT，让它做实体抽取任务。结果完成的非常好，甚至可以按照指定的格式（如 Json）输出，而且如果你再告诉它一些特有规则，它还能进一步提取。这就很厉害了，至少之前的对话机器人可做不到这点。于是赶紧关注起来，先看介绍——哇靠，居然有强化学习（个人兴趣，一直比较关注强化学习在 NLP 方面的应用【相关文献1和2】），顿时来了兴趣——再看，发现 InstructGPT 这篇 Paper 在 11 月已经读过了，顿时恍然——原来是这篇。然后就上淘宝买了个账号开始玩起来，玩着玩着就感觉到这东西对 NLP 这个职业的冲击，但当时并没有想到它能出圈，能影响整个行业甚至产业。&lt;/p&gt;
&lt;p&gt;过年的时候，在用它写春节祝福时发现 Prompt 技能不够用了，搜了一下才发现是自己狭隘了，于是赶紧补充了一波，写下了这篇 Prompt 工程：&lt;a href=&quot;https://yam.gift/2023/01/25/NLP/2023-01-25-ChatGPT-Prompt-Engineering/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ChatGPT Prompt 工程：设计、实践与思考 | Yam&lt;/a&gt;。然后，我感觉好像应该差不多了吧，没想到，一切才刚刚开始……现在，大家都知道了……在 Prompt 工程中，我在文末写到：“本想继续谈谈关于 ChatGPT 对 NLP 行业甚至 AI 领域的影响，以及是否马上就会出现强 AI，以及与此相关的影响等，由于与本文主旨关系不大，我将择文再议”。其实后面一直想写，只不过因为要研究 ChatGPT 的实现和&lt;a href=&quot;https://yam.gift/2023/02/19/NLP/2023-02-19-ChatGPT-Labeling/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;标注&lt;/a&gt;，所以耽搁到现在，现在总算可以把这个坑给填上，只不过我把影响范围进一步扩大了——到产业级别。&lt;/p&gt;
&lt;p&gt;本文主要就 ChatGPT 对职业、行业和产业的影响展开讨论，为了避免被其他信息影响，最近一段时间几乎没看（刻意为之）类似新闻或文章，所以内容更多会偏主观，仅供参考。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT 标注指南：任务、数据与规范</title>
    <link href="https://www.yam.gift/2023/02/19/NLP/2023-02-19-ChatGPT-Labeling/"/>
    <id>https://www.yam.gift/2023/02/19/NLP/2023-02-19-ChatGPT-Labeling/</id>
    <published>2023-02-19T15:30:00.000Z</published>
    <updated>2023-02-19T14:23:17.132Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;ChatGPT 刚刚出来时，业内人士一致认为高质量的数据是一个非常关键的因素。且不论这个结论在 ChatGPT 这里是否正确，但高质量的数据对模型大有裨益却是公认的。而且，我们也可以从公开的 InstructGPT 标注指南中对此窥探一二。本文主要就围绕这份指南进行介绍，有点标题党了，但是考虑到 ChatGPT 和 InstructGPT 是兄弟关系，我们有理由相信 ChatGPT 的标注也是基于 InstructGPT 给出的指南进行的。当然不一定是全部，但至少我们可以从中学习和借鉴一些东西，是有此文。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Prompt" scheme="https://www.yam.gift/tags/Prompt/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="Labeling" scheme="https://www.yam.gift/tags/Labeling/"/>
    
      <category term="InstructGPT" scheme="https://www.yam.gift/tags/InstructGPT/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT Prompt 示例</title>
    <link href="https://www.yam.gift/2023/01/31/NLP/2023-01-31-ChatGPT-Prompt-Example/"/>
    <id>https://www.yam.gift/2023/01/31/NLP/2023-01-31-ChatGPT-Prompt-Example/</id>
    <published>2023-01-31T15:00:00.000Z</published>
    <updated>2023-05-24T07:58:59.686Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;下面的 Case 主要收集自网络，我会在后面添加上出处。关于 Prompt 设计技巧可以参考之前的文章：&lt;a href=&quot;https://yam.gift/2023/01/25/NLP/2023-01-25-ChatGPT-Prompt-Engineering/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ChatGPT Prompt 工程：设计、实践与思考 | Yam&lt;/a&gt;，这里面的一些代表性 Case 也挪过来了。&lt;/p&gt;
&lt;p&gt;特别说明：我们还是尽量从「设计」的角度给出 Case，而不是任务或内容。&lt;/p&gt;
&lt;p&gt;另外需要说明：经测试，有些在中文下效果不如英文好（英文 Prompt 中文版本都是 ChatGPT 翻译的）。目前已有 Case 如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接问答、解释（不需要设计）&lt;/li&gt;
&lt;li&gt;扮演互动&lt;/li&gt;
&lt;li&gt;扮演+任务+步骤+上下文+目标+格式&lt;/li&gt;
&lt;li&gt;使用扮演绕过限制&lt;/li&gt;
&lt;li&gt;目标+上下文+多任务&lt;/li&gt;
&lt;li&gt;标题+指定对象+任务&lt;/li&gt;
&lt;li&gt;合作创作&lt;/li&gt;
&lt;li&gt;表格转换&lt;/li&gt;
&lt;li&gt;简化长 Prompt&lt;/li&gt;
&lt;li&gt;综合多个结果&lt;/li&gt;
&lt;li&gt;创造力增强&lt;/li&gt;
&lt;li&gt;游戏引擎&lt;/li&gt;
&lt;li&gt;推荐&lt;/li&gt;
&lt;li&gt;思维树&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Prompt" scheme="https://www.yam.gift/tags/Prompt/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
  </entry>
  
  <entry>
    <title>语言模型级联</title>
    <link href="https://www.yam.gift/2023/01/27/Paper/2023-01-27-Language-Model-Cascades/"/>
    <id>https://www.yam.gift/2023/01/27/Paper/2023-01-27-Language-Model-Cascades/</id>
    <published>2023-01-27T15:00:00.000Z</published>
    <updated>2023-01-29T07:19:21.140Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;这是一篇来自 Google 的研究结果，通过一定的方法和策略，比如多个预训练模型结合，进一步提升模型整体推理能力。本文主要是对这方面的研究做了一个整体统一的划分，包括：思维链（Chain-of-Thought），验证器（Verifiers, STaR）选择-推理（Selection-Inference），工具使用（Tool Use）等，这些统称为：语言模型级联（Language Model Cascades）。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Cascades" scheme="https://www.yam.gift/tags/Cascades/"/>
    
      <category term="CoT" scheme="https://www.yam.gift/tags/CoT/"/>
    
      <category term="Verifier" scheme="https://www.yam.gift/tags/Verifier/"/>
    
      <category term="STaR" scheme="https://www.yam.gift/tags/STaR/"/>
    
      <category term="Selection-Inference" scheme="https://www.yam.gift/tags/Selection-Inference/"/>
    
  </entry>
  
  <entry>
    <title>ChatGPT Prompt工程：设计、实践与思考</title>
    <link href="https://www.yam.gift/2023/01/25/NLP/2023-01-25-ChatGPT-Prompt-Engineering/"/>
    <id>https://www.yam.gift/2023/01/25/NLP/2023-01-25-ChatGPT-Prompt-Engineering/</id>
    <published>2023-01-25T15:30:00.000Z</published>
    <updated>2023-03-13T07:19:36.321Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;ChatGPT 火爆出圈了，有些人惊叹于它的能力，当然也有部分人觉得也就那样。这就不得不提 Prompt 了，据说【相关文献1】，ChatGPT 效果好不好完全取决于你的 Prompt，“看来 Propmt 也成一个技术活儿了”。当我这么想的时候，没想到国外居然已经有了成熟的售卖 Prompt 的&lt;a href=&quot;https://promptbase.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;网站&lt;/a&gt;，这玩意儿居然成了 NFT（Non-Fungible Token），真是世界变化太快，本人过于迟钝。&lt;/p&gt;
&lt;p&gt;其实，对于 ChatGPT 的能力，作为 NLPer 第一时间就领教过了，作为行业内人士，而且多年来一直关注文本生成领域，ChatGPT 带给我的冲击和震撼是非常大的，甚至那几天晚上连觉都睡不着，真是焦虑感爆棚。记得在 &lt;a href=&quot;https://www.datawhale.club/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DataWhale&lt;/a&gt; 团队群里一次讨论 ChatGPT 时，我发过这样的消息，原话如下：&lt;/p&gt;
&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;NLP真的考虑要转行了&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ChatGPT已经抹平了任务、行业、语言&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;以后也不用分那么多task榜单了，不用管行业&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;强大的一批，LM as SAAS 将统治一切&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;LM as SAAS，其实应该是 LMAS——Language Model as Service。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;过了几天看到这篇文章：&lt;a href=&quot;https://mp.weixin.qq.com/s/1HZoNBovqn1FNlxghDXMFg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ChatGPT 会对未来 5 年的 NLP 算法从业者带来怎样的冲击？&lt;/a&gt;发现业内大家也是差不多的想法（虽然我发消息在这篇文章之后，但之前的确没看过），尤其是张俊林博士的观点个人比较认同，NLP 工程师的确不容乐观。这里不是说这个职业的职责不容乐观，而是说整个行业可能会受到冲击。&lt;/p&gt;
&lt;p&gt;有点跑偏了，说回 Prompt，春节时就想用 ChatGPT 生成一些祝福语，突然发现自己掌握的 Prompt 出来的效果不太好了。Google 了一下结果就发现了 &lt;a href=&quot;https://fka.gumroad.com/l/art-of-chatgpt-prompting&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The Art of ChatGPT Prompting: A Guide to Crafting Clear and Effective Prompts&lt;/a&gt; 这本电子书，再一搜，发现这个领域居然已经发展到如斯境地。本着学习的心态，阅读整理了一些 Prompt 工程的资料（见《文献和参考——核心文献》），是有此文。本文主要介绍关于 ChatGPT Prompt 的方法，我会结合这些资料加上自己的理解写出来，同时会在中文环境下做相关试验。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Few-Shot" scheme="https://www.yam.gift/tags/Few-Shot/"/>
    
      <category term="Prompt" scheme="https://www.yam.gift/tags/Prompt/"/>
    
      <category term="Zero-Shot" scheme="https://www.yam.gift/tags/Zero-Shot/"/>
    
      <category term="In-Context Learning" scheme="https://www.yam.gift/tags/In-Context-Learning/"/>
    
      <category term="One-Shot" scheme="https://www.yam.gift/tags/One-Shot/"/>
    
      <category term="ChatGPT" scheme="https://www.yam.gift/tags/ChatGPT/"/>
    
      <category term="Instruct" scheme="https://www.yam.gift/tags/Instruct/"/>
    
  </entry>
  
  <entry>
    <title>Put Human in NLP Loop</title>
    <link href="https://www.yam.gift/2023/01/21/Paper/2023-01-21-Human-in-Loop/"/>
    <id>https://www.yam.gift/2023/01/21/Paper/2023-01-21-Human-in-Loop/</id>
    <published>2023-01-21T15:00:00.000Z</published>
    <updated>2023-01-23T04:49:57.300Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章基于一篇 Survey（见核心文献），就是如何将人类的反馈放在 NLP 过程中，这个过程可以是训练，也可以是部署后，也可以是数据标注阶段。总之，它泛指把人类的反馈与 NLP 过程结合。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Human-in-Loop" scheme="https://www.yam.gift/tags/Human-in-Loop/"/>
    
      <category term="Human-in-the-Loop" scheme="https://www.yam.gift/tags/Human-in-the-Loop/"/>
    
  </entry>
  
  <entry>
    <title>人生随笔</title>
    <link href="https://www.yam.gift/2023/01/21/Diary/2023-01-21-Life/"/>
    <id>https://www.yam.gift/2023/01/21/Diary/2023-01-21-Life/</id>
    <published>2023-01-21T00:00:00.000Z</published>
    <updated>2023-01-25T05:13:51.470Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;今天是除夕，一大早因为一个梦醒来了，不是噩梦，当然也不是美梦。要搁平时，肯定继续睡了，但上完厕所发现居然毫无睡意，而且很想写点什么。至于原因，好像也没什么原因，可能是近期一直想写点东西，也可能是马上又过完了一年，还有可能是年纪见长容易感慨。总之，好似有方向，但又无目的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Life" scheme="https://www.yam.gift/tags/Life/"/>
    
      <category term="Growth" scheme="https://www.yam.gift/tags/Growth/"/>
    
  </entry>
  
  <entry>
    <title>GPT3 和它的 In-Context Learning</title>
    <link href="https://www.yam.gift/2023/01/20/NLP/2023-01-20-GPT3/"/>
    <id>https://www.yam.gift/2023/01/20/NLP/2023-01-20-GPT3/</id>
    <published>2023-01-20T15:30:00.000Z</published>
    <updated>2023-01-21T02:06:39.500Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;ChatGPT 的爆火让很多 NLPer 大吃一惊，焦虑感爆棚，它的思路和方法都不复杂，但效果却出奇的好。我想任何研究成果的爆发都不可能是一蹴而就的，期间必然包含这一系列小的创新和优化。于是，重新把 GPT3 的 Paper 拉出来读了一遍，重点关注了实验结果之外的东西，果然发现不少细节。因此，本文还是以 GPT3 为主。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Few-Shot" scheme="https://www.yam.gift/tags/Few-Shot/"/>
    
      <category term="Zero-Shot" scheme="https://www.yam.gift/tags/Zero-Shot/"/>
    
      <category term="GPT3" scheme="https://www.yam.gift/tags/GPT3/"/>
    
      <category term="In-Context Learning" scheme="https://www.yam.gift/tags/In-Context-Learning/"/>
    
      <category term="One-Shot" scheme="https://www.yam.gift/tags/One-Shot/"/>
    
  </entry>
  
  <entry>
    <title>IDE Memo</title>
    <link href="https://www.yam.gift/2022/11/15/Unix/2022-11-15-IDE/"/>
    <id>https://www.yam.gift/2022/11/15/Unix/2022-11-15-IDE/</id>
    <published>2022-11-15T15:00:00.000Z</published>
    <updated>2022-11-15T15:55:54.750Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;IDE 相关备忘。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="IDE" scheme="https://www.yam.gift/tags/IDE/"/>
    
      <category term="vim" scheme="https://www.yam.gift/tags/vim/"/>
    
      <category term="emacs" scheme="https://www.yam.gift/tags/emacs/"/>
    
  </entry>
  
  <entry>
    <title>W2NER解读</title>
    <link href="https://www.yam.gift/2022/10/30/Paper/2022-10-30-W2NER-Study/"/>
    <id>https://www.yam.gift/2022/10/30/Paper/2022-10-30-W2NER-Study/</id>
    <published>2022-10-30T15:00:00.000Z</published>
    <updated>2022-10-30T02:34:57.500Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;NER 任务主要有三种类型：Flat（平铺）、overlapped（重叠或嵌套）、discontinuous（不连续），越来越多的研究致力于将它们统一起来。当前的 STOA 主要包括基于 Span 和 Seq2Seq 模型，不过它们很少关注边界，可能会导致后续的偏移。本文提出的统一方法（W2NER）是将其视为词词关系分类，为此引入两种词词关系：&lt;code&gt;NNW&lt;/code&gt;（&lt;code&gt;Next-Neighboring-Word&lt;/code&gt;）和 &lt;code&gt;THW-*&lt;/code&gt;（&lt;code&gt;Tail-Head-Word-*&lt;/code&gt;）。具体而言，构造一个 2D 的词词关系网格，然后使用多粒度 2D 卷积，以更好地细化网格表示。最后，使用一个共同预测器来推理词-词关系。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="NER" scheme="https://www.yam.gift/tags/NER/"/>
    
      <category term="W2NER" scheme="https://www.yam.gift/tags/W2NER/"/>
    
      <category term="NNW" scheme="https://www.yam.gift/tags/NNW/"/>
    
      <category term="THW" scheme="https://www.yam.gift/tags/THW/"/>
    
      <category term="Span" scheme="https://www.yam.gift/tags/Span/"/>
    
      <category term="BIO" scheme="https://www.yam.gift/tags/BIO/"/>
    
      <category term="BIOHD" scheme="https://www.yam.gift/tags/BIOHD/"/>
    
  </entry>
  
  <entry>
    <title>《麦肯锡战略化思维》读书笔记</title>
    <link href="https://www.yam.gift/2022/10/23/CogPsy/2022-10-23-Structured-Strategic-Thought/"/>
    <id>https://www.yam.gift/2022/10/23/CogPsy/2022-10-23-Structured-Strategic-Thought/</id>
    <published>2022-10-23T15:00:00.000Z</published>
    <updated>2022-10-31T12:10:54.900Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;很多职场人士会处于 PAID（Pressured，Action Addicted，Information Overload and Distracted，即压力、没时间思考、信息超载和无法专注）亚健康状态。理性思考、理性办事是我们应该有的习惯，但首先要走出心理上的舒适区和大脑快速思考的本能。本书介绍的结构化战略思维能让我们更加理性，对抗焦虑。前面部分是简化记录，后文是较详细记录。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;四大原则&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数字说话（行为准则）&lt;/li&gt;
&lt;li&gt;洞见优于表象（行为准则）&lt;/li&gt;
&lt;li&gt;MECE 原则（方法论）&lt;/li&gt;
&lt;li&gt;假设为前提（方法论）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;结构化战略思维框架&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;战略思维定义正确的问题&lt;ul&gt;
&lt;li&gt;SMART 原则&lt;/li&gt;
&lt;li&gt;【六步】：背景、成功标准、边界、限制条件、责任人/相关人、资源&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;结构化分析（原则13）&lt;ul&gt;
&lt;li&gt;3-3 原则四种【切】法：公式法、子目录列举法、流程法和逻辑框架法&lt;/li&gt;
&lt;li&gt;单维、多维【图谱】：产品 BCG、客户消费者感知图、项目优先级分析、品类拓展分析&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;头脑风暴提出假设（原则24）&lt;/li&gt;
&lt;li&gt;调研验证假设（原则12）&lt;/li&gt;
&lt;li&gt;交付沟通（原则1-4）&lt;ul&gt;
&lt;li&gt;【故事线】串联核心要素：Why 为什么、What 用什么、How 怎么解决、Why 为什么是你、HowMuch 投入产出&lt;/li&gt;
&lt;li&gt;【SCP-I 框架】描述行业现状：S（规则）描述行业整体商业模式、C（行为）描述主流商业模式和竞争策略、P（业绩）包括财务和非财务&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Strategic" scheme="https://www.yam.gift/tags/Strategic/"/>
    
      <category term="Thought" scheme="https://www.yam.gift/tags/Thought/"/>
    
  </entry>
  
  <entry>
    <title>Global Pointer：Novel Efficient Span-based Approach for NER</title>
    <link href="https://www.yam.gift/2022/10/16/Paper/2022-10-16-GlobalPointer/"/>
    <id>https://www.yam.gift/2022/10/16/Paper/2022-10-16-GlobalPointer/</id>
    <published>2022-10-16T15:00:00.000Z</published>
    <updated>2022-10-16T14:50:04.100Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2208.03054&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2208.03054] Global Pointer: Novel Efficient Span-based Approach for Named Entity Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/bojone/bert4keras&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/bojone/bert4keras&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概述：全局指针识别 NER，Span 预测得到 NER 类型。&lt;/p&gt;
&lt;p&gt;摘要：NER 任务是从一段文本中识别出预先定义的语义实体类型。SOTA 方案通常会因为捕获底层文本的细粒度语义信息而受到影响。基于 Span 的方法克服了这种缺陷，但性能是个问题。本文提出基于 Span 的 NER 框架——全局指针（GP），通过乘法注意力机制利用相对位置，目标是考虑开始和结束位置的全局视图来预测实体。除了设计了两个模块（Token 表征和 Span 预测）来识别实体外，还引入了一种新的损失函数来解决标签不均衡问题，另外还引入了一种简单有效的近似方法减少参数。实验表明 GP 胜过现有方案，此外损失函数也表现出有效性。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="NER" scheme="https://www.yam.gift/tags/NER/"/>
    
      <category term="Span" scheme="https://www.yam.gift/tags/Span/"/>
    
      <category term="GP" scheme="https://www.yam.gift/tags/GP/"/>
    
      <category term="Global Pointer" scheme="https://www.yam.gift/tags/Global-Pointer/"/>
    
      <category term="Class Imbalance Loss" scheme="https://www.yam.gift/tags/Class-Imbalance-Loss/"/>
    
  </entry>
  
  <entry>
    <title>DeepGen：Diverse Search Ad Generation and Real-Time Customization</title>
    <link href="https://www.yam.gift/2022/10/15/Paper/2022-10-15-DeepGen/"/>
    <id>https://www.yam.gift/2022/10/15/Paper/2022-10-15-DeepGen/</id>
    <published>2022-10-15T15:00:00.000Z</published>
    <updated>2022-10-16T12:19:36.800Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2208.03438&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2208.03438] DeepGen: Diverse Search Ad Generation and Real-Time Customization&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：无&lt;/p&gt;
&lt;p&gt;一句话概述：端到端广告文本生成方案。&lt;/p&gt;
&lt;p&gt;摘要：DeepGen 是一个 Web 部署的用于为 Bing 广告客户自动生成搜索广告的系统。它使用最新的 NLG 模型从广告商的网页生成流畅的广告，并解决一些实际问题（真实性、推理速度）。系统会根据用户的搜索查询实时创建定制化广告，从而根据用户「正在寻找的内容」突出显示同一产品的不同方面。为了实现此目标，系统提前生成各种可选择的小广告片段素材，查询时选择最相关的拼接到完整广告中。通过训练可控 NLG 模型为同一网页生成多个广告，突出不同卖点，从而提高生成的多样性。更进一步，通过首先运行使用不同目标训练的生成模型集合，然后使用多样性采样算法选择不同生成结果子集进行在线选择，进一步横向提升了多样性。实验结果验证了系统设计的有效性，目前已部署生产环境，提供了必应投放的大约 4% 的全球广告。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="NLG" scheme="https://www.yam.gift/tags/NLG/"/>
    
      <category term="DeepGen" scheme="https://www.yam.gift/tags/DeepGen/"/>
    
  </entry>
  
  <entry>
    <title>只如初见的不只爱情</title>
    <link href="https://www.yam.gift/2022/09/11/Diary/2022-09-11-Passion/"/>
    <id>https://www.yam.gift/2022/09/11/Diary/2022-09-11-Passion/</id>
    <published>2022-09-11T15:00:00.000Z</published>
    <updated>2022-09-11T15:51:39.770Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;很久没有静心写一些文字了，回顾近大半年，感觉好像每天都在忙，时刻都有未做完的任务，时间就这样一点点慢慢流逝，安静的让人毫无知觉。&lt;/p&gt;
&lt;p&gt;今天是 2022 年中秋假的第二天，第一天睡了大半天，第二天浑浑噩噩待了近一个白天，大脑完全不想动，只想着到处刷刷，随便刷什么。这不是在打发时间，只是一种大脑潜意识或有意识地在「放纵」，提不起精气神干任何该干的事。躺着刷手机到下午 6 点多，突然觉得应该下楼走走，于是一边遛狗，一边慢慢开始自己与自己的对话。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="Diary" scheme="https://www.yam.gift/tags/Diary/"/>
    
      <category term="Passion" scheme="https://www.yam.gift/tags/Passion/"/>
    
      <category term="Life" scheme="https://www.yam.gift/tags/Life/"/>
    
      <category term="Growth" scheme="https://www.yam.gift/tags/Growth/"/>
    
  </entry>
  
  <entry>
    <title>FLAN：Fine-tuned Language Models are Zero-Shot Learners</title>
    <link href="https://www.yam.gift/2022/08/28/Paper/2022-08-28-FLAN/"/>
    <id>https://www.yam.gift/2022/08/28/Paper/2022-08-28-FLAN/</id>
    <published>2022-08-28T15:00:00.000Z</published>
    <updated>2022-08-28T09:47:47.470Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/abs/2109.01652&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[2109.01652] Finetuned Language Models Are Zero-Shot Learners&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/google-research/flan&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/google-research/flan&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一句话概述：指示微调赋予 MTL Zero-Shot 能力。&lt;/p&gt;
&lt;p&gt;摘要：本文探索了一种简单的方法来提升语言模型的 Zero-Shot 能力——指示（或指令）微调（instruction tuning），在一组通过指示描述的数据集上对语言模型微调，大大提高了在未见过任务上的 Zero-Shot 能力。模型 137B，在超过 60 个使用描述模板描述的数据集上微调。FLAN 在 20/25 个任务上超过了 175B 的 GPT3，Few-Shot 能力也大部分超过了 GPT3。消融实结果发现，微调的数据集数量、模型规模、指示，这三个因素是指示微调的关键。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="MTL" scheme="https://www.yam.gift/tags/MTL/"/>
    
      <category term="Prompt" scheme="https://www.yam.gift/tags/Prompt/"/>
    
      <category term="FLAN" scheme="https://www.yam.gift/tags/FLAN/"/>
    
      <category term="NLG" scheme="https://www.yam.gift/tags/NLG/"/>
    
      <category term="Zero-Shot" scheme="https://www.yam.gift/tags/Zero-Shot/"/>
    
  </entry>
  
  <entry>
    <title>W2NER 代码</title>
    <link href="https://www.yam.gift/2022/07/17/Paper/2022-07-17-W2NER-Code/"/>
    <id>https://www.yam.gift/2022/07/17/Paper/2022-07-17-W2NER-Code/</id>
    <published>2022-07-17T15:00:00.000Z</published>
    <updated>2022-07-17T14:23:23.230Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文主要讲述 W2NER 的代码，关于论文相关部分可阅读：&lt;a href=&quot;https://yam.gift/2022/06/11/Paper/2022-06-11-W2NER/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;统一NER为词词关系分类 | Yam&lt;/a&gt;。代码主要包括：输入、训练输出和解码部分，对于模型部分可参考前面的链接。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="AI" scheme="https://www.yam.gift/tags/AI/"/>
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="NER" scheme="https://www.yam.gift/tags/NER/"/>
    
      <category term="W2NER" scheme="https://www.yam.gift/tags/W2NER/"/>
    
      <category term="NNW" scheme="https://www.yam.gift/tags/NNW/"/>
    
      <category term="THW" scheme="https://www.yam.gift/tags/THW/"/>
    
  </entry>
  
</feed>
