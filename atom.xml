<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yam</title>
  <icon>https://www.yam.gift/icon.png</icon>
  <subtitle>Feeling, Coding, Thinking</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.yam.gift/"/>
  <updated>2020-04-23T15:18:05.796Z</updated>
  <id>https://www.yam.gift/</id>
  
  <author>
    <name>Yam</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer Understanding</title>
    <link href="https://www.yam.gift/2020/04/23/Paper/2020-04-23-Transformer/"/>
    <id>https://www.yam.gift/2020/04/23/Paper/2020-04-23-Transformer/</id>
    <published>2020-04-23T15:00:00.000Z</published>
    <updated>2020-04-23T15:18:05.796Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;之前写过一篇关于 &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention Is All You Need&lt;/a&gt; 的 &lt;a href=&quot;https://yam.gift/2019/08/04/Paper/2019-08-04-Attention-Is-All-You-Need/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文笔记&lt;/a&gt;，不过那时候写的笔记都没有深入 Code 环节，再加上其实已经有了一篇 &lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The Annotated Transformer&lt;/a&gt;，也没必要做重复工作。不过现在 Transformer 已经大放异彩到几乎成为了标准配件，所以觉得有必要单独拿出来就组件角度再次学习一遍，于是就有了这篇文章。&lt;/p&gt;
&lt;p&gt;本文代码主要基于 &lt;a href=&quot;https://github.com/OpenNMT/OpenNMT-py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;OpenNMT&lt;/a&gt;，另外也参考了一点 &lt;a href=&quot;https://github.com/pytorch/fairseq&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;fairseq&lt;/a&gt;，这俩都是 PyTorch 实现的。Tensorflow 实现的版本相对更多一些，详见 Appendix 部分。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Attention" scheme="https://www.yam.gift/tags/Attention/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="Self-Attention" scheme="https://www.yam.gift/tags/Self-Attention/"/>
    
      <category term="Multi-Head Attention" scheme="https://www.yam.gift/tags/Multi-Head-Attention/"/>
    
      <category term="Encoder" scheme="https://www.yam.gift/tags/Encoder/"/>
    
      <category term="Decoder" scheme="https://www.yam.gift/tags/Decoder/"/>
    
  </entry>
  
  <entry>
    <title>Luong Attention Understanding</title>
    <link href="https://www.yam.gift/2020/04/14/Paper/2020-04-14-Luong-Attention/"/>
    <id>https://www.yam.gift/2020/04/14/Paper/2020-04-14-Luong-Attention/</id>
    <published>2020-04-14T04:00:00.000Z</published>
    <updated>2020-04-14T04:27:54.568Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/pdf/1508.04025v5.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1508.04025v5.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：文章未提供，见 Appendix&lt;/p&gt;
&lt;p&gt;核心思想：通过在 Decoder 的每一步使用 Encoder 信息，并对 Encoder 信息赋予不同权重来获得更好的 Decoder 结果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Attention" scheme="https://www.yam.gift/tags/Attention/"/>
    
      <category term="Luong Attention" scheme="https://www.yam.gift/tags/Luong-Attention/"/>
    
  </entry>
  
  <entry>
    <title>GPT-2 Understanding</title>
    <link href="https://www.yam.gift/2020/04/07/Paper/2020-04-07-GPT2/"/>
    <id>https://www.yam.gift/2020/04/07/Paper/2020-04-07-GPT2/</id>
    <published>2020-04-07T04:00:00.000Z</published>
    <updated>2020-04-11T03:30:19.875Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/openai/gpt-2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;openai/gpt-2: Code for the paper “Language Models are Unsupervised Multitask Learners”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/minimaxir/gpt-2-simple&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;minimaxir/gpt-2-simple: Python package to easily retrain OpenAI’s GPT-2 text-generating model on new texts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;核心思想：基于 Transformer 的更加 General 的语言模型。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="GPT-2" scheme="https://www.yam.gift/tags/GPT-2/"/>
    
      <category term="Language Model" scheme="https://www.yam.gift/tags/Language-Model/"/>
    
  </entry>
  
  <entry>
    <title>node2vec, Scalable Feature Learning for Networks Note</title>
    <link href="https://www.yam.gift/2020/03/30/Paper/2020-03-30-Node2Vec/"/>
    <id>https://www.yam.gift/2020/03/30/Paper/2020-03-30-Node2Vec/</id>
    <published>2020-03-30T15:55:00.000Z</published>
    <updated>2020-03-30T15:59:45.259Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper：&lt;a href=&quot;https://arxiv.org/pdf/1607.00653.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;node2vec: Scalable Feature Learning for Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code：&lt;a href=&quot;https://github.com/aditya-grover/node2vec&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;aditya-grover/node2vec&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：通过给网络节点的邻居定义一个灵活的概念，并设计了一个能够有效探索邻居多样性的有偏随机游走程序，来学习网络的节点表征。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Graph" scheme="https://www.yam.gift/tags/Graph/"/>
    
      <category term="node2vec" scheme="https://www.yam.gift/tags/node2vec/"/>
    
      <category term="DeepGraph" scheme="https://www.yam.gift/tags/DeepGraph/"/>
    
  </entry>
  
  <entry>
    <title>TextRank Keyword Extraction</title>
    <link href="https://www.yam.gift/2020/03/21/Paper/2020-03-21-Text-Rank/"/>
    <id>https://www.yam.gift/2020/03/21/Paper/2020-03-21-Text-Rank/</id>
    <published>2020-03-21T10:00:00.000Z</published>
    <updated>2020-03-29T12:23:27.847Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;论文：&lt;a href=&quot;https://www.aclweb.org/anthology/W04-3252.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TextRank: Bringing Order into Texts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;代码：&lt;a href=&quot;https://github.com/networkx/networkx/blob/master/networkx/algorithms/link_analysis/pagerank_alg.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;networkx/pagerank_alg.py at master · networkx/networkx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：TextRank 是基于 Google PageRank 的一种关键词（句子）提取方法，它的本质是对文本 Token 按窗口构建节点和边（实际为节点在一定窗口范围内的共现关系），根据 PageRank 得到节点的 Score 排序。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Graph" scheme="https://www.yam.gift/tags/Graph/"/>
    
      <category term="Keyword" scheme="https://www.yam.gift/tags/Keyword/"/>
    
      <category term="TextRank" scheme="https://www.yam.gift/tags/TextRank/"/>
    
      <category term="PageRank" scheme="https://www.yam.gift/tags/PageRank/"/>
    
  </entry>
  
  <entry>
    <title>Neural Machine Tanslation By Jointly Learning To Align And Translate Note</title>
    <link href="https://www.yam.gift/2020/02/08/Paper/2020-02-08-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/"/>
    <id>https://www.yam.gift/2020/02/08/Paper/2020-02-08-Neural-Machine-Translation-By-Jointly-Learning-To-Align-And-Translate/</id>
    <published>2020-02-08T09:00:00.000Z</published>
    <updated>2020-04-14T04:31:59.919Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;paper: &lt;a href=&quot;https://arxiv.org/pdf/1409.0473.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1409.0473.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;作者猜测 encoder 中使用固定长度的向量（即将句子编码成一个固定长度的向量）可能是 performance 的瓶颈。因此提出一种能够自动 search 源句子中与预测词相关的部分。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Attention" scheme="https://www.yam.gift/tags/Attention/"/>
    
      <category term="Bahdanau Attention" scheme="https://www.yam.gift/tags/Bahdanau-Attention/"/>
    
  </entry>
  
  <entry>
    <title>Neural Architectures for Named Entity Recognition Note</title>
    <link href="https://www.yam.gift/2019/12/28/Paper/2019-12-28-Neural-Architectures-for-Named-Entity-Recognition/"/>
    <id>https://www.yam.gift/2019/12/28/Paper/2019-12-28-Neural-Architectures-for-Named-Entity-Recognition/</id>
    <published>2019-12-28T09:00:00.000Z</published>
    <updated>2019-12-28T08:56:49.711Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/1603.01360.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1603.01360.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;code: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/clab/stack-lstm-ner&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;clab/stack-lstm-ner: NER system based on stack LSTMs&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/glample/tagger&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;glample/tagger: Named Entity Recognition Tool&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;核心思想：pretrained + character-based 词表示分别学习形态和拼写，Bi-LSTM + CRF 和基于转移的模型均可以对输出标签的依赖关系建模。&lt;/p&gt;
&lt;p&gt;看了 Related Work 后发现很多想法其实早就冒出来了，不同的论文在不同点上使用了不同的方法，本篇恰好用这样的方法取得了最好的效果。其实，我觉得更加有意思的是基于转移的模型，它构建了一个 action 的时间序列，感觉更加抽象，想法更加精妙。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="NER" scheme="https://www.yam.gift/tags/NER/"/>
    
      <category term="CRF" scheme="https://www.yam.gift/tags/CRF/"/>
    
      <category term="Bi-LSTM" scheme="https://www.yam.gift/tags/Bi-LSTM/"/>
    
      <category term="Embedding" scheme="https://www.yam.gift/tags/Embedding/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch3）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch3/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch3/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-23T15:12:09.172Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;toc&quot;&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#第三章：高质量的代码&quot; data-toc-modified-id=&quot;第三章：高质量的代码-1&quot;&gt;第三章：高质量的代码&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-16：数值的整数次方&quot; data-toc-modified-id=&quot;面试题-16：数值的整数次方-1.1&quot;&gt;面试题 16：数值的整数次方&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-17：打印-1-到最大的-n-位数&quot; data-toc-modified-id=&quot;面试题-17：打印-1-到最大的-n-位数-1.2&quot;&gt;面试题 17：打印 1 到最大的 n 位数&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-18（一）：在-O-(1)-时间删除链表节点&quot; data-toc-modified-id=&quot;面试题-18（一）：在-O-(1)-时间删除链表节点-1.3&quot;&gt;面试题 18（一）：在 O (1) 时间删除链表节点&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-18（二）：删除链表中重复的节点&quot; data-toc-modified-id=&quot;面试题-18（二）：删除链表中重复的节点-1.4&quot;&gt;面试题 18（二）：删除链表中重复的节点&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-19：正则表达式匹配&quot; data-toc-modified-id=&quot;面试题-19：正则表达式匹配-1.5&quot;&gt;面试题 19：正则表达式匹配&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-20：表示数值的字符串&quot; data-toc-modified-id=&quot;面试题-20：表示数值的字符串-1.6&quot;&gt;面试题 20：表示数值的字符串&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-21：调整数组顺序使奇数位于偶数前面&quot; data-toc-modified-id=&quot;面试题-21：调整数组顺序使奇数位于偶数前面-1.7&quot;&gt;面试题 21：调整数组顺序使奇数位于偶数前面&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-22：链表中倒数第-k-个节点&quot; data-toc-modified-id=&quot;面试题-22：链表中倒数第-k-个节点-1.8&quot;&gt;面试题 22：链表中倒数第 k 个节点&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-23：链表中环的入口节点&quot; data-toc-modified-id=&quot;面试题-23：链表中环的入口节点-1.9&quot;&gt;面试题 23：链表中环的入口节点&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-24：反转链表&quot; data-toc-modified-id=&quot;面试题-24：反转链表-1.10&quot;&gt;面试题 24：反转链表&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-25：合并两个排序的链表&quot; data-toc-modified-id=&quot;面试题-25：合并两个排序的链表-1.11&quot;&gt;面试题 25：合并两个排序的链表&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-26：树的子结构&quot; data-toc-modified-id=&quot;面试题-26：树的子结构-1.12&quot;&gt;面试题 26：树的子结构&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch4）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch4/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch4/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-23T15:12:17.759Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;toc&quot;&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#第四章：解决面试题的思路&quot; data-toc-modified-id=&quot;第四章：解决面试题的思路-1&quot;&gt;第四章：解决面试题的思路&lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-27：二叉树的镜像&quot; data-toc-modified-id=&quot;面试题-27：二叉树的镜像-1.1&quot;&gt;面试题 27：二叉树的镜像&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-28：对称的二叉树&quot; data-toc-modified-id=&quot;面试题-28：对称的二叉树-1.2&quot;&gt;面试题 28：对称的二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-29：顺时针打印矩阵&quot; data-toc-modified-id=&quot;面试题-29：顺时针打印矩阵-1.3&quot;&gt;面试题 29：顺时针打印矩阵&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-30：包含-min-函数的栈&quot; data-toc-modified-id=&quot;面试题-30：包含-min-函数的栈-1.4&quot;&gt;面试题 30：包含 min 函数的栈&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-31：栈的压入、弹出序列&quot; data-toc-modified-id=&quot;面试题-31：栈的压入、弹出序列-1.5&quot;&gt;面试题 31：栈的压入、弹出序列&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-32（一）：不分行从上往下打印二叉树&quot; data-toc-modified-id=&quot;面试题-32（一）：不分行从上往下打印二叉树-1.6&quot;&gt;面试题 32（一）：不分行从上往下打印二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-32（二）：分行从上到下打印二叉树&quot; data-toc-modified-id=&quot;面试题-32（二）：分行从上到下打印二叉树-1.7&quot;&gt;面试题 32（二）：分行从上到下打印二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-32（三）：之字形打印二叉树&quot; data-toc-modified-id=&quot;面试题-32（三）：之字形打印二叉树-1.8&quot;&gt;面试题 32（三）：之字形打印二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-33：二叉搜索树的后序遍历序列&quot; data-toc-modified-id=&quot;面试题-33：二叉搜索树的后序遍历序列-1.9&quot;&gt;面试题 33：二叉搜索树的后序遍历序列&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-34：二叉树中和为某一值的路径&quot; data-toc-modified-id=&quot;面试题-34：二叉树中和为某一值的路径-1.10&quot;&gt;面试题 34：二叉树中和为某一值的路径&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-35：复杂链表的复制&quot; data-toc-modified-id=&quot;面试题-35：复杂链表的复制-1.11&quot;&gt;面试题 35：复杂链表的复制&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-36：二叉搜索树与双向链表&quot; data-toc-modified-id=&quot;面试题-36：二叉搜索树与双向链表-1.12&quot;&gt;面试题 36：二叉搜索树与双向链表&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-37：序列化二叉树&quot; data-toc-modified-id=&quot;面试题-37：序列化二叉树-1.13&quot;&gt;面试题 37：序列化二叉树&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-38：字符串的排列&quot; data-toc-modified-id=&quot;面试题-38：字符串的排列-1.14&quot;&gt;面试题 38：字符串的排列&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch6）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch6/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch6/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-26T01:17:12.037Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch5）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch5/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch5/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-26T01:16:56.062Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>剑指 Offer2（Python 版）解析（Ch2）</title>
    <link href="https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch2/"/>
    <id>https://www.yam.gift/2019/12/15/DS/2019-12-15-Coding-Review2-Ch2/</id>
    <published>2019-12-15T15:00:00.000Z</published>
    <updated>2020-04-23T15:11:57.000Z</updated>
    
    <summary type="html">
    
      &lt;div class=&quot;toc&quot;&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#第一章：面试的流程&quot; data-toc-modified-id=&quot;第一章：面试的流程-1&quot;&gt;第一章：面试的流程 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#第二章：面试需要的基础知识&quot; data-toc-modified-id=&quot;第二章：面试需要的基础知识-2&quot;&gt;第二章：面试需要的基础知识 &lt;/a&gt;&lt;/span&gt;&lt;ul class=&quot;toc-item&quot;&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-2：实现-Singleton-模式&quot; data-toc-modified-id=&quot;面试题-2：实现-Singleton-模式-2.1&quot;&gt;面试题 2：实现 Singleton 模式 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-3（一）：找出数组中重复的数字&quot; data-toc-modified-id=&quot;面试题-3（一）：找出数组中重复的数字-2.2&quot;&gt;面试题 3（一）：找出数组中重复的数字 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-3（二）：不修改数组找出重复的数字&quot; data-toc-modified-id=&quot;面试题-3（二）：不修改数组找出重复的数字-2.3&quot;&gt;面试题 3（二）：不修改数组找出重复的数字 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-4：二维数组中的查找&quot; data-toc-modified-id=&quot;面试题-4：二维数组中的查找-2.4&quot;&gt;面试题 4：二维数组中的查找 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-5：替换空格&quot; data-toc-modified-id=&quot;面试题-5：替换空格-2.5&quot;&gt;面试题 5：替换空格 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-6：从尾到头打印链表&quot; data-toc-modified-id=&quot;面试题-6：从尾到头打印链表-2.6&quot;&gt;面试题 6：从尾到头打印链表 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-7：重建二叉树&quot; data-toc-modified-id=&quot;面试题-7：重建二叉树-2.7&quot;&gt;面试题 7：重建二叉树 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-8：二叉树的下一个节点&quot; data-toc-modified-id=&quot;面试题-8：二叉树的下一个节点-2.8&quot;&gt;面试题 8：二叉树的下一个节点 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-9：用两个栈实现队列&quot; data-toc-modified-id=&quot;面试题-9：用两个栈实现队列-2.9&quot;&gt;面试题 9：用两个栈实现队列 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-10：斐波那契数列&quot; data-toc-modified-id=&quot;面试题-10：斐波那契数列-2.10&quot;&gt;面试题 10：斐波那契数列 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-11：旋转数组的最小数字&quot; data-toc-modified-id=&quot;面试题-11：旋转数组的最小数字-2.11&quot;&gt;面试题 11：旋转数组的最小数字 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-12：矩阵中的路径&quot; data-toc-modified-id=&quot;面试题-12：矩阵中的路径-2.12&quot;&gt;面试题 12：矩阵中的路径 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-13：机器人的运动范围&quot; data-toc-modified-id=&quot;面试题-13：机器人的运动范围-2.13&quot;&gt;面试题 13：机器人的运动范围 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-14：剪绳子&quot; data-toc-modified-id=&quot;面试题-14：剪绳子-2.14&quot;&gt;面试题 14：剪绳子 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href=&quot;#面试题-15：二进制中-1-的个数&quot; data-toc-modified-id=&quot;面试题-15：二进制中-1-的个数-2.15&quot;&gt;面试题 15：二进制中 1 的个数 &lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;具体实现和测试代码&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/hscspring/The-DataStructure-and-Algorithms/tree/master/CodingInterview2-Python&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;The-DataStructure-and-Algorithms/CodingInterview2-Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;系列解析&lt;/strong&gt;（TBD）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 单例模式&lt;/li&gt;
&lt;li&gt;好玩儿的 DP&lt;/li&gt;
&lt;li&gt;递归还是递归&lt;/li&gt;
&lt;li&gt;双指针的威力&lt;/li&gt;
&lt;li&gt;双列表的威力&lt;/li&gt;
&lt;li&gt;有趣的排列组合&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;特别说明&lt;/strong&gt;：下文中的实例代码一般仅包括核心算法（不一定能直接运行），完整代码可以参考对应的链接。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Data Structure" scheme="https://www.yam.gift/tags/Data-Structure/"/>
    
      <category term="Algorithm" scheme="https://www.yam.gift/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Few-Shot Charge Prediction with Discriminative Legal Attributes Note</title>
    <link href="https://www.yam.gift/2019/12/15/Paper/2019-12-15-Few-Shot-Charge-Prediction-with-Discriminative-Legal-Attributes/"/>
    <id>https://www.yam.gift/2019/12/15/Paper/2019-12-15-Few-Shot-Charge-Prediction-with-Discriminative-Legal-Attributes/</id>
    <published>2019-12-15T11:00:00.000Z</published>
    <updated>2019-12-15T11:16:03.324Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;http://nlp.csai.tsinghua.edu.cn/~tcc/publications/coling2018_attribute.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;coling2018_attribute.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;code: &lt;a href=&quot;https://github.com/thunlp/attribute_charge&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;thunlp/attribute_charge&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;核心思想：基于类别属性的注意力机制共同学习属性感知和无属性的文本表示。&lt;/p&gt;
&lt;p&gt;这是 COLING2018 上的一篇老论文了，最近因为一些事情正好遇上，当时大概看了一下就发现这篇文章正好解决了我之前在做多分类&lt;a href=&quot;https://github.com/hscspring/Multi-Label-Text-Classification-for-Chinese#others&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;任务&lt;/a&gt;时没有解决的问题。所以拿来记录一下，顺便研究下代码。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Few-Shot" scheme="https://www.yam.gift/tags/Few-Shot/"/>
    
      <category term="Imbalance Data" scheme="https://www.yam.gift/tags/Imbalance-Data/"/>
    
      <category term="Confusing Labels" scheme="https://www.yam.gift/tags/Confusing-Labels/"/>
    
  </entry>
  
  <entry>
    <title>关系提取简述</title>
    <link href="https://www.yam.gift/2019/12/11/KG/2019-12-11-Relationship-Extraction/"/>
    <id>https://www.yam.gift/2019/12/11/KG/2019-12-11-Relationship-Extraction/</id>
    <published>2019-12-11T15:00:00.000Z</published>
    <updated>2020-04-02T03:57:00.463Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;之前整理过一篇关于信息提取的笔记，也是基于大名鼎鼎的 &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SLP&lt;/a&gt; 第 18 章的内容，最近在做一个 chatbot 的 &lt;a href=&quot;https://yam.gift/2019/12/02/2019-12-02-NLM/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NLMLayer&lt;/a&gt; 时涉及到了不少知识图谱有关的技术，由于 NLMLayer 默认的输入是 NLU 的 output，所以实体识别（包括实体和类别）已经自动完成了。接下来最重要的就是实体属性和关系提取了，所以这里就针对这块内容做一个整理。&lt;/p&gt;
&lt;p&gt;属性一般的形式是（实体，属性，属性值），关系的一般形式是（实体，关系，实体）。简单来区分的话，关系涉及到两个实体，而属性只有一个实体。属性提取的文章比较少，关系提取方面倒是比较成熟，不过这两者之间其实可以借鉴的。具体的一些方法其实&lt;a href=&quot;https://yam.gift/2019/04/09/SLP/2019-04-09-Information-Extraction/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;已经提到不少了，这里单独提出来再梳理一遍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Knowledge Graph" scheme="https://www.yam.gift/tags/Knowledge-Graph/"/>
    
      <category term="Relationship Extraction" scheme="https://www.yam.gift/tags/Relationship-Extraction/"/>
    
  </entry>
  
  <entry>
    <title>AINLP GPU 使用体验指南</title>
    <link href="https://www.yam.gift/2019/12/09/2019-12-09-AINLP-GPU-Guide/"/>
    <id>https://www.yam.gift/2019/12/09/2019-12-09-AINLP-GPU-Guide/</id>
    <published>2019-12-09T13:00:00.000Z</published>
    <updated>2019-12-09T12:49:20.106Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://gpu.ainlp.cn/home&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AINLP-DBC GPU&lt;/a&gt; 是一个 GPU 算力服务平台，采用 DBC TOKEN 进行结算。在这里可以租用 GPU，也可以将自己的 GPU 出租出去。&lt;/p&gt;
&lt;h2 id=&quot;注册&quot;&gt;&lt;a href=&quot;#注册&quot; class=&quot;headerlink&quot; title=&quot;注册&quot;&gt;&lt;/a&gt;注册&lt;/h2&gt;&lt;p&gt;第一步：&lt;a href=&quot;https://gpu.ainlp.cn/gpu/myWallet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;创建钱包&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这里需要输入密码，之后会产生一个加密后的私钥文件，下载继续后会产生你真正的私钥。一定要记住你的密码并在物理介质上保存好加密后的私钥文件以及你的私钥。只有通过密码+加密的私钥文件，或者私钥才能打开你的钱包，如果都丢了，就等于你的钱包没了。&lt;/p&gt;
&lt;p&gt;第二步：充值 DBC&lt;/p&gt;
&lt;p&gt;点击 “如何购买 DBC” 链接，选择自己喜欢的方式充值即可，推荐使用支付宝，点击 “继续” 后，充值一定金额（比如 1块或者 0.1 块）就好了。这步其实就是给你的钱包地址充值一定数额的 DBC。大概等个几十秒就能在 “我的钱包” 里看到你购买金额对应的 DBC 数量了。&lt;/p&gt;
&lt;p&gt;第三步：&lt;a href=&quot;https://gpu.ainlp.cn/gpu/myMachineUnlock&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;绑定邮箱&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;点击 “绑定邮箱” 后，输入邮箱地址，会给你发送一个类似 &lt;code&gt;请输入如下数量dbc:0.7311,验证有效期为30分钟&lt;/code&gt; 内容的邮件，将对应的额度（比如这里的 0.7311）输入 “验证的 DBC 数量” 框即可完成绑定。&lt;/p&gt;
&lt;p&gt;第四步：&lt;a href=&quot;https://gpu.ainlp.cn/gpu/list&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;选择机器&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在列表中选择一台符合自己要求的机器，点击 “租用” 后，填写租用时长（最短 1 小时），等待大约 1 分钟左右（验证机器环境），确认支付后就可以正式使用了。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="Computer" scheme="https://www.yam.gift/tags/Computer/"/>
    
      <category term="ssh" scheme="https://www.yam.gift/tags/ssh/"/>
    
      <category term="GPU" scheme="https://www.yam.gift/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>ELECTRA, Pre-Training Text Encoders as Discriminators Rather Than Generators Note</title>
    <link href="https://www.yam.gift/2019/12/08/Paper/2019-12-08-ELECTRA-Paper/"/>
    <id>https://www.yam.gift/2019/12/08/Paper/2019-12-08-ELECTRA-Paper/</id>
    <published>2019-12-08T14:00:00.000Z</published>
    <updated>2019-12-08T13:48:06.706Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Paper: &lt;a href=&quot;https://openreview.net/pdf?id=r1xMH1BtvB&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本来代码还没出来不想看的，不过前段时间确实太火了，先偷偷瞄一眼，看看到底是什么个情况。&lt;/p&gt;
&lt;p&gt;核心思想：Replaced token detection Task + Transformer。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Feeling" scheme="https://www.yam.gift/categories/Feeling/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="Transformer" scheme="https://www.yam.gift/tags/Transformer/"/>
    
      <category term="Pre-training" scheme="https://www.yam.gift/tags/Pre-training/"/>
    
      <category term="Electra" scheme="https://www.yam.gift/tags/Electra/"/>
    
  </entry>
  
  <entry>
    <title>自然语言记忆模块（NLM）</title>
    <link href="https://www.yam.gift/2019/12/02/2019-12-02-NLM/"/>
    <id>https://www.yam.gift/2019/12/02/2019-12-02-NLM/</id>
    <published>2019-12-02T14:00:00.000Z</published>
    <updated>2019-12-10T07:34:13.088Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍自然语言的记忆（存储与查询）模块，初衷是作为 chatbot 的 Layer 之一，主要功能是记忆（存储）从对话或训练数据学到的 “知识”，然后在需要时唤起（查询） 。目前成熟的方法是以图数据库作为载体，将知识存储为一系列的 ”节点“ 和 ”关系“。之后再基于这些存储的 ”节点“ 和 ”关系“ 进行相关查询。也可以理解为构建 Data Model 的问题。&lt;/p&gt;
&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/hscspring/NLM&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/hscspring/NLM&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;设计思想&quot;&gt;&lt;a href=&quot;#设计思想&quot; class=&quot;headerlink&quot; title=&quot;设计思想&quot;&gt;&lt;/a&gt;设计思想&lt;/h2&gt;&lt;p&gt;图数据库的典型代表是 &lt;a href=&quot;http://neo4j.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Neo4j&lt;/a&gt;，Neo4j 中有几个很重要的概念：标签、节点和关系。标签是一类节点，可以看作是节点的类别，节点一般是某一个实体；关系存在于两个实体间，可以有多种不同的关系。节点和关系可以有多个属性。实践来看，Python 语言可以使用社区的 &lt;a href=&quot;https://github.com/technige/py2neo&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;technige/py2neo&lt;/a&gt;，当然还可以使用官方的 &lt;a href=&quot;https://github.com/neo4j/neo4j-python-driver&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;neo4j/neo4j-python-driver: Neo4j Bolt driver for Python&lt;/a&gt;，两者的目的都是将数据 import 进 database 并进行相应的查询。&lt;/p&gt;
&lt;p&gt;Neo4j 的特点要求导入的数据尽量是结构化的，也就是我们要事先有实体和它的类别（实体的属性可有可无），实体与实体间的关系（关系的属性可有可无）。我们期待能从对话或无监督的语料中自动提取实体和关系，然后自动 import 进 Neo4j。为了避免导入数据的混乱，自然最好能有先验的 “类别”，比如节点类别 Person，Movie 等，关系类别 LOVES，ACTS 等。所以，对于文本输入，我们需要一个信息提取器，将文本中的符合先验类别的节点和关系提取出来。如果输入是 NLU 模块输出的 ”意图和实体“ ，则需要一个分类器，将意图分类到对应的 Relation 类别，将实体分类到 Node 类别。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Thinking" scheme="https://www.yam.gift/categories/Thinking/"/>
    
    
      <category term="NLP" scheme="https://www.yam.gift/tags/NLP/"/>
    
      <category term="NLM" scheme="https://www.yam.gift/tags/NLM/"/>
    
      <category term="Neo4j" scheme="https://www.yam.gift/tags/Neo4j/"/>
    
      <category term="Knowledge Graph" scheme="https://www.yam.gift/tags/Knowledge-Graph/"/>
    
  </entry>
  
  <entry>
    <title>Sort Based on Multiway Tree</title>
    <link href="https://www.yam.gift/2019/11/03/DS/2019-11-03-Multiway-Tree-Sort/"/>
    <id>https://www.yam.gift/2019/11/03/DS/2019-11-03-Multiway-Tree-Sort/</id>
    <published>2019-11-03T15:00:00.000Z</published>
    <updated>2019-11-04T04:15:13.421Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Several weeks ago, we met a sort problem in our program of web app. We tried kinds of methods and finally have gotten a nearly 10 times performance improvement. The problem is very interesting and worth recording.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Elixir" scheme="https://www.yam.gift/tags/Elixir/"/>
    
      <category term="Sort" scheme="https://www.yam.gift/tags/Sort/"/>
    
      <category term="Tree" scheme="https://www.yam.gift/tags/Tree/"/>
    
      <category term="Multiway Tree" scheme="https://www.yam.gift/tags/Multiway-Tree/"/>
    
  </entry>
  
  <entry>
    <title>Python 小白快速从入门到放弃：使用框架</title>
    <link href="https://www.yam.gift/2019/10/10/Py4F/2019-10-10-Python-for-Freshman-Ch03/"/>
    <id>https://www.yam.gift/2019/10/10/Py4F/2019-10-10-Python-for-Freshman-Ch03/</id>
    <published>2019-10-10T15:00:00.000Z</published>
    <updated>2019-10-11T03:14:49.194Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;随着学习的不断深入，我们肯定会越来越不满足只在 Jupyter Notebook 中写一些小任务。我们可能会希望做一个 Web 应用，或者一个小程序，甚至是一个 APP。对于这种系统性的工程项目，框架就必不可少了，它可以极大地提高我们的效率。这节课我们就以 Python 的 Django 框架为例来开发一个小的 Web 应用程序。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
      <category term="Django" scheme="https://www.yam.gift/tags/Django/"/>
    
  </entry>
  
  <entry>
    <title>Python 小白快速从入门到放弃：在结束后</title>
    <link href="https://www.yam.gift/2019/10/07/Py4F/2019-10-07-Python-for-Freshman-Ch05/"/>
    <id>https://www.yam.gift/2019/10/07/Py4F/2019-10-07-Python-for-Freshman-Ch05/</id>
    <published>2019-10-07T12:00:00.000Z</published>
    <updated>2019-11-03T03:15:55.236Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;这个系列的课程的目标在《在开始前》已经说得很清楚了：解决重复劳动或自己做好玩儿的小项目；尝试新的思维方式。这短短的几节课要想把 Python 的相关知识面面俱到是不可能的，但我觉得已经给出了一个全图景，大家只要围绕这个做，达到目标应该是不成问题的。我想说的还是一直提倡的：Just do it，在实践中不断成长。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Coding" scheme="https://www.yam.gift/categories/Coding/"/>
    
    
      <category term="Python" scheme="https://www.yam.gift/tags/Python/"/>
    
  </entry>
  
</feed>
